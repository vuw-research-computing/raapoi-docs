{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the R\u0101poi HPC documentation! \u00b6 The R\u0101poi HPC (High Performance Computing) system, also known as Raapoi, is a computing resource provided by Te Herenga Waka that allows you to run large and complex computing tasks. The cluster uses the Slurm resource manager to schedule jobs and allocate resources, such as CPU, memory, and time, to each job. It's important to carefully consider how much of these resources you need for your job, as requesting too little can result in your job terminating prematurely or running slowly, while requesting too much may unnecessarily delay the execution of other jobs. R\u0101poi consists of different partitions, each of which is a group of compute nodes with a specific hardware configuration. The partition you use for your job will depend on the type of work or task you want to submit. To customize your computing environment, R\u0101poi uses the module system, which allows you to easily access the applications and programming languages you need. In this documentation, you will find all the information you need to get started with running jobs on the R\u0101poi HPC Cluster. If you need more help, check out the training tutorials, which provide step-by-step instructions for popular applications and languages, or the examples section for simpler examples. If you notice any errors or outdated information in the documentation, please don't hesitate to create an issue on the documentation github page or submit a pull request to improve the documentation. Visit the R\u0101poi Slack Channel for help, software requests or to communicate with others in the R\u0101poi community. The Slack Channel should be the first place you go for help with issues related to the HPC. (Please do not contact Digital Solutions for help with issues related to R\u0101poi!) You can also find more information about the current R\u0101poi hardware layout .","title":"Overview"},{"location":"#welcome-to-the-rapoi-hpc-documentation","text":"The R\u0101poi HPC (High Performance Computing) system, also known as Raapoi, is a computing resource provided by Te Herenga Waka that allows you to run large and complex computing tasks. The cluster uses the Slurm resource manager to schedule jobs and allocate resources, such as CPU, memory, and time, to each job. It's important to carefully consider how much of these resources you need for your job, as requesting too little can result in your job terminating prematurely or running slowly, while requesting too much may unnecessarily delay the execution of other jobs. R\u0101poi consists of different partitions, each of which is a group of compute nodes with a specific hardware configuration. The partition you use for your job will depend on the type of work or task you want to submit. To customize your computing environment, R\u0101poi uses the module system, which allows you to easily access the applications and programming languages you need. In this documentation, you will find all the information you need to get started with running jobs on the R\u0101poi HPC Cluster. If you need more help, check out the training tutorials, which provide step-by-step instructions for popular applications and languages, or the examples section for simpler examples. If you notice any errors or outdated information in the documentation, please don't hesitate to create an issue on the documentation github page or submit a pull request to improve the documentation. Visit the R\u0101poi Slack Channel for help, software requests or to communicate with others in the R\u0101poi community. The Slack Channel should be the first place you go for help with issues related to the HPC. (Please do not contact Digital Solutions for help with issues related to R\u0101poi!) You can also find more information about the current R\u0101poi hardware layout .","title":"Welcome to the R\u0101poi HPC documentation!"},{"location":"accessing_the_cluster/","text":"Accessing the Cluster \u00b6 To access R\u0101poi, you'll first need to get an account provisioned for you by completing the form in the Service Desk Portal or contacting the R\u0101poi support team with your: Full Name VUW STAFF username Faculty, School or Institute affiliation. If you don't have a VUW staff account, it may still be possible to be given access - please contact us to determine options. Connecting to the login node \u00b6 Access R\u0101poi via SSH clients or a terminal application. The details are as follows: Hostname: raapoi.vuw.ac.nz IP Address: 130.195.19.126 Port: 22 Username: Your VUW username Password: Your VUW password Note VSCode users , please follow this link to connect: VSCode on R\u0101poi Note A wired network connection or VPN is required if connecting from campus wifi or from off-campus. Some users have had issues with using the hostname and instead need to use the IP address, eg harrelwe@130.195.19.126 More information on VUW VPN services can be found here . SSH Clients \u00b6 Linux SSH Clients Linux built-in terminal application e.g., GNOME Terminal, Konsole, xTerm, etc. can provide access. Windows SSH Clients Recommended Clients: Git Bash is a great option and is part of the Git for Windows project. MobaXterm is a good option, especially if you require access to GUI applications such as MATLAB or xStata. This also has a built-in SFTP transfer window. Mac OSX SSH Clients You can use the built-in Terminal.app or you can download iTerm2 or XQuartz. XQuartz is required to be installed if you wish to forward GUI applications (matlab, rstudio, xstata, sas, etc), aka X forwarding. Terminal.app is the default application for command-line interface To login using the built-in Terminal.app on Mac, go to Applications --> Utilities --> Terminal.app Or use Spotlight search (aka Command-Space) iTerm2 is a good replacement for the default Terminal app XQuartz is a Xforwarding application with its own terminal. XQuartz can be used in conjuction with the Terminal.app for GUI apps. NOTE: Mac users should run the following command: sudo defaults write org.macosforge.xquartz.X11 enable_iglx -bool true We have found that this allows some older GUI applications to run with fewer errors. Note Once at the command prompt you can type the following to login (replace \"username\" with your VUW user): $ ssh -X username@raapoi.vuw.ac.nz The -X parameter tells SSH to forward any GUI windows to your local machine, this is called X forwarding. The first time an SSH client connects to the server, it displays the servers public key fingerprint. An SSH host key identifies the server to your ssh client. They are an important security feature and not something you should just hit ENTER to accept. The authenticity of host 'raapoi.vuw.ac.nz (130.195.19.126)' can't be established. ED25519 key fingerprint is SHA256:f+rhB7q5nt/HxcNK3qA8UfSdSJ7J05L1dU4C2fslkxg. This host key is known by the following other names/addresses: C:\\Users\\username/.ssh/known_hosts:109: raapoi Are you sure you want to continue connecting (yes/no/[fingerprint])? Confirm that the finger print on the login server matches the appropriate fingerprint shown below and type 'yes' . From August 2023 (\"New R\u0101poi\" 130.195.19.126): ssh-ed25519 256 SHA256:f+rhB7q5nt/HxcNK3qA8UfSdSJ7J05L1dU4C2fslkxg ssh-ecdsa 256 SHA256:ChU88YMNnUiXWmQRV0cgeDdnUpsdybgF14Dk3KW3dr4 ssh-rsa 3072 SHA256:izq2NXKroc7gpu0vkWNQnXd4kmjk/pmrQw9vMvwAsIs For returning users, please follow these instructions to update SSH public key: Notes regarding old host keys Upon successfully logging in, the prompt will display: <username>@raapoi-login:~$ # Remember to not compute on the login node! Get yourself familiar with a Compute node! After logging in, do most things on the compute node by running command srun --pty bash . For more info, please see Interactive Jobs. File Transfer with SFTP, SCP or rsync \u00b6 Caution: Massive Data Transfer Kindly use a non-login node for massive data transfer. This can be done by requesting an interactive session . If you are using a file trasnfer tool, instructions are available for FileZilla WinSCP MobaXterm Globus Please contact the support team for help. One can use built-in command-line tools on Linux, Mac and Windows (if running Git Bash or MobaXterm). The most common command-line utilities are scp, sftp and rsync . To copy a single file to or from the cluster, we can use scp (\u201csecure copy\u201d). To upload to another computer, try: [you@laptop:~]$ scp <file_name_to_transfer> \\ <user_name>@raapoi.vuw.ac.nz:/nfs/home/<user_name>/<destination> I would upload a file from my device(local) to R\u0101poi(remote) using: you@laptop:~$ scp demo.tar.gz <user_name>@raapoi.vuw.ac.nz:/nfs/scratch/<user_name>/ SFTP client can be also be set up. In the address bar of my file explorer, I can add the directory I wish to access in the form sftp://<username>@raapoi.vuw.ac.nz:/nfs/home/<username> . In all cases you will need to supply the hostname or IP address of the cluster, see above. You may also need to supply the port (22) and a path. The paths that you will most likely use are your home or your scratch space: /nfs/home/username or /nfs/scratch/username There are many file transfer clients available for Mac, Windows and Linux, including but not limited to Free/OpenSource Desktop tools such as Filezilla, Cyberduck, Dolphin and proprietary/licenced offerings such as WinSCP, ExpanDrive, etc File transfer with cloud tools If you are using cloud storage such as AWS, DropBox, Cloudstor, GLOBUS please look at the examples we have in Connecting to Cloud Providers Notes regarding old host keys \u00b6 IMPORTANT: If the host key does not match the one stored on your client, you will see a warning (example below). The Raapoi login node was replaced in August 2023, if you had been using the previous login node you can expect to see this warning about the change of host key. Double check that the fingerprint matches one of the above before replacing the key stored in your client. @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ @ WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED! @ @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY! Someone could be eavesdropping on you right now (man-in-the-middle attack)! It is also possible that a host key has just been changed. The fingerprint for the ED25519 key sent by the remote host is SHA256:f+rhB7q5nt/HxcNK3qA8UfSdSJ7J05L1dU4C2fslkxg. Please contact your system administrator. Add correct host key in C:\\\\Users\\\\username/.ssh/known_hosts to get rid of this message. Offending ED25519 key in C:\\\\Users\\\\username/.ssh/known_hosts:38 Host key for raapoi.vuw.ac.nz has changed and you have requested strict checking. Host key verification failed. To remove an old host key for raapoi.vuw.ac.nz cached on your client run the following: ssh-keygen -R raapoi.vuw.ac.nz ... and you could also run this to remove the IP address(s) for R\u0101poi: ssh-keygen -R 130 .195.19.14 ssh-keygen -R 130 .195.19.126 If you remove all of these, on your next login you'll be asked to verify the fingerprint again, as illustrated above.","title":"Accessing the Cluster"},{"location":"accessing_the_cluster/#accessing-the-cluster","text":"To access R\u0101poi, you'll first need to get an account provisioned for you by completing the form in the Service Desk Portal or contacting the R\u0101poi support team with your: Full Name VUW STAFF username Faculty, School or Institute affiliation. If you don't have a VUW staff account, it may still be possible to be given access - please contact us to determine options.","title":"Accessing the Cluster"},{"location":"accessing_the_cluster/#connecting-to-the-login-node","text":"Access R\u0101poi via SSH clients or a terminal application. The details are as follows: Hostname: raapoi.vuw.ac.nz IP Address: 130.195.19.126 Port: 22 Username: Your VUW username Password: Your VUW password Note VSCode users , please follow this link to connect: VSCode on R\u0101poi Note A wired network connection or VPN is required if connecting from campus wifi or from off-campus. Some users have had issues with using the hostname and instead need to use the IP address, eg harrelwe@130.195.19.126 More information on VUW VPN services can be found here .","title":"Connecting to the login node"},{"location":"accessing_the_cluster/#ssh-clients","text":"Linux SSH Clients Linux built-in terminal application e.g., GNOME Terminal, Konsole, xTerm, etc. can provide access. Windows SSH Clients Recommended Clients: Git Bash is a great option and is part of the Git for Windows project. MobaXterm is a good option, especially if you require access to GUI applications such as MATLAB or xStata. This also has a built-in SFTP transfer window. Mac OSX SSH Clients You can use the built-in Terminal.app or you can download iTerm2 or XQuartz. XQuartz is required to be installed if you wish to forward GUI applications (matlab, rstudio, xstata, sas, etc), aka X forwarding. Terminal.app is the default application for command-line interface To login using the built-in Terminal.app on Mac, go to Applications --> Utilities --> Terminal.app Or use Spotlight search (aka Command-Space) iTerm2 is a good replacement for the default Terminal app XQuartz is a Xforwarding application with its own terminal. XQuartz can be used in conjuction with the Terminal.app for GUI apps. NOTE: Mac users should run the following command: sudo defaults write org.macosforge.xquartz.X11 enable_iglx -bool true We have found that this allows some older GUI applications to run with fewer errors. Note Once at the command prompt you can type the following to login (replace \"username\" with your VUW user): $ ssh -X username@raapoi.vuw.ac.nz The -X parameter tells SSH to forward any GUI windows to your local machine, this is called X forwarding. The first time an SSH client connects to the server, it displays the servers public key fingerprint. An SSH host key identifies the server to your ssh client. They are an important security feature and not something you should just hit ENTER to accept. The authenticity of host 'raapoi.vuw.ac.nz (130.195.19.126)' can't be established. ED25519 key fingerprint is SHA256:f+rhB7q5nt/HxcNK3qA8UfSdSJ7J05L1dU4C2fslkxg. This host key is known by the following other names/addresses: C:\\Users\\username/.ssh/known_hosts:109: raapoi Are you sure you want to continue connecting (yes/no/[fingerprint])? Confirm that the finger print on the login server matches the appropriate fingerprint shown below and type 'yes' . From August 2023 (\"New R\u0101poi\" 130.195.19.126): ssh-ed25519 256 SHA256:f+rhB7q5nt/HxcNK3qA8UfSdSJ7J05L1dU4C2fslkxg ssh-ecdsa 256 SHA256:ChU88YMNnUiXWmQRV0cgeDdnUpsdybgF14Dk3KW3dr4 ssh-rsa 3072 SHA256:izq2NXKroc7gpu0vkWNQnXd4kmjk/pmrQw9vMvwAsIs For returning users, please follow these instructions to update SSH public key: Notes regarding old host keys Upon successfully logging in, the prompt will display: <username>@raapoi-login:~$ # Remember to not compute on the login node! Get yourself familiar with a Compute node! After logging in, do most things on the compute node by running command srun --pty bash . For more info, please see Interactive Jobs.","title":"SSH Clients"},{"location":"accessing_the_cluster/#file-transfer-with-sftp-scp-or-rsync","text":"Caution: Massive Data Transfer Kindly use a non-login node for massive data transfer. This can be done by requesting an interactive session . If you are using a file trasnfer tool, instructions are available for FileZilla WinSCP MobaXterm Globus Please contact the support team for help. One can use built-in command-line tools on Linux, Mac and Windows (if running Git Bash or MobaXterm). The most common command-line utilities are scp, sftp and rsync . To copy a single file to or from the cluster, we can use scp (\u201csecure copy\u201d). To upload to another computer, try: [you@laptop:~]$ scp <file_name_to_transfer> \\ <user_name>@raapoi.vuw.ac.nz:/nfs/home/<user_name>/<destination> I would upload a file from my device(local) to R\u0101poi(remote) using: you@laptop:~$ scp demo.tar.gz <user_name>@raapoi.vuw.ac.nz:/nfs/scratch/<user_name>/ SFTP client can be also be set up. In the address bar of my file explorer, I can add the directory I wish to access in the form sftp://<username>@raapoi.vuw.ac.nz:/nfs/home/<username> . In all cases you will need to supply the hostname or IP address of the cluster, see above. You may also need to supply the port (22) and a path. The paths that you will most likely use are your home or your scratch space: /nfs/home/username or /nfs/scratch/username There are many file transfer clients available for Mac, Windows and Linux, including but not limited to Free/OpenSource Desktop tools such as Filezilla, Cyberduck, Dolphin and proprietary/licenced offerings such as WinSCP, ExpanDrive, etc File transfer with cloud tools If you are using cloud storage such as AWS, DropBox, Cloudstor, GLOBUS please look at the examples we have in Connecting to Cloud Providers","title":"File Transfer with SFTP, SCP or rsync"},{"location":"advanced_examples/","text":"Advanced Examples \u00b6 Using Containers \u00b6 Researchers can use Docker or Singularity containers within the cluster. This is a great way to run difficult-to-compile applications or to share workflows among colleagues. See also: Singularity Running an interactive container \u00b6 User can run within a container interactively, this is great for testing code before running a job. Here is an example of running within a docker container lolcow: srun --pty -c 4 --mem=64G bash module purge module load GCC/10.2.0 OpenMPI/4.0.5 Singularity/3.10.2 singularity pull docker://godlovedc/lolcow singularity shell lolcow.sif Once you have typed the singularity shell command you will be within the container and can type the commands available from within the container such as the lolcow utility fortune | cowsay | lolcat Running a container in batch \u00b6 Running a batch job with containers is similar to running a regular job, but will ultimately depend on how the container was created, so your mileage may vary. Here is an example batch submit script that will run the autometa software that was created in a docker image, lets name the submit file runContainer.sh: #SBATCH -J autometa-job #SBATCH -c 4 #SBATCH --mem=64G #SBATCH --mailtype=BEGIN,END,FAIL #SBATCH --mail-user=myemail@email.net #SBATCH --time=12:00:00 module load Singularity/3.10.2 singularity pull docker://jasonkwan/autometa:latest singularity exec autometa_latest.sif calculate_read_coverage.py somedata.dat Now to run the file you can: sbatch runContainer.sh Note that singularity shell is primarily for interactive use and singularity exec (or possibly singularity run ) are for executing the applications that were built within the container directly. It is important to know how the container was created to make effective use of the software. Starting and Working with a Jupyter Notebook \u00b6 Running Jupyter notebooks on R\u0101poi is usually a two step processes. First you start the jupyter server on a compute node - either via an interactive session or an sbatch job. Then you connect to R\u0101poi again via a new ssh session port forwarding the port selected by Jupter to your local machine for the web session. Tip There is a potentially simpler method at the end of this guide using firefox and tab containers. For general information on using Python, see the Python users guide . The first step is getting and modifying a submission script. Example submission scripts are included at /home/software/vuwrc/examples/jupyter/ notebook-bare.sh # The base notebook script - you manage your dependancies via pip notebook-anaconda.sh # a version for if you prefer anaconda R-notebook.sh # Using the R kernel instead R-notebook-anaconda.sh # R kernel and managed by anaconda All these scripts will need to be copied to your working directory and modified to suit your needs. In each case you'll need to install your dependancies first - at a bare minimum you'll need Jupyter, installed either via pip or anaconda. Note if you are intending to do anything needing GPU in your notebooks, you'll need to do all these installs in the gpu or highmem nodes as you'll likely need the relavent CUDA modules loaded during the installs. notebook-bare.sh example \u00b6 Step 1: The best way to start jupyter is with a batch submit script. We have created an example script. You can copy this script from one available on the cluster, just type the following: cp /home/software/vuwrc/examples/jupyter/notebook-bare.sh notebook.sh If you are using Anaconda and have installed it in the default location you need to use the following submit file instead: cp /home/software/vuwrc/examples/jupyter/notebook-anaconda.sh notebook-anaconda.sh If you have any python dependancies you will need to install them before you run your script. You will also have to install jupyter. Currenly you'll need to do that in an interactive session. You only need to do this once. srun -c 4 --mem = 8G --partition = quicktest --time = 0 -01:00:00 --pty bash # get a 1 hour interactive session on quicktest #prompt changes to something like #<username@itl02n02> you are now \"on\" a quicktest node # Load required module for the python version in the notebook-bare.sh module load gompi/2022a module load Python/3.10.4-bare python3 -m venv env # setup python virtual env in the env directory pip install jupyterlab pandas plotnine # install dependancies - you *must* at least install jupyter #exit the interactive session exit #prompt changes to something like #<username@raapoi-login> you are now back on the login/master node This script is ready to run as is, but we recommend editing it to satisfy your own CPU, memory and time requirements. Once you have edited the file you can run it thusly: sbatch notebook.sh or if using Anaconda: sbatch notebook-anaconda.sh NOTE: If you copied the R notebook script, replace notebook.sh with R-notebook.sh This will submit the file to run a job. It may take some time for the job to run, depending on how busy the cluster is at the time. Once the job begins to run you will see some information in the file called notebook-JOBID.out (JOBID will be the actual jobid of this job, eg notebook-478903.out. If you view this file (users can type cat notebook-JOBID.out to view the file onscreen). You will see a line such as: Or copy and paste one of these URLs: http://130.195.19.20:47033/lab?token=<some string of characters> http://127.0.0.1:47033/lab?token=<some string of characters> The 2 important pieces of information here are the IP address, in this case 130.195.19.20 and the port number, 47033 . These numbers should be different for you since the port number is random, although the IP Address may be the same since we have a limited number of compute nodes. Also notice after the ?token= you will see a random hash. This hash is a security feature and allows you to connect to the notebook. You will need to use these to view the notebook from your local machine. Step 2: To start working with the notebook you will need to tunnel a ssh session. In your SSH tunnel you will use the cluster login node (raapoi.vuw.ac.nz) to connect to the compute node (in the example above the compute node is at address 130.195.19.20) and transfer all the traffic back and forth between your computer and the compute node). Step 2a from a Mac: Open a new session window from Terminal.app or other terminal utility such as Xquartz and type the following: ssh -L <PORT_NUMBER>:<IP_ADDRESS>:<PORT_NUMBER> username@raapoi.vuw.ac.nz For example: ssh -L 47033:130.195.19.20:47033 janedoe@raapoi.vuw.ac.nz Once you are at a prompt you can go to Step 3 Step 2b: from Windows We recommend tunnelling using Git Bash, which is part of the Git for Windows project or MobaXTerm . There are 2 methods for tunneling in Moba, one is command line, the other is GUI-based. Method 1 (Git Bash or MobaXterm): Command-line, start a local Git Bash or MobaXterm terminal (or try the GUI method, below) From the command prompt type: ssh -L <PORT_NUMBER>:<IP_ADDRESS>:<PORT_NUMBER> username@raapoi.vuw.ac.nz For example: ssh -L 47033:130.195.19.20:47033 janedoe@raapoi.vuw.ac.nz Once you are at a prompt you can go to Step 3 Method 2 (MobaXterm): GUI-based, go to the Tunneling menu: Now click on New SSH Tunnel When you complete the creation of your tunnel click Start all tunnels . Enter your password and reply \"yes\" to any questions asked about accepting hostkeys or opening firewalls. You can safely exit the tunnel building menu. Step 3 Now open your favorite web browser and then use the URL from your job output file and paste it in your browsers location bar, for example my full URL was: http://130.195.19.20:47033/?token=badef11b1371945b314e2e89b9a182f68e39dc40783ed68e Step 4 One last thing you need to do is to replace the IP address with the word localhost . This will allow your browser to follow the tunnel you just opened and connect to the notebook running on an engaging compute node, in my case my address line will now look like this: http://localhost:47033/?token=badef11b1371945b314e2e89b9a182f68e39dc40783ed68e Now you can hit return and you should see your notebook running on an Engaging compute node. NOTE : when you are done with your notebook, please remember to cancel your job to free up the resources for others, hint: scancel If you want more information on working with Jupyter, there is good documentation, here: Jupyter Notebooks Working with notebooks using Firefox tab containers \u00b6 There is a perhaps simpler single step process of working with jupyter notebooks. It relies on some nice features in Firefox . Firefox has tab containers - you can have categories of tabs that are basically independent from each other with separate web cookies but importantly for our case separate proxy settings. You will also currenly need to get the firefox add-on Container-Proxy its github page Setup a tab container in Firefox called something like Raapoi. Use the container-proxy extension to assign a proxy to that tab set. I choose 9001, but you can use any fairly high port number - note it doens't matter if many people connect on the same port. When you connect to raapoi, use ssh socks5 proxy settings. In MacOS/linux/wsl2 ssh -D 9001 <username>@raapoi.vuw.ac.nz putty: Use Putty as a Socks Proxy MobaXterm: MobaXterm SOCKS5 Proxy - stackoverflow In Firefox open a new tab by holding down the new tab + button and selecting your Raapoi tab container. Any tabs opened with that container will have all their webtraffic directed via the R\u0101poi login node. Your laptop/desktop can't directly see all the compute nodes, but the login node can. When you start a jupyter notebook and get the message: Or copy and paste one of these URLs: http://130.195.19.20:47033/lab?token=<some string of characters> http://127.0.0.1:47033/lab?token=<some string of characters> You can just immediatly open http://130.195.19.20:47033/lab?token=<some string of characters> in your Raapoi container tab. OpenBLAS/FlexiBLAS users guide \u00b6 What is BLAS/OpenBLAS/FlexiBLAS? \u00b6 BLAS stands for Basic Linear Algebra Subprograms and consists of a set routines that probide standard building blocks for numerical calculations involving linear algebra (e.g. addition and multiplication of vectors and matrices). Software such as OpenBLAS and FlexiBLAS (and other variants) provide optimised implementations and extensions of such routines. How do I know if my software uses OpenBLAS/FlexiBLAS? \u00b6 Linear algebra is fundamental to a broad range of numerical algorithms, so it is quite common for computationally intensive software to rely on some form of BLAS implementation. One way you can potentially find out if OpenBLAS or FlexiBLAS might be used by your software is to load the modules associated with your piece of software, then execute module list and see if OpenBLAS of FlexiBLAS are listed. If yes, it is not a guarantee they are being used, but is certainly a possibility. Known issues \u00b6 Most versions of OpenBLAS have been installed with OpenMP support. Some of the newer versions (0.3.20 onwards) have a bug which can cause OpenBLAS to hang on startup (unless you allocate an obscene and uneccessary amount of memory). When using software that links to one of those versions it is essential that you: (i) set the environment variable OMP_NUM_THREADS to the desired number of threads, and/or (ii) set the environment varialbe OMP_PROC_BIND=true (or any other valid option, apart from false ). The recommended approach is to add the line export OMP_NUM_THREADS=$SLURM_NTASKS and/or export OMP_PROC_BIND=true after you have loaded all of the modules you require. (You will need to change export OMP_NUM_THREADS=$SLURM_NTASKS accordingly if you are using some form of hybrid paralellism, and/or are setting a value of --ncpus-per-task which is more than one.)","title":"Advanced Examples"},{"location":"advanced_examples/#advanced-examples","text":"","title":"Advanced Examples"},{"location":"advanced_examples/#using-containers","text":"Researchers can use Docker or Singularity containers within the cluster. This is a great way to run difficult-to-compile applications or to share workflows among colleagues. See also: Singularity","title":"Using Containers"},{"location":"advanced_examples/#running-an-interactive-container","text":"User can run within a container interactively, this is great for testing code before running a job. Here is an example of running within a docker container lolcow: srun --pty -c 4 --mem=64G bash module purge module load GCC/10.2.0 OpenMPI/4.0.5 Singularity/3.10.2 singularity pull docker://godlovedc/lolcow singularity shell lolcow.sif Once you have typed the singularity shell command you will be within the container and can type the commands available from within the container such as the lolcow utility fortune | cowsay | lolcat","title":"Running an interactive container"},{"location":"advanced_examples/#running-a-container-in-batch","text":"Running a batch job with containers is similar to running a regular job, but will ultimately depend on how the container was created, so your mileage may vary. Here is an example batch submit script that will run the autometa software that was created in a docker image, lets name the submit file runContainer.sh: #SBATCH -J autometa-job #SBATCH -c 4 #SBATCH --mem=64G #SBATCH --mailtype=BEGIN,END,FAIL #SBATCH --mail-user=myemail@email.net #SBATCH --time=12:00:00 module load Singularity/3.10.2 singularity pull docker://jasonkwan/autometa:latest singularity exec autometa_latest.sif calculate_read_coverage.py somedata.dat Now to run the file you can: sbatch runContainer.sh Note that singularity shell is primarily for interactive use and singularity exec (or possibly singularity run ) are for executing the applications that were built within the container directly. It is important to know how the container was created to make effective use of the software.","title":"Running a container in batch"},{"location":"advanced_examples/#starting-and-working-with-a-jupyter-notebook","text":"Running Jupyter notebooks on R\u0101poi is usually a two step processes. First you start the jupyter server on a compute node - either via an interactive session or an sbatch job. Then you connect to R\u0101poi again via a new ssh session port forwarding the port selected by Jupter to your local machine for the web session. Tip There is a potentially simpler method at the end of this guide using firefox and tab containers. For general information on using Python, see the Python users guide . The first step is getting and modifying a submission script. Example submission scripts are included at /home/software/vuwrc/examples/jupyter/ notebook-bare.sh # The base notebook script - you manage your dependancies via pip notebook-anaconda.sh # a version for if you prefer anaconda R-notebook.sh # Using the R kernel instead R-notebook-anaconda.sh # R kernel and managed by anaconda All these scripts will need to be copied to your working directory and modified to suit your needs. In each case you'll need to install your dependancies first - at a bare minimum you'll need Jupyter, installed either via pip or anaconda. Note if you are intending to do anything needing GPU in your notebooks, you'll need to do all these installs in the gpu or highmem nodes as you'll likely need the relavent CUDA modules loaded during the installs.","title":"Starting and Working with a Jupyter Notebook"},{"location":"advanced_examples/#notebook-baresh-example","text":"Step 1: The best way to start jupyter is with a batch submit script. We have created an example script. You can copy this script from one available on the cluster, just type the following: cp /home/software/vuwrc/examples/jupyter/notebook-bare.sh notebook.sh If you are using Anaconda and have installed it in the default location you need to use the following submit file instead: cp /home/software/vuwrc/examples/jupyter/notebook-anaconda.sh notebook-anaconda.sh If you have any python dependancies you will need to install them before you run your script. You will also have to install jupyter. Currenly you'll need to do that in an interactive session. You only need to do this once. srun -c 4 --mem = 8G --partition = quicktest --time = 0 -01:00:00 --pty bash # get a 1 hour interactive session on quicktest #prompt changes to something like #<username@itl02n02> you are now \"on\" a quicktest node # Load required module for the python version in the notebook-bare.sh module load gompi/2022a module load Python/3.10.4-bare python3 -m venv env # setup python virtual env in the env directory pip install jupyterlab pandas plotnine # install dependancies - you *must* at least install jupyter #exit the interactive session exit #prompt changes to something like #<username@raapoi-login> you are now back on the login/master node This script is ready to run as is, but we recommend editing it to satisfy your own CPU, memory and time requirements. Once you have edited the file you can run it thusly: sbatch notebook.sh or if using Anaconda: sbatch notebook-anaconda.sh NOTE: If you copied the R notebook script, replace notebook.sh with R-notebook.sh This will submit the file to run a job. It may take some time for the job to run, depending on how busy the cluster is at the time. Once the job begins to run you will see some information in the file called notebook-JOBID.out (JOBID will be the actual jobid of this job, eg notebook-478903.out. If you view this file (users can type cat notebook-JOBID.out to view the file onscreen). You will see a line such as: Or copy and paste one of these URLs: http://130.195.19.20:47033/lab?token=<some string of characters> http://127.0.0.1:47033/lab?token=<some string of characters> The 2 important pieces of information here are the IP address, in this case 130.195.19.20 and the port number, 47033 . These numbers should be different for you since the port number is random, although the IP Address may be the same since we have a limited number of compute nodes. Also notice after the ?token= you will see a random hash. This hash is a security feature and allows you to connect to the notebook. You will need to use these to view the notebook from your local machine. Step 2: To start working with the notebook you will need to tunnel a ssh session. In your SSH tunnel you will use the cluster login node (raapoi.vuw.ac.nz) to connect to the compute node (in the example above the compute node is at address 130.195.19.20) and transfer all the traffic back and forth between your computer and the compute node). Step 2a from a Mac: Open a new session window from Terminal.app or other terminal utility such as Xquartz and type the following: ssh -L <PORT_NUMBER>:<IP_ADDRESS>:<PORT_NUMBER> username@raapoi.vuw.ac.nz For example: ssh -L 47033:130.195.19.20:47033 janedoe@raapoi.vuw.ac.nz Once you are at a prompt you can go to Step 3 Step 2b: from Windows We recommend tunnelling using Git Bash, which is part of the Git for Windows project or MobaXTerm . There are 2 methods for tunneling in Moba, one is command line, the other is GUI-based. Method 1 (Git Bash or MobaXterm): Command-line, start a local Git Bash or MobaXterm terminal (or try the GUI method, below) From the command prompt type: ssh -L <PORT_NUMBER>:<IP_ADDRESS>:<PORT_NUMBER> username@raapoi.vuw.ac.nz For example: ssh -L 47033:130.195.19.20:47033 janedoe@raapoi.vuw.ac.nz Once you are at a prompt you can go to Step 3 Method 2 (MobaXterm): GUI-based, go to the Tunneling menu: Now click on New SSH Tunnel When you complete the creation of your tunnel click Start all tunnels . Enter your password and reply \"yes\" to any questions asked about accepting hostkeys or opening firewalls. You can safely exit the tunnel building menu. Step 3 Now open your favorite web browser and then use the URL from your job output file and paste it in your browsers location bar, for example my full URL was: http://130.195.19.20:47033/?token=badef11b1371945b314e2e89b9a182f68e39dc40783ed68e Step 4 One last thing you need to do is to replace the IP address with the word localhost . This will allow your browser to follow the tunnel you just opened and connect to the notebook running on an engaging compute node, in my case my address line will now look like this: http://localhost:47033/?token=badef11b1371945b314e2e89b9a182f68e39dc40783ed68e Now you can hit return and you should see your notebook running on an Engaging compute node. NOTE : when you are done with your notebook, please remember to cancel your job to free up the resources for others, hint: scancel If you want more information on working with Jupyter, there is good documentation, here: Jupyter Notebooks","title":"notebook-bare.sh example"},{"location":"advanced_examples/#working-with-notebooks-using-firefox-tab-containers","text":"There is a perhaps simpler single step process of working with jupyter notebooks. It relies on some nice features in Firefox . Firefox has tab containers - you can have categories of tabs that are basically independent from each other with separate web cookies but importantly for our case separate proxy settings. You will also currenly need to get the firefox add-on Container-Proxy its github page Setup a tab container in Firefox called something like Raapoi. Use the container-proxy extension to assign a proxy to that tab set. I choose 9001, but you can use any fairly high port number - note it doens't matter if many people connect on the same port. When you connect to raapoi, use ssh socks5 proxy settings. In MacOS/linux/wsl2 ssh -D 9001 <username>@raapoi.vuw.ac.nz putty: Use Putty as a Socks Proxy MobaXterm: MobaXterm SOCKS5 Proxy - stackoverflow In Firefox open a new tab by holding down the new tab + button and selecting your Raapoi tab container. Any tabs opened with that container will have all their webtraffic directed via the R\u0101poi login node. Your laptop/desktop can't directly see all the compute nodes, but the login node can. When you start a jupyter notebook and get the message: Or copy and paste one of these URLs: http://130.195.19.20:47033/lab?token=<some string of characters> http://127.0.0.1:47033/lab?token=<some string of characters> You can just immediatly open http://130.195.19.20:47033/lab?token=<some string of characters> in your Raapoi container tab.","title":"Working with notebooks using Firefox tab containers"},{"location":"advanced_examples/#openblasflexiblas-users-guide","text":"","title":"OpenBLAS/FlexiBLAS users guide"},{"location":"advanced_examples/#what-is-blasopenblasflexiblas","text":"BLAS stands for Basic Linear Algebra Subprograms and consists of a set routines that probide standard building blocks for numerical calculations involving linear algebra (e.g. addition and multiplication of vectors and matrices). Software such as OpenBLAS and FlexiBLAS (and other variants) provide optimised implementations and extensions of such routines.","title":"What is BLAS/OpenBLAS/FlexiBLAS?"},{"location":"advanced_examples/#how-do-i-know-if-my-software-uses-openblasflexiblas","text":"Linear algebra is fundamental to a broad range of numerical algorithms, so it is quite common for computationally intensive software to rely on some form of BLAS implementation. One way you can potentially find out if OpenBLAS or FlexiBLAS might be used by your software is to load the modules associated with your piece of software, then execute module list and see if OpenBLAS of FlexiBLAS are listed. If yes, it is not a guarantee they are being used, but is certainly a possibility.","title":"How do I know if my software uses OpenBLAS/FlexiBLAS?"},{"location":"advanced_examples/#known-issues","text":"Most versions of OpenBLAS have been installed with OpenMP support. Some of the newer versions (0.3.20 onwards) have a bug which can cause OpenBLAS to hang on startup (unless you allocate an obscene and uneccessary amount of memory). When using software that links to one of those versions it is essential that you: (i) set the environment variable OMP_NUM_THREADS to the desired number of threads, and/or (ii) set the environment varialbe OMP_PROC_BIND=true (or any other valid option, apart from false ). The recommended approach is to add the line export OMP_NUM_THREADS=$SLURM_NTASKS and/or export OMP_PROC_BIND=true after you have loaded all of the modules you require. (You will need to change export OMP_NUM_THREADS=$SLURM_NTASKS accordingly if you are using some form of hybrid paralellism, and/or are setting a value of --ncpus-per-task which is more than one.)","title":"Known issues"},{"location":"basic_commands/","text":"Basic Commands \u00b6 The vuw Commands \u00b6 In an effort to make using R\u0101poi just a bit easier, CAD staff have created commands to help you view useful information. We call these the vuw commands. This is because all the commands begin with the string vuw . This makes it easier to see the commands available to you. If, at a command prompt you type vuw followed immediately by two TAB keys you will see a list of available commands beginning with vuw . Go ahead and type vuw-TAB-TAB to see for yourself. The commands available as of this update are: Command Description vuw-help Prints this help information vuw-job-report Provides summary information about a job vuw-quota Prints current storage quota and usage vuw-partitions Lists available partitions and compute node availability vuw-alljobs Lists all user jobs vuw-myjobs Lists your running or pending jobs vuw-job-history Shows jobs finished in the last 5 days vuw-job-eff Shows job efficiency (use vuw-job-eff --help for details) vuw-info Shows node resource usage and availability (use vuw-info --help for details) vuw-alloc Shows current usage against user accounts (use vuw-alloc --help for details) Tip If you are unable to use these commands (e.g. with an error message \"command not found\") then double check you have the \"config\" module loaded (i.e. enter the command module load config ). Linux Commands \u00b6 R\u0101poi is built using the Linux operating system. Access is primarily via command line interface (CLI) as opposed to the graphical user interfaces (GUI) that you are more familiar with (such as those on Windows or Mac) Below are a list of common commands for viewing and managing files and directories (replace the file and directory names with ones you own): ls - This command lists the contents of the current directory ls -l This is the same command with a flag (-l) which lists the contents with more information, including access permissions ls -a Same ls command but this time the -a flag which will also list hidden files. Hidden files start with a . (period) ls -la Stringing flags together cd - This will change your location to a different directory (folder) cd projects/calctest_proj Typing cd with no arguments will take you back to your home directory mv - This will move or rename a file mv project1.txt project2.txt mv project2.txt projects/calctest_proj/ cp - This allows you to copy file/s and/or directories to defined locations. The cp command works very much like mv , except it copies a file instead of moving it. The general form of the command is cp source destination , for example: cp myfile.txt myfilecopy.txt Further examples and options can be seen here . rm - This will delete a file rm projects/calctest_proj/projects2.txt rm -r projects/calctest_proj/code The -r flag recursively removes files and directories mkdir - This will create a new directory mkdir /nfs/home/myusername/financial To find more detailed information about any command you can use the manpages, eg: man ls Learning the Linux Shell A good tutorial for using linux can be found here: Learning the linux shell . Software Carpentry also provides a good introduction to the shell, including how to work with files and directories .","title":"Basic Commands"},{"location":"basic_commands/#basic-commands","text":"","title":"Basic Commands"},{"location":"basic_commands/#the-vuw-commands","text":"In an effort to make using R\u0101poi just a bit easier, CAD staff have created commands to help you view useful information. We call these the vuw commands. This is because all the commands begin with the string vuw . This makes it easier to see the commands available to you. If, at a command prompt you type vuw followed immediately by two TAB keys you will see a list of available commands beginning with vuw . Go ahead and type vuw-TAB-TAB to see for yourself. The commands available as of this update are: Command Description vuw-help Prints this help information vuw-job-report Provides summary information about a job vuw-quota Prints current storage quota and usage vuw-partitions Lists available partitions and compute node availability vuw-alljobs Lists all user jobs vuw-myjobs Lists your running or pending jobs vuw-job-history Shows jobs finished in the last 5 days vuw-job-eff Shows job efficiency (use vuw-job-eff --help for details) vuw-info Shows node resource usage and availability (use vuw-info --help for details) vuw-alloc Shows current usage against user accounts (use vuw-alloc --help for details) Tip If you are unable to use these commands (e.g. with an error message \"command not found\") then double check you have the \"config\" module loaded (i.e. enter the command module load config ).","title":"The vuw Commands"},{"location":"basic_commands/#linux-commands","text":"R\u0101poi is built using the Linux operating system. Access is primarily via command line interface (CLI) as opposed to the graphical user interfaces (GUI) that you are more familiar with (such as those on Windows or Mac) Below are a list of common commands for viewing and managing files and directories (replace the file and directory names with ones you own): ls - This command lists the contents of the current directory ls -l This is the same command with a flag (-l) which lists the contents with more information, including access permissions ls -a Same ls command but this time the -a flag which will also list hidden files. Hidden files start with a . (period) ls -la Stringing flags together cd - This will change your location to a different directory (folder) cd projects/calctest_proj Typing cd with no arguments will take you back to your home directory mv - This will move or rename a file mv project1.txt project2.txt mv project2.txt projects/calctest_proj/ cp - This allows you to copy file/s and/or directories to defined locations. The cp command works very much like mv , except it copies a file instead of moving it. The general form of the command is cp source destination , for example: cp myfile.txt myfilecopy.txt Further examples and options can be seen here . rm - This will delete a file rm projects/calctest_proj/projects2.txt rm -r projects/calctest_proj/code The -r flag recursively removes files and directories mkdir - This will create a new directory mkdir /nfs/home/myusername/financial To find more detailed information about any command you can use the manpages, eg: man ls Learning the Linux Shell A good tutorial for using linux can be found here: Learning the linux shell . Software Carpentry also provides a good introduction to the shell, including how to work with files and directories .","title":"Linux Commands"},{"location":"environment/","text":"Enviroment Setup \u00b6 R\u0101poi has an extensive library of applications and software available. There are numerous programming languages and libraries (R, Julia, Python, lua, OpenMPI, blas, etc) as well as dozens of applications (Matlab, Gaussian, etc). We also keep older versions of software to ensure compatibility. Because of this, R\u0101poi developers use a tool called lmod to allow a user to load a specific version of an application, language or library and start using it for their work. The module command will show you what software is available to load, and will add the software to your environment for immediate use. Entering module help into the command prompt will give you some basic information about all of the module sub-commands. Here we briefly describe the most important ones. Searching for software packages \u00b6 This section will give you a brief description on how to find which software packages are available and load them. Searching with module avail \u00b6 To show all software available to load type the following: module avail This is a long list of software packages which can each be loaded immediately (i.e. without first loading pre-requisites). Generally each software package is listed as a path of the form / , eg. lua/5.3.5 . The list is separated into a few sections (via lines of dashes). The section of most interest has the heading -------------------- /home/software/tools/eb_modulefiles/all/Core -------------------- The modules under this heading (or similar) are from the new module system (see New Module System for more extensive details). In contrast, the modules listed under the heading -------------------------- /home/software/tools/modulefiles -------------------------- are from the old module system. Beware that many of these older software packages in this section may no longer work. There may be other sections, depending on what modules you already have loaded. For example, if GCC/10.3.0 is loaded then you will see the additional sections ------------ /home/software/tools/eb_modulefiles/all/Compiler/GCC/10.3.0 ------------- Bio-SearchIO-hmmer/1.7.3 GEOS/3.9.1 Haploflow/1.0 OpenMPI/4.1.1 Boost/1.76.0 GSL/2.7 OpenBLAS/0.3.15 Subread/2.0.3 FlexiBLAS/3.0.4 ---------- /home/software/tools/eb_modulefiles/all/Compiler/GCCcore/10.3.0 ----------- Autoconf/2.71 ( D ) Python/3.9.5 ( D ) libevent/2.1.12 Automake/1.16.3 Qhull/2020.2 libfabric/1.12.1 Autotools/20210128 RE2/2022-02-01 libffi/3.3 BLIS/0.8.1 RapidJSON/1.1.0 libgeotiff/1.6.0 Bazel/3.7.2 Rust/1.52.1 libgit2/1.1.0 <etc. many more lines...> These sections show various additional modules for which GCC/10.3.0 is a pre-requisite. Many pieces of software have one or more pre-requisites in the new module system. A more convenient way to search for specific pieces of software, and determine what their pre-requisite modules are, is via the module spider command. Searching with module spider \u00b6 Suppose you want to find out which Python versions are available, you can search via the command module spider Python , for example: username@raapoi-login:~$ module spider Python -------------------------------------------------------------------------------------- Python: -------------------------------------------------------------------------------------- Description: Python is a programming language that lets you work more quickly and integrate your systems more effectively. Versions: Python/2.7.15-bare Python/2.7.15 Python/2.7.16 Python/2.7.18 <additional output not included here> Note that the capital P in Python is important here, the module spider command is case-sensitive. Capital P Python modules/packages are generally from the new module system, whereas lower case python modules/packages are older (and may no longer work). The same goes for many several other software packages such as R (versus r ) and OpenMPI (versus openmpi ). Among the list of Python versions is Python/3.9.6 . To find out how to load it we call module spider again with the specific Python version included, e.g. username@raapoi-login:~$ module spider Python/3.9.6 -------------------------------------------------------------------------------------- Python: Python/3.9.6 -------------------------------------------------------------------------------------- Description: Python is a programming language that lets you work more quickly and integrate your systems more effectively. You will need to load all module ( s ) on any one of the lines below before the \"Python/3.9.6\" module is available to load. GCCcore/11.2.0 <additional output not included here> Loading software packages \u00b6 Loading a software package is done via the command module load <package name> where should generally include the version as well. For example, the output of module spider Python/3.9.6 (from above) tells us that in order to load this version of Python we must first load GCCcore/11.2.0 . That is, we can load this version of Python via username@raapoi-login:~$ module load GCCcore/11.2.0 username@raapoi-login:~$ module load Python/3.9.6 If we wanted to ensure we had a clean/minimal software environment, we could start with a purge (see below for details), that is username@raapoi-login:~$ module purge username@raapoi-login:~$ module load config GCCcore/11.2.0 username@raapoi-login:~$ module load Python/3.9.6 In either case, you will now be able to run Python version 3.9.6 by entering python at the command prompt. If you are intending to do some computation, you should call srun --pty python to start an interactive job on the quicktest node running Python. Other useful module sub-commands \u00b6 Listing loaded modules \u00b6 To see what modules you have loaded into your environment you can run the command: module list By default you will have the config module loaded (which makes the vuw- commands available) and possibly several others. For example, here are the modules I have loaded in my environment when I login: username@raapoi-login:~$ module list Currently Loaded Modules: 1 ) autotools 3 ) gnu9/9.4.0 5 ) ucx/1.11.2 7 ) openmpi4/4.1.1 9 ) config 2 ) prun/2.2 4 ) hwloc/2.7.2 6 ) libfabric/1.13.0 8 ) ohpc You can also check if a specific module is loaded via the command module is-loaded <package name> . The module whatis and module show commands \u00b6 If you want to know more about a particular module you can use the whatis or show subcommand. The output of module whatis will be more details for some modules than others. Here is one example: username@raapoi-login:~$ module whatis foss/2020b foss/2020b : Description: GNU Compiler Collection ( GCC ) based compiler toolchain, including OpenMPI for MPI support, OpenBLAS ( BLAS and LAPACK support ) , FFTW and ScaLAPACK. foss/2020b : Homepage: https://easybuild.readthedocs.io/en/master/Common-toolchains.html#foss-toolchain foss/2020b : URL: https://easybuild.readthedocs.io/en/master/Common-toolchains.html#foss-toolchain The output of module show shows the help information associated with the module/package as well as various steps which are executed when the module is loaded. username@raapoi-login:~$ module show GCC/10.3.0 ----------------------------------------------------------------------------------------- /home/software/tools/eb_modulefiles/all/Core/GCC/10.3.0.lua: ----------------------------------------------------------------------------------------- help ([[ Description =========== The GNU Compiler Collection includes front ends for C, C++, Objective-C, Fortran, Java, and Ada, as well as libraries for these languages ( libstdc++, libgcj,... ) . More information ================ - Homepage: https://gcc.gnu.org/ ]]) whatis ( \"Description: The GNU Compiler Collection includes front ends for C, C++, Objective-C, Fortran, Java, and Ada, as well as libraries for these languages (libstdc++, libgcj,...).\" ) whatis ( \"Homepage: https://gcc.gnu.org/\" ) whatis ( \"URL: https://gcc.gnu.org/\" ) conflict ( \"GCC\" ) load ( \"GCCcore/10.3.0\" ) load ( \"binutils/2.36.1\" ) prepend_path ( \"MODULEPATH\" , \"/home/software/tools/eb_modulefiles/all/Compiler/GCC/10.3.0\" ) setenv ( \"EBROOTGCC\" , \"/home/software/EasyBuild/software/GCCcore/10.3.0\" ) setenv ( \"EBVERSIONGCC\" , \"10.3.0\" ) setenv ( \"EBDEVELGCC\" , \"/home/software/EasyBuild/software/GCC/10.3.0/easybuild/Core-GCC-10.3.0-easybuild-devel\" ) Note that both the whatis and show commands will only work if you have loaded the pre-requisites of the module/package you are looking up. The module keyword command \u00b6 Using module keyword is another way to search through the module system. Specifically, module keyword blah will search through help messages and whatis descriptions to find any mention of blah . For example, if you know you want to do a search for lua , you can find lua packages and any packages which mention lua in their help/whatis via keyword subcommand: module keyword lua Clearing/purging your environment \u00b6 The command module purge will unload all of your modules. This can be quite useful to do as a first step before loading modules. It can help ensure you don't accidentally end up with conflicting versions of any modules (which generally gets taken care of by lmod, but can sometimes be an issue), and can help you work out a minimal environment that works for your desired software. If you execute module purge you will lose access to the vuw- commands, but you can reload them via module load config . Individual modules/packages can also be unloaded via module unload <package name> . Troubleshooting modules/lmod \u00b6 There are occasions where your local lmod cache may become corrupted, resulting in error messages such as /usr/bin/lua: <...> bad argument #1 to 'next' <...> or similar when you try to use module commands. The first thing you can try to resolve this issue is to delete your local lmod cache files rm ~/.cache/lmod/*.lua . When you next run a module command the cache files will be re-generated. In most cases this resolves module/lmod issues. If you continue to have issues after deleting the local cache, get in touch with support people via the R\u0101poi Slack channel.","title":"Preparing your Environment (modules)"},{"location":"environment/#enviroment-setup","text":"R\u0101poi has an extensive library of applications and software available. There are numerous programming languages and libraries (R, Julia, Python, lua, OpenMPI, blas, etc) as well as dozens of applications (Matlab, Gaussian, etc). We also keep older versions of software to ensure compatibility. Because of this, R\u0101poi developers use a tool called lmod to allow a user to load a specific version of an application, language or library and start using it for their work. The module command will show you what software is available to load, and will add the software to your environment for immediate use. Entering module help into the command prompt will give you some basic information about all of the module sub-commands. Here we briefly describe the most important ones.","title":"Enviroment Setup"},{"location":"environment/#searching-for-software-packages","text":"This section will give you a brief description on how to find which software packages are available and load them.","title":"Searching for software packages"},{"location":"environment/#searching-with-module-avail","text":"To show all software available to load type the following: module avail This is a long list of software packages which can each be loaded immediately (i.e. without first loading pre-requisites). Generally each software package is listed as a path of the form / , eg. lua/5.3.5 . The list is separated into a few sections (via lines of dashes). The section of most interest has the heading -------------------- /home/software/tools/eb_modulefiles/all/Core -------------------- The modules under this heading (or similar) are from the new module system (see New Module System for more extensive details). In contrast, the modules listed under the heading -------------------------- /home/software/tools/modulefiles -------------------------- are from the old module system. Beware that many of these older software packages in this section may no longer work. There may be other sections, depending on what modules you already have loaded. For example, if GCC/10.3.0 is loaded then you will see the additional sections ------------ /home/software/tools/eb_modulefiles/all/Compiler/GCC/10.3.0 ------------- Bio-SearchIO-hmmer/1.7.3 GEOS/3.9.1 Haploflow/1.0 OpenMPI/4.1.1 Boost/1.76.0 GSL/2.7 OpenBLAS/0.3.15 Subread/2.0.3 FlexiBLAS/3.0.4 ---------- /home/software/tools/eb_modulefiles/all/Compiler/GCCcore/10.3.0 ----------- Autoconf/2.71 ( D ) Python/3.9.5 ( D ) libevent/2.1.12 Automake/1.16.3 Qhull/2020.2 libfabric/1.12.1 Autotools/20210128 RE2/2022-02-01 libffi/3.3 BLIS/0.8.1 RapidJSON/1.1.0 libgeotiff/1.6.0 Bazel/3.7.2 Rust/1.52.1 libgit2/1.1.0 <etc. many more lines...> These sections show various additional modules for which GCC/10.3.0 is a pre-requisite. Many pieces of software have one or more pre-requisites in the new module system. A more convenient way to search for specific pieces of software, and determine what their pre-requisite modules are, is via the module spider command.","title":"Searching with module avail"},{"location":"environment/#searching-with-module-spider","text":"Suppose you want to find out which Python versions are available, you can search via the command module spider Python , for example: username@raapoi-login:~$ module spider Python -------------------------------------------------------------------------------------- Python: -------------------------------------------------------------------------------------- Description: Python is a programming language that lets you work more quickly and integrate your systems more effectively. Versions: Python/2.7.15-bare Python/2.7.15 Python/2.7.16 Python/2.7.18 <additional output not included here> Note that the capital P in Python is important here, the module spider command is case-sensitive. Capital P Python modules/packages are generally from the new module system, whereas lower case python modules/packages are older (and may no longer work). The same goes for many several other software packages such as R (versus r ) and OpenMPI (versus openmpi ). Among the list of Python versions is Python/3.9.6 . To find out how to load it we call module spider again with the specific Python version included, e.g. username@raapoi-login:~$ module spider Python/3.9.6 -------------------------------------------------------------------------------------- Python: Python/3.9.6 -------------------------------------------------------------------------------------- Description: Python is a programming language that lets you work more quickly and integrate your systems more effectively. You will need to load all module ( s ) on any one of the lines below before the \"Python/3.9.6\" module is available to load. GCCcore/11.2.0 <additional output not included here>","title":"Searching with module spider"},{"location":"environment/#loading-software-packages","text":"Loading a software package is done via the command module load <package name> where should generally include the version as well. For example, the output of module spider Python/3.9.6 (from above) tells us that in order to load this version of Python we must first load GCCcore/11.2.0 . That is, we can load this version of Python via username@raapoi-login:~$ module load GCCcore/11.2.0 username@raapoi-login:~$ module load Python/3.9.6 If we wanted to ensure we had a clean/minimal software environment, we could start with a purge (see below for details), that is username@raapoi-login:~$ module purge username@raapoi-login:~$ module load config GCCcore/11.2.0 username@raapoi-login:~$ module load Python/3.9.6 In either case, you will now be able to run Python version 3.9.6 by entering python at the command prompt. If you are intending to do some computation, you should call srun --pty python to start an interactive job on the quicktest node running Python.","title":"Loading software packages"},{"location":"environment/#other-useful-module-sub-commands","text":"","title":"Other useful module sub-commands"},{"location":"environment/#listing-loaded-modules","text":"To see what modules you have loaded into your environment you can run the command: module list By default you will have the config module loaded (which makes the vuw- commands available) and possibly several others. For example, here are the modules I have loaded in my environment when I login: username@raapoi-login:~$ module list Currently Loaded Modules: 1 ) autotools 3 ) gnu9/9.4.0 5 ) ucx/1.11.2 7 ) openmpi4/4.1.1 9 ) config 2 ) prun/2.2 4 ) hwloc/2.7.2 6 ) libfabric/1.13.0 8 ) ohpc You can also check if a specific module is loaded via the command module is-loaded <package name> .","title":"Listing loaded modules"},{"location":"environment/#the-module-whatis-and-module-show-commands","text":"If you want to know more about a particular module you can use the whatis or show subcommand. The output of module whatis will be more details for some modules than others. Here is one example: username@raapoi-login:~$ module whatis foss/2020b foss/2020b : Description: GNU Compiler Collection ( GCC ) based compiler toolchain, including OpenMPI for MPI support, OpenBLAS ( BLAS and LAPACK support ) , FFTW and ScaLAPACK. foss/2020b : Homepage: https://easybuild.readthedocs.io/en/master/Common-toolchains.html#foss-toolchain foss/2020b : URL: https://easybuild.readthedocs.io/en/master/Common-toolchains.html#foss-toolchain The output of module show shows the help information associated with the module/package as well as various steps which are executed when the module is loaded. username@raapoi-login:~$ module show GCC/10.3.0 ----------------------------------------------------------------------------------------- /home/software/tools/eb_modulefiles/all/Core/GCC/10.3.0.lua: ----------------------------------------------------------------------------------------- help ([[ Description =========== The GNU Compiler Collection includes front ends for C, C++, Objective-C, Fortran, Java, and Ada, as well as libraries for these languages ( libstdc++, libgcj,... ) . More information ================ - Homepage: https://gcc.gnu.org/ ]]) whatis ( \"Description: The GNU Compiler Collection includes front ends for C, C++, Objective-C, Fortran, Java, and Ada, as well as libraries for these languages (libstdc++, libgcj,...).\" ) whatis ( \"Homepage: https://gcc.gnu.org/\" ) whatis ( \"URL: https://gcc.gnu.org/\" ) conflict ( \"GCC\" ) load ( \"GCCcore/10.3.0\" ) load ( \"binutils/2.36.1\" ) prepend_path ( \"MODULEPATH\" , \"/home/software/tools/eb_modulefiles/all/Compiler/GCC/10.3.0\" ) setenv ( \"EBROOTGCC\" , \"/home/software/EasyBuild/software/GCCcore/10.3.0\" ) setenv ( \"EBVERSIONGCC\" , \"10.3.0\" ) setenv ( \"EBDEVELGCC\" , \"/home/software/EasyBuild/software/GCC/10.3.0/easybuild/Core-GCC-10.3.0-easybuild-devel\" ) Note that both the whatis and show commands will only work if you have loaded the pre-requisites of the module/package you are looking up.","title":"The module whatis and module show commands"},{"location":"environment/#the-module-keyword-command","text":"Using module keyword is another way to search through the module system. Specifically, module keyword blah will search through help messages and whatis descriptions to find any mention of blah . For example, if you know you want to do a search for lua , you can find lua packages and any packages which mention lua in their help/whatis via keyword subcommand: module keyword lua","title":"The module keyword command"},{"location":"environment/#clearingpurging-your-environment","text":"The command module purge will unload all of your modules. This can be quite useful to do as a first step before loading modules. It can help ensure you don't accidentally end up with conflicting versions of any modules (which generally gets taken care of by lmod, but can sometimes be an issue), and can help you work out a minimal environment that works for your desired software. If you execute module purge you will lose access to the vuw- commands, but you can reload them via module load config . Individual modules/packages can also be unloaded via module unload <package name> .","title":"Clearing/purging your environment"},{"location":"environment/#troubleshooting-moduleslmod","text":"There are occasions where your local lmod cache may become corrupted, resulting in error messages such as /usr/bin/lua: <...> bad argument #1 to 'next' <...> or similar when you try to use module commands. The first thing you can try to resolve this issue is to delete your local lmod cache files rm ~/.cache/lmod/*.lua . When you next run a module command the cache files will be re-generated. In most cases this resolves module/lmod issues. If you continue to have issues after deleting the local cache, get in touch with support people via the R\u0101poi Slack channel.","title":"Troubleshooting modules/lmod"},{"location":"examples/","text":"Examples \u00b6 Simple Bash Example - start here if new to HPC \u00b6 In this example we will run a very simple bash script on the quicktest partition. The bash script is very simple, it just prints the hostname - the node you're running on - and prints the date into a file. It also sleeps for 1 minute - it just does this to give you a chance to see your job in the queue with squeue First lets create a sensible working directory mkdir bash_example cd bash_example We'll use the text editor nano to create our bash script as well as our submission script. In real life, you might find it easier to create your code and submission script on your local machine, then copy them over as nano is not a great editor for large projects. Create and edit our simple bash script - this is our code we will run on the HPC nano test.sh Paste or type the following into the file #!/bin/bash hostname #prints the host name to the terminal date > date_when_job_ran.txt #puts the content of the date command into a txt file sleep 1m # do nothing for 1 minute. Job will still be \"running\" press ctrl-O to save the text in nano, then ctrl-X to exit nano. Using nano again create a file called submit.sh with the following content #!/bin/bash # #SBATCH --job-name=bash_test #SBATCH -o bash_test.out #SBATCH -e bash_test.err # #SBATCH --partition=quicktest # #SBATCH --cpus-per-task=2 #Note: you are always allocated an even number of cpus #SBATCH --mem=1G #SBATCH --time=10:00 bash test.sh #actually run our bash script, using bash If you're familiar with bash scripts, the above is a bit weird. The #SBATCH lines would normally be comments and hence not do anything, but Slurm will read those lines to determine how many resources to provide your job. In this case we ask for the following: quicktest partition (the default - so you don't technically need to ask for it). 1 cpu per task - we have one task, so we're asking for 1 cpu 1 gig of memory. a max runtime of 10 min If your job uses more memory or time than requested, Slurm will immediately kill it. If you use more CPU's than requested - your job will keep running, but your \"cpus\" will be shared bewteen the CPUs you actually requested. So if your job tried to use 10 CPUs but you only asked for one, it'll run extremely slowly - don't do this. Our submit.sh script also names our job bash_test this is what the job will show up as in squeue. We ask for things printed out on the terminal to go to two seperate files. Normal, non error, things that would be printed out on the terminal will be put into the text file bash_test.out . Errors will be printed into the text file bash_test.err Now submit your job to the Slurm queue. sbatch submit.sh #See your job in the queue squeue -u <your_username> #When job is done see the new files ls #look at the content that would have been printed to the terminal if running locally cat bash_test.out # See the content of the file that your bash script created cat date_when_job_ran.txt Python users guide \u00b6 Which versions of Python are working on R\u0101poi? \u00b6 There are a number of versions of Python on R\u0101poi, although many of these are old installations (prior to an OS update and changes to the module system) and may no longer work. Generally speaking, your best bet is to try a version which appears when you search via module spider Python (noting that the capital 'P' in Python is important here). A few examples of relatively recent version of Python which are available (as of April 2024) are Python/3.9.5 , Python/3.10.8 and Python/3.11.5 . Each of these Python modules has one or more of pre-requisite modules that need to be loaded first (generally a specific version of GCC compilers). To find out what you need to load first for a specific version of Python you just need to check the output of module spider Python/x.y.z (with the appropriate values for x,y,z). One of the examples below shows how to use Python/3.9.5 . In cases where your Python code needs to interact with software from another module which also requires a specific GCC module, that will dictate which version of Python to load (i.e. whichever one depends on the same GCC version). Otherwise, you are free to use any desired Python module. The Python installations generally have a minimal number of packages/libraries installed. If you require additional packages/libraries it is recommended to create a virtual environment and install any desired packages within that environment. This is illustrated in the examples below using both virtualenv/pip and anaconda/conda. See also: Using Jupyter Notebooks Simple Python program using virtualenv and pip \u00b6 First we need to create a working directory and move there mkdir python_test cd python_test Next we load the python 3 module and use python 3 to create a python virtualenv. This way we can install pip packages which are not installed on the cluster module load GCCcore/10.3.0 module load Python/3.9.5 python3 -m venv mytest Activate the mytest virtualenv and use pip to install the webcolors package source mytest/bin/activate pip install webcolors Create the file test.py with the following contents using nano import webcolors from random import randint from socket import gethostname colour_list = list ( webcolors . CSS3_HEX_TO_NAMES . items ()) requested_colour = randint ( 0 , len ( colour_list )) colour_name = colour_list [ requested_colour ][ 1 ] print ( \"Random colour name:\" , colour_name , \" on host: \" , gethostname ()) Alternatively download it with wget: wget https://raw.githubusercontent.com/ \\ vuw-research-computing/raapoi-tools/ \\ master/examples/python_venv/test.py Using nano create the submissions script called python_submit.sh with the following content - change me@email.com to your email address. #!/bin/bash # #SBATCH --job-name=python_test #SBATCH -o python_test.out #SBATCH -e python_test.err # #SBATCH --cpus-per-task=2 #Note: you are always allocated an even number of cpus #SBATCH --mem=1G #SBATCH --time=10:00 # #SBATCH --mail-type=BEGIN,END,FAIL #SBATCH --mail-user=me@email.com module load GCCcore/10.3.0 module load Python/3.9.5 source mytest/bin/activate python test.py Alternatively download it with wget wget https://raw.githubusercontent.com/ \\ vuw-research-computing/raapoi-tools/ \\ master/examples/python_venv/python_submit.sh To submit your job to the Slurm scheduler sbatch python_submit.sh Check for your job on the queue with squeue though it might finish very fast. The output files will appear in your working directory. Using Anaconda/Miniconda/conda \u00b6 Many users use Anaconda/Miniconda to manage software stacks. One way to do this is to use singularity containers with the conda environment inside - this allows the conda environment to load quickly as the many small conda files are inside a container which the file system sees as one file. However, this is also an additional bit of complexity so many users just use conda outside of singularity. You can install your own version of Anaconda/Miniconda to your home directory or scratch. We have also got packaged versions of Anaconda/Miniconda installed with our module loading system. Anaconda has many built in packages so we will use that in our examples, but Miniconda is also available if prefer to start from a minimal initial setup. module load Anaconda3/2020.11 export PIP_NO_CACHE_DIR = 1 export PYTHONNOUSERSITE = 1 Note Setting the variables PIP_NO_CACHE_DIR and PYTHONNOUSERSITE prevents conda from trying to use the system python and pip. This ensures an isolated environment and avoids potential conflicts with system packages. Let's create a new conda environment for this example, in a sensible location, I used ~/examples/conda/idba conda create --name idba-example # press y for the Proceed prompt if it looks correct source $( conda info --base ) /etc/profile.d/conda.sh conda activate idba-example #activate our example environment. Warning On HPC systems, conda init is not recommended as it modifies your shell configuration files. This can cause problems with the module system and other software. Instead, use the source $(conda info --base)/etc/profile.d/conda.sh command to activate conda in your current shell session. Conda environments are beyond the scope of this example, but they are a good way to contain all the dependencies and programs for a particular workflow, in this case, idba. Install idba in our conda environment. Tip Note that best practise is to do the install on a compute node We'll just do it here on the login node for now - the code will run slower on the compute nodes as a result! conda install -c bioconda idba Idba is a genome assembler, we will use paired-end illumina reads of E. coli. The data is available on an Amazon S3 bucket (a cloud storage location), and we can download it using wget. mkdir data # put our data in a sensible location cd data wget --content-disposition goo.gl/JDJTaz #sequence data wget --content-disposition goo.gl/tt9fsn #sequence data cd .. #back to our project directory The reads we have are paired-end fastq files but idba requires a fasta file. We can use a tool installed with idba to convert them. We'll do this on the R\u0101poi login node as it is a fast task that doesn't need many resources. fq2fa --merge --filter data/MiSeq_Ecoli_MG1655_50x_R1.fastq data/MiSeq_Ecoli_MG1655_50x_R2.fastq data/read.fa To create our submission script we need to know the path to our conda enviroment. To get this: conda env list You'll need to find your idba-example environment, and next to it is the path you'll need for your submission script. In my case: # conda environments: # base * /home/username/anaconda3 idba-example /home/username/anaconda3/envs/idba-example # We need this line, it'll be different for you! Create our sbatch submission script. Note that this sequence doesn't need a lot of memory, so we'll use 3G. To see your usage after the job has run use vuw-job-report <job-id> idba_submit.sh #!/bin/bash #SBATCH --job-name=idba_test #SBATCH -o _output.out #SBATCH -e _output.err #SBATCH --time=00:5:00 #SBATCH --partition=quicktest #SBATCH --ntasks=12 #SBATCH --mem=3G module load Anaconda3/2020.11 eval \" $( conda shell.bash hook ) \" # basically inits your conda - prevents errors like: CommandNotFoundError: Your shell has not been properly configured ... conda activate /home/username/anaconda3/envs/idba-example # We will need to activate our conda enviroment on the remote node idba idba_ud -r data/read.fa -o output To submit our job sbatch idba_submit.sh To see our job running or queuing squeue -u $USER This job will take a few minutes to run, generally less than 5. When the job is done we can see the output in the output folder. We can also see the std output and std err in the files _output.out and _output.err . The quickest way to examine them is to cat the files when the run is done. cat _output.out R users guide \u00b6 Which versions of R are working on R\u0101poi? \u00b6 There are a number of versions of R on R\u0101poi, although many of these are old installations (prior to an OS update and changes to the module system) and no longer work. There are three relatively recent versions of R which currently work (as of April 2024), these are R/3.6.3 , R/4.1.0 and R/4.2.0 . Each of these modules has a couple of pre-requisite modules that need to be loaded prior to loading the R module. To find out what you need to load first you just need to check the output of module spider R/x.y.z (with the appropriate values for x,y,z). The following example shows how to use R/4.2.0 . Loading R packages & running a simple job \u00b6 First login to R\u0101poi and load the R module: module purge # clean/reset your environment module load config # reload utilities such as vuw-job-report module load GCC/11.2.0 OpenMPI/4.1.1 # pre-requisites for the new R module module load R/4.2.0 Then run R on the command line: R Test library existence: > library ( tidyverse ) This should load the package, and give some output like this: \u2500\u2500 Attaching packages \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 tidyverse 1.3 . 1 \u2500\u2500 \u2714 ggplot2 3.3 . 5 \u2714 purrr 0.3 . 4 \u2714 tibble 3.1 . 6 \u2714 dplyr 1.0 . 8 \u2714 tidyr 1.2 . 0 \u2714 stringr 1.4 . 0 \u2714 readr 2.1 . 2 \u2714 forcats 0.5 . 1 \u2500\u2500 Conflicts \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 tidyverse_conflicts () \u2500\u2500 \u2716 dplyr :: filter () masks stats :: filter () \u2716 dplyr :: lag () masks stats :: lag () (These conflicts are normal and can be ignored.) To quit R, type: > q () Next create a bash submission script called r_submit.sh (or another name of your choice) using your preferred text editor, e.g. nano. #!/bin/bash # #SBATCH --job-name=r_test #SBATCH -o r_test.out #SBATCH -e r_test.err # #SBATCH --cpus-per-task=2 #Note: you are always allocated an even number of cpus #SBATCH --mem=1G #SBATCH --time=10:00 module purge module load GCC/11.2.0 OpenMPI/4.1.1 module load R/4.2.0 Rscript mytest.R Save this to the current working directory, and then create another file using your preferred text editor called mytest.R (or another name of your choice) containing the following R commands: library ( tidyverse ) sprintf ( \"Hello World!\" ) then run it with the previously written bash script: sbatch r_submit.sh This submits a task that should execute quickly and create files in the directory from which it was run. Examine r_test.out . You can use an editor like nano, vi or emacs, or you can just cat or less the file to see its contents on the terminal. You should see: \"Hello World\" Installing additional R packages/extensions in your local user directory \u00b6 If you are in need of additional R packages which are not included in the R installation, you may intall them into your user directory. Start by launching an R session module purge module load GCC/11.2.0 OpenMPI/4.1.1 module load R/4.2.0 R Then, supposing you want to install a package from CRAN named \"A3\". If this is the first time you are attempting to install local packages for this R version then the steps look something like this. > library ( A3 ) # confirm that foo is not already available > install.packages ( 'A3' ) Warning in install.packages ( \"A3\" ) : 'lib = \"/home/software/EasyBuild/software/R/4.2.0-foss-2021b/lib64/R/library\"' is not writable Would you like to use a personal library instead ? ( yes / No / cancel ) yes Would you like to create a personal library \u2018 / nfs / home /< username >/ R / x86_64 - pc - linux - gnu - library / 4.2 \u2019 to install packages into ? ( yes / No / cancel ) yes --- Please select a CRAN mirror for use in this session --- Secure CRAN mirrors < long list of mirrors , the NZ mirror was number 54 in my list > Selection : 54 trying URL 'https://cran.stat.auckland.ac.nz/src/contrib/A3_1.0.0.tar.gz' < additional output from the installation steps... > In future, when you run this version of R, it should automatically check the local user directory created above for installed packages. Any other packages you install in future should automatically go into this directory as well (assuming you don't play around with .libPaths() ). Matlab GPU example \u00b6 Matlab has various built-in routines which are GPU accelerated. We will run a simple speed comparison between cpu and gpu tasks. In a sensible location create a file called matlab_gpu.m I used ~/examples/matlab/cuda/matlab_gpu.m . % Set an array which will calculate the Eigenvalues of A = rand ( 1000 ); % Copy the Array to the GPU memory - this process takes an erratic amount of time, so we will not time it. Agpu = gpuArray ( A ); tic B=eig(Agpu) ; t1 = toc % Let's compare the time with CPU tic B = eig ( A ); t2 = toc We will also need a Slurm submission script; we'll call this matlab_gpu.sh . Note that we will need to use the new Easybuild module files for our cuda libraries, so make sure to include the module use line module use /home/software/tools/eb_modulefiles/all/Core #!/bin/bash #SBATCH --job-name=matlab-gpu-example #SBATCH --output=out-gpu-example.out #SBATCH --error=out-gpu-example.err #SBATCH --time=00:05:00 #SBATCH --partition=gpu #SBATCH --gres=gpu:1 #SBATCH --ntasks=2 #SBATCH --mem=60G module use /home/software/tools/eb_modulefiles/all/Core module load MATLAB/2024a module load fosscuda/2020b matlab -nodisplay -nosplash -nodesktop -r \"run('matlab_gpu.m');exit;\" To submit this job to the Slurm queue sbatch matlab_gpu.sh . This job will take a few minutes to run - this is mostly the Matlab startup time. Examine the queue for your job squeue -u $USER . When your job is done, inspect the output file. You can use an editor like nano, vi or emacs, or you can just cat or less the file to see its contents on the terminal. cat out-gpu-example.out What do you notice about the output? Surely GPUs should be faster than the CPU! It takes time for the GPU to start processing your task, the CPU is able to start the task far more quickly. So for short operations, the CPU can be faster than the GPU - remember to benchmark your code for optimal performance! Just because you can use a GPU for your task doesn't mean it is necessarily faster! To get a better idea of the advantage of the GPU let's increase the size of the array from 1000 to 10000 matlab_gpu.m % Set an array which will calculate the Eigenvalues of A = rand ( 10000 ); % Copy the Array to the GPU memory - this process takes an erratic amount of time, so we will not time it. Agpu = gpuArray ( A ); tic B=eig(Agpu) ; t1 = toc % Let's compare the time with CPU tic B = eig ( A ); t2 = toc To make things fairer for the CPU in this case, we will also allocate half the CPUs on the node to Matlab. Half the CPUs, half the memory and half the GPUs, just to be fair. matlab_gpu.sh #!/bin/bash #SBATCH --job-name=matlab-gpu-example #SBATCH --output=out-gpu-example.out #SBATCH --error=out-gpu-example.err #SBATCH --time=00:05:00 #SBATCH --partition=gpu #SBATCH --gres=gpu:1 #SBATCH --ntasks=128 #SBATCH --mem=256G module use /home/software/tools/eb_modulefiles/all/Core module load MATLAB/2024a module load fosscuda/2020b matlab -nodisplay -nosplash -nodesktop -r \"run('matlab_gpu.m');exit;\" The output in my case was: < M A T L A B ( R ) > Copyright 1984 -2024 The MathWorks, Inc. R2024a ( 24 .1.0.2537033 ) 64 -bit ( glnxa64 ) February 21 , 2024 For online documentation, see https://www.mathworks.com/support For product information, visit www.mathworks.com. t1 = 62 .0212 t2 = 223 .0818 So in thise case the GPU was considerably faster. Matlab can do this a bit faster on the CPU if you give it fewer CPUs, the optimum appears to be around 20, but it still takes 177s. Again, optimise your resource requests for your problem, less can sometimes be more, however the GPU easily wins in this case. Job Arrays - running many similar jobs \u00b6 Slurm makes it easy to run many jobs which are similar to each other. This could be one piece of code running over many datasets in parallel or running a set of simulations with a different set of parameters for each run. Simple Bash Job Array example \u00b6 The following code will run the submission script 16 times as resources become available (i.e. they will not neccesarily run at the same time). It will just print out the Slurm array task ID and exit. submit.sh: #!/bin/bash #SBATCH --job-name=test_array #SBATCH --output=out_array_%A_%a.out #SBATCH --error=out_array_%A_%a.err #SBATCH --array=1-16 #SBATCH --time=00:00:20 #SBATCH --partition=parallel #SBATCH --ntasks=1 #SBATCH --mem=1G # Print the task id. echo \"My SLURM_ARRAY_TASK_ID: \" $SLURM_ARRAY_TASK_ID # Add lines here to run your computations. Run the example with the standard sbatch submit.sh A simple R job Array Example \u00b6 As a slightly more practical example the following will run an R script 5 times as resources become available. The R script takes as an input the $SLURM_ARRAY_TASK_ID which then selects a parameter alpha out of a lookup table. This is one way you could run simulations or similar with a set parameters defined in a lookuop table in your code. To make outputs more tidy and to help organisation, instead of dumping all the outputs into the directory with our code and submission script, we will separate the outputs into directories. Dataframes saved from R will be saved to the output/ directory, and all output which would otherwise be printed to the commnd line (stdout and stderr) will be saved to the stdout/ directory. Both of these directories will need to be created before running the script. r_random_alpha.R: # get the arguments supplied to R. # trailingOnly = TRUE gets the user supplied # arguments, and for now we will only get the # first user supplied argument args <- commandArgs ( trailingOnly = TRUE ) inputparam <- args [ 1 ] # a vector with all our parameters. alpha_vec <- c ( 2.5 , 3.3 , 5.1 , 8.2 , 10.9 ) alpha <- alpha_vec [ as.integer ( inputparam )] # Generate a random number between 0 and alpha # store it in dataframe with the coresponding # alpha value randomnum <- runif ( 1 , min = 0 , max = as.double ( alpha )) df <- data.frame ( \"alpha\" = alpha , \"random_num\" = randomnum ) # Save the data frame to a file with the alpha value # Note that the output/ folder will need to be # manually created first! outputname <- paste ( \"output/\" , \"alpha_\" , alpha , \".Rda\" , sep = \"\" ) save ( df , file = outputname ) Next create the submision script. Which we will run on the parallel partition rather than quicktest. r_submit.sh: #!/bin/bash #SBATCH --job-name=test_R_array #SBATCH --output=stdout/array_%A_%a.out #SBATCH --error=stdout/array_%A_%a.err #SBATCH --array=1-5 #SBATCH --time=00:00:20 #SBATCH --partition=parallel #SBATCH --ntasks=1 #SBATCH --mem=1G module purge module load GCC/11.2.0 OpenMPI/4.1.1 module load R/4.2.0 # Print the task id. Rscript r_random_alpha.R $SLURM_ARRAY_TASK_ID Run the jobs with sbatch r_submit.sh Singularity \u00b6 While there are many modules on R\u0101poi, sometimes you might want to install your own packages in your own way. Singularity allows you to do this. If you are familiar with Docker, Singularity is similar, except you can't get root (or sudo) once your container is running on the R\u0101poi. However, you can have sudo rights locally on your own machine, setup your container however you like, then run it without sudo on the cluster. Note As per updated singularity configuration, mount hostfs is set to false and mount home is set to true . As a result, filesystems available inside a singularity session by default are: devtmpfs /dev tmpfs /dev/shm /vg<id> /etc/hosts .. /tmp .. /var/tmp .. /etc/group $HOME /nfs/home/$USER This means any other file system such as nfs should be manually specified using --bind command. For example, singularity shell --bind /nfs/scratch/$USER:/nfs/scratch/$USER image_file.simg pwd See also: Using containers Singularity/Docker container example \u00b6 Singularity allows you to use most (but not all!) docker images on R\u0101poi. On your local machine create the singularity definition file input_args_example.def BootStrap : library From : ubuntu: 16.04 %runscript exec echo \" $@ \" %labels Author JaneDoe This will build an ubuntu 16.04 container that will eventually run on R\u0101poi which runs Centos. This container has a runscript which just echos back any arguments sent to the container when your start it up. Build the container locally with sudo and singularity sudo singularity build inputexample.sif input_args_example.def OR Using Sylabs Remote Builder is another option to build containers remotely. A Sylabs account and access token are required to use this feature. To build the container remotely, use the following command: singularity build --remote inputexample.sif input_args_example.def This will build an image that you can't modify any further and is immediately suitable to run on R\u0101poi Copy this file to R\u0101poi via sftp sftp <username>@raapoi.vuw.ac.nz Create a submit script using singularity on the cluster singularity_submit.sh #!/bin/bash #SBATCH --job-name=singularity_test #SBATCH -o sing_test.out #SBATCH -e sing_test.err #SBATCH --time=00:00:20 #SBATCH --ntasks=1 #SBATCH --mem=1G module load singularity singularity run inputtest.sif \"hello from a container\" Run the script with the usual singularity_submit.sh Singularity/TensorFlow Example \u00b6 tensor.def Bootstrap: docker From: tensorflow/tensorflow:latest-py3 %post apt-get update && apt-get -y install wget build-essential %runscript exec python \" $@ \" compile this locally with sudo and singularity. sudo singularity build tensorflow.sif tensor.def Create a quick tensorflow test code tensortest.py import tensorflow as tf mnist = tf . keras . datasets . mnist ( x_train , y_train ),( x_test , y_test ) = mnist . load_data () x_train , x_test = x_train / 255.0 , x_test / 255.0 model = tf . keras . models . Sequential ([ tf . keras . layers . Flatten ( input_shape = ( 28 , 28 )), tf . keras . layers . Dense ( 512 , activation = tf . nn . relu ), tf . keras . layers . Dropout ( 0.2 ), tf . keras . layers . Dense ( 10 , activation = tf . nn . softmax ) ]) model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) model . fit ( x_train , y_train , epochs = 5 ) model . evaluate ( x_test , y_test ) Copy your files to R\u0101poi via sftp (or whatever you prefer) sftp <username>@raapoi.vuw.ac.nz cd <where you want to work> put * #put all files in your local directory onto R\u0101poi Lets quickly test the code via an interactive session on a node. Note I find the tensorflow container only runs properly on intel nodes, which we don't have many of at the moment, I'll investigate this further. srun --partition = \"parallel\" --constraint = \"Intel\" --pty bash #now on the remote node - note you might need to wait if nodes are busy module load singularity #load singularity singularity shell tensorflow.sif #now inside the tensorflow container on the remote node python tensortest.py #once that runs, exit the container exit #exit the container exit #exit the interactive session on the node Create a submit script using singularity on the cluster singularity_submit.sh #!/bin/bash #SBATCH --job-name=singularity_test #SBATCH -o sing_test.out #SBATCH -e sing_test.err #SBATCH --time=00:10:00 #SBATCH --partition=parallel #SBATCH --constraint=Intel #SBATCH --ntasks=1 #SBATCH --mem=4G module load singularity #run the container with the runscript defined when we created it singularity run tensorflow.sif tensortest.py Singularity/MaxBin2 Example \u00b6 In a sensible location, either in your home directory or on the scratch: Get the maxbin2 container, there are a few places to get this, but will get the bioconda container as it is more recent than the one referenced on the official maxbin site. module load module load singularity singularity pull docker://quay.io/biocontainers/maxbin2:2.2.6--h14c3975_0 mv maxbin2_2.2.6--h14c3975_0.sif maxbin2_2.2.6.sif #rename for convenience Download some test data mkdir rawdata curl https://downloads.jbei.org/data/microbial_communities/MaxBin/getfile.php?20x.scaffold > rawdata/20x.scaffold curl https://downloads.jbei.org/data/microbial_communities/MaxBin/getfile.php?20x.abund > rawdata/20x.abund Create an output data location mkdir output Create a submit script using singularity on the cluster singularity_submit.sh #!/bin/bash #SBATCH --job-name=maxbin2_test #SBATCH -o sing_test.out #SBATCH -e sing_test.err #SBATCH --time=00:10:00 #SBATCH --partition=parallel #SBATCH --ntasks=4 #SBATCH --mem=4G module load singularity singularity exec maxbin2_2.2.6.sif run_MaxBin.pl -contig rawdata/20x.scaffold -abund rawdata/20x.abund -out output/20x.out -thread 4 Singularity/Sandbox Example \u00b6 This lets you have root inside a container locally and make changes to it. This is really handy for determining how to setuop your container. While you can convert the sandbox container to one you can run on R\u0101poi, I suggest you don't do this . Use the sandbox to figure out how you need to configure your container, what packages to install, config files to change etc. Then create a .def file that contains all the nessesary steps without the need to use the sandbox - this will make your work more reproducable and easier to share with others. example.def BootStrap: library From: ubuntu:16.04 %post apt-get update && apt-get -y install wget build-essential %runscript exec echo \" $@ \" %labels Author JaneDoe Compile this locally with sudo and singularity. We are using the sandbox flag to create a writable container directory ( example/ ) on our local machine where we have sudo rights. sudo singularity build --sandbox example/ example.def Now we can run the container we just built, but with sudo rights inside the container. Your rights outside the container match the rights inside the container, so we need to do this with sudo. sudo singularity shell --writable example/ Inside the container we now have root and can install packages and modify files in the root directories Singularity example:~> apt update Singularity example:~> apt install sqlite Singularity example:~> touch /test.txt #create an empty file in root Singularity example:~> ls / Singularity example:~> exit #exit container To run the container on R\u0101poi we convert it to the default immutable image with build. We might need sudo for this as the prior use of sudo will have created a directory that your usual user can't see every file. sudo singularity build new-example-sif example/ You could now copy the new-example-sif file to R\u0101poi and run it there. However a better workflow is to use this to experiment, to find out what changes you need to make to the image and what packages you need to install. Once you've done that, I suggest starting afresh and putting everything in the.def file . That way when you return to your project in 6 months, or hand it over to someone else, there is a clear record of how the image was built. Singularity/Custom Conda Container - idba example \u00b6 In this example we'll build a singularity container using conda. The example is building a container for idba - a genome assembler. Idba is available in bioconda, but not as a biocontainer. We'll build this container locally to match a local conda environment, then run it on the HPC and do an example assembly. Locally \u00b6 Make sure you have conda setup on your local machine, anaconda and miniconda are good choices. Create a new conda environment and install idba conda create --name idba conda install -c bioconda idba Export your conda environment, we will use this to build the container. conda env export > environment.yml We will use a singularity definition, basing our build on a docker miniconda image. There is a bunch of stuff in this file to make sure the conda environment is in the path. From stackoverflow idba.def Bootstrap: docker From: continuumio/miniconda3 %files environment.yml %environment PATH=/opt/conda/envs/$(head -1 environment.yml | cut -d' ' -f2)/bin:$PATH %post echo \". /opt/conda/etc/profile.d/conda.sh\" >> ~/.bashrc echo \"source activate $(head -1 environment.yml | cut -d' ' -f2)\" > ~/.bashrc /opt/conda/bin/conda env create -f environment.yml %runscript exec \"$@\" Build the image sudo singularity build idba.img idba.def Now copy the idba.img and environment.yml (technically the environment file is not needed, but not having it creates a warning) to somewhere sensible on R\u0101poi. On R\u0101poi \u00b6 Create a data directory, so we can separate our inputs and outputs. Download a paired end illumina read of Ecoli from S3 with wget. The data comes from the Illumina public data library mkdir data cd data wget --content-disposition goo.gl/JDJTaz #sequence data wget --content-disposition goo.gl/tt9fsn #sequence data cd .. #back to our project directory The reads we have are paired end fastq files but idba requires a fasta file. We can use a tool built into our container to convert them. We'll do this on the R\u0101poi login node as it is a fast task that doesn't need many resources. module load singularity singularity exec fq2fa --merge --filter data/MiSeq_Ecoli_MG1655_50x_R1.fastq data/MiSeq_Ecoli_MG1655_50x_R2.fastq data/read.fa Create our sbatch submission script. Note that this sequence doesn't need a lot of memory, so we'll use 1G. To see your usage after the job has run use vuw-job-report <job-id> idba_submit.sh #!/bin/bash #SBATCH --job-name=idba_test #SBATCH -o output.out #SBATCH -e output.err #SBATCH --time=00:10:00 #SBATCH --partition=quicktest #SBATCH --ntasks=12 #SBATCH --mem=1G module load singularity singularity exec idba.img idba idba_ud -r data/read.fa -o output Now we can submit our script to the queue with sbatch idba_submit.sh","title":"Examples and User Guides"},{"location":"examples/#examples","text":"","title":"Examples"},{"location":"examples/#simple-bash-example-start-here-if-new-to-hpc","text":"In this example we will run a very simple bash script on the quicktest partition. The bash script is very simple, it just prints the hostname - the node you're running on - and prints the date into a file. It also sleeps for 1 minute - it just does this to give you a chance to see your job in the queue with squeue First lets create a sensible working directory mkdir bash_example cd bash_example We'll use the text editor nano to create our bash script as well as our submission script. In real life, you might find it easier to create your code and submission script on your local machine, then copy them over as nano is not a great editor for large projects. Create and edit our simple bash script - this is our code we will run on the HPC nano test.sh Paste or type the following into the file #!/bin/bash hostname #prints the host name to the terminal date > date_when_job_ran.txt #puts the content of the date command into a txt file sleep 1m # do nothing for 1 minute. Job will still be \"running\" press ctrl-O to save the text in nano, then ctrl-X to exit nano. Using nano again create a file called submit.sh with the following content #!/bin/bash # #SBATCH --job-name=bash_test #SBATCH -o bash_test.out #SBATCH -e bash_test.err # #SBATCH --partition=quicktest # #SBATCH --cpus-per-task=2 #Note: you are always allocated an even number of cpus #SBATCH --mem=1G #SBATCH --time=10:00 bash test.sh #actually run our bash script, using bash If you're familiar with bash scripts, the above is a bit weird. The #SBATCH lines would normally be comments and hence not do anything, but Slurm will read those lines to determine how many resources to provide your job. In this case we ask for the following: quicktest partition (the default - so you don't technically need to ask for it). 1 cpu per task - we have one task, so we're asking for 1 cpu 1 gig of memory. a max runtime of 10 min If your job uses more memory or time than requested, Slurm will immediately kill it. If you use more CPU's than requested - your job will keep running, but your \"cpus\" will be shared bewteen the CPUs you actually requested. So if your job tried to use 10 CPUs but you only asked for one, it'll run extremely slowly - don't do this. Our submit.sh script also names our job bash_test this is what the job will show up as in squeue. We ask for things printed out on the terminal to go to two seperate files. Normal, non error, things that would be printed out on the terminal will be put into the text file bash_test.out . Errors will be printed into the text file bash_test.err Now submit your job to the Slurm queue. sbatch submit.sh #See your job in the queue squeue -u <your_username> #When job is done see the new files ls #look at the content that would have been printed to the terminal if running locally cat bash_test.out # See the content of the file that your bash script created cat date_when_job_ran.txt","title":"Simple Bash Example - start here if new to HPC"},{"location":"examples/#python-users-guide","text":"","title":"Python users guide"},{"location":"examples/#which-versions-of-python-are-working-on-rapoi","text":"There are a number of versions of Python on R\u0101poi, although many of these are old installations (prior to an OS update and changes to the module system) and may no longer work. Generally speaking, your best bet is to try a version which appears when you search via module spider Python (noting that the capital 'P' in Python is important here). A few examples of relatively recent version of Python which are available (as of April 2024) are Python/3.9.5 , Python/3.10.8 and Python/3.11.5 . Each of these Python modules has one or more of pre-requisite modules that need to be loaded first (generally a specific version of GCC compilers). To find out what you need to load first for a specific version of Python you just need to check the output of module spider Python/x.y.z (with the appropriate values for x,y,z). One of the examples below shows how to use Python/3.9.5 . In cases where your Python code needs to interact with software from another module which also requires a specific GCC module, that will dictate which version of Python to load (i.e. whichever one depends on the same GCC version). Otherwise, you are free to use any desired Python module. The Python installations generally have a minimal number of packages/libraries installed. If you require additional packages/libraries it is recommended to create a virtual environment and install any desired packages within that environment. This is illustrated in the examples below using both virtualenv/pip and anaconda/conda. See also: Using Jupyter Notebooks","title":"Which versions of Python are working on R\u0101poi?"},{"location":"examples/#simple-python-program-using-virtualenv-and-pip","text":"First we need to create a working directory and move there mkdir python_test cd python_test Next we load the python 3 module and use python 3 to create a python virtualenv. This way we can install pip packages which are not installed on the cluster module load GCCcore/10.3.0 module load Python/3.9.5 python3 -m venv mytest Activate the mytest virtualenv and use pip to install the webcolors package source mytest/bin/activate pip install webcolors Create the file test.py with the following contents using nano import webcolors from random import randint from socket import gethostname colour_list = list ( webcolors . CSS3_HEX_TO_NAMES . items ()) requested_colour = randint ( 0 , len ( colour_list )) colour_name = colour_list [ requested_colour ][ 1 ] print ( \"Random colour name:\" , colour_name , \" on host: \" , gethostname ()) Alternatively download it with wget: wget https://raw.githubusercontent.com/ \\ vuw-research-computing/raapoi-tools/ \\ master/examples/python_venv/test.py Using nano create the submissions script called python_submit.sh with the following content - change me@email.com to your email address. #!/bin/bash # #SBATCH --job-name=python_test #SBATCH -o python_test.out #SBATCH -e python_test.err # #SBATCH --cpus-per-task=2 #Note: you are always allocated an even number of cpus #SBATCH --mem=1G #SBATCH --time=10:00 # #SBATCH --mail-type=BEGIN,END,FAIL #SBATCH --mail-user=me@email.com module load GCCcore/10.3.0 module load Python/3.9.5 source mytest/bin/activate python test.py Alternatively download it with wget wget https://raw.githubusercontent.com/ \\ vuw-research-computing/raapoi-tools/ \\ master/examples/python_venv/python_submit.sh To submit your job to the Slurm scheduler sbatch python_submit.sh Check for your job on the queue with squeue though it might finish very fast. The output files will appear in your working directory.","title":"Simple Python program using virtualenv and pip"},{"location":"examples/#using-anacondaminicondaconda","text":"Many users use Anaconda/Miniconda to manage software stacks. One way to do this is to use singularity containers with the conda environment inside - this allows the conda environment to load quickly as the many small conda files are inside a container which the file system sees as one file. However, this is also an additional bit of complexity so many users just use conda outside of singularity. You can install your own version of Anaconda/Miniconda to your home directory or scratch. We have also got packaged versions of Anaconda/Miniconda installed with our module loading system. Anaconda has many built in packages so we will use that in our examples, but Miniconda is also available if prefer to start from a minimal initial setup. module load Anaconda3/2020.11 export PIP_NO_CACHE_DIR = 1 export PYTHONNOUSERSITE = 1 Note Setting the variables PIP_NO_CACHE_DIR and PYTHONNOUSERSITE prevents conda from trying to use the system python and pip. This ensures an isolated environment and avoids potential conflicts with system packages. Let's create a new conda environment for this example, in a sensible location, I used ~/examples/conda/idba conda create --name idba-example # press y for the Proceed prompt if it looks correct source $( conda info --base ) /etc/profile.d/conda.sh conda activate idba-example #activate our example environment. Warning On HPC systems, conda init is not recommended as it modifies your shell configuration files. This can cause problems with the module system and other software. Instead, use the source $(conda info --base)/etc/profile.d/conda.sh command to activate conda in your current shell session. Conda environments are beyond the scope of this example, but they are a good way to contain all the dependencies and programs for a particular workflow, in this case, idba. Install idba in our conda environment. Tip Note that best practise is to do the install on a compute node We'll just do it here on the login node for now - the code will run slower on the compute nodes as a result! conda install -c bioconda idba Idba is a genome assembler, we will use paired-end illumina reads of E. coli. The data is available on an Amazon S3 bucket (a cloud storage location), and we can download it using wget. mkdir data # put our data in a sensible location cd data wget --content-disposition goo.gl/JDJTaz #sequence data wget --content-disposition goo.gl/tt9fsn #sequence data cd .. #back to our project directory The reads we have are paired-end fastq files but idba requires a fasta file. We can use a tool installed with idba to convert them. We'll do this on the R\u0101poi login node as it is a fast task that doesn't need many resources. fq2fa --merge --filter data/MiSeq_Ecoli_MG1655_50x_R1.fastq data/MiSeq_Ecoli_MG1655_50x_R2.fastq data/read.fa To create our submission script we need to know the path to our conda enviroment. To get this: conda env list You'll need to find your idba-example environment, and next to it is the path you'll need for your submission script. In my case: # conda environments: # base * /home/username/anaconda3 idba-example /home/username/anaconda3/envs/idba-example # We need this line, it'll be different for you! Create our sbatch submission script. Note that this sequence doesn't need a lot of memory, so we'll use 3G. To see your usage after the job has run use vuw-job-report <job-id> idba_submit.sh #!/bin/bash #SBATCH --job-name=idba_test #SBATCH -o _output.out #SBATCH -e _output.err #SBATCH --time=00:5:00 #SBATCH --partition=quicktest #SBATCH --ntasks=12 #SBATCH --mem=3G module load Anaconda3/2020.11 eval \" $( conda shell.bash hook ) \" # basically inits your conda - prevents errors like: CommandNotFoundError: Your shell has not been properly configured ... conda activate /home/username/anaconda3/envs/idba-example # We will need to activate our conda enviroment on the remote node idba idba_ud -r data/read.fa -o output To submit our job sbatch idba_submit.sh To see our job running or queuing squeue -u $USER This job will take a few minutes to run, generally less than 5. When the job is done we can see the output in the output folder. We can also see the std output and std err in the files _output.out and _output.err . The quickest way to examine them is to cat the files when the run is done. cat _output.out","title":"Using Anaconda/Miniconda/conda"},{"location":"examples/#r-users-guide","text":"","title":"R users guide"},{"location":"examples/#which-versions-of-r-are-working-on-rapoi","text":"There are a number of versions of R on R\u0101poi, although many of these are old installations (prior to an OS update and changes to the module system) and no longer work. There are three relatively recent versions of R which currently work (as of April 2024), these are R/3.6.3 , R/4.1.0 and R/4.2.0 . Each of these modules has a couple of pre-requisite modules that need to be loaded prior to loading the R module. To find out what you need to load first you just need to check the output of module spider R/x.y.z (with the appropriate values for x,y,z). The following example shows how to use R/4.2.0 .","title":"Which versions of R are working on R\u0101poi?"},{"location":"examples/#loading-r-packages-running-a-simple-job","text":"First login to R\u0101poi and load the R module: module purge # clean/reset your environment module load config # reload utilities such as vuw-job-report module load GCC/11.2.0 OpenMPI/4.1.1 # pre-requisites for the new R module module load R/4.2.0 Then run R on the command line: R Test library existence: > library ( tidyverse ) This should load the package, and give some output like this: \u2500\u2500 Attaching packages \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 tidyverse 1.3 . 1 \u2500\u2500 \u2714 ggplot2 3.3 . 5 \u2714 purrr 0.3 . 4 \u2714 tibble 3.1 . 6 \u2714 dplyr 1.0 . 8 \u2714 tidyr 1.2 . 0 \u2714 stringr 1.4 . 0 \u2714 readr 2.1 . 2 \u2714 forcats 0.5 . 1 \u2500\u2500 Conflicts \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 tidyverse_conflicts () \u2500\u2500 \u2716 dplyr :: filter () masks stats :: filter () \u2716 dplyr :: lag () masks stats :: lag () (These conflicts are normal and can be ignored.) To quit R, type: > q () Next create a bash submission script called r_submit.sh (or another name of your choice) using your preferred text editor, e.g. nano. #!/bin/bash # #SBATCH --job-name=r_test #SBATCH -o r_test.out #SBATCH -e r_test.err # #SBATCH --cpus-per-task=2 #Note: you are always allocated an even number of cpus #SBATCH --mem=1G #SBATCH --time=10:00 module purge module load GCC/11.2.0 OpenMPI/4.1.1 module load R/4.2.0 Rscript mytest.R Save this to the current working directory, and then create another file using your preferred text editor called mytest.R (or another name of your choice) containing the following R commands: library ( tidyverse ) sprintf ( \"Hello World!\" ) then run it with the previously written bash script: sbatch r_submit.sh This submits a task that should execute quickly and create files in the directory from which it was run. Examine r_test.out . You can use an editor like nano, vi or emacs, or you can just cat or less the file to see its contents on the terminal. You should see: \"Hello World\"","title":"Loading R packages &amp; running a simple job"},{"location":"examples/#installing-additional-r-packagesextensions-in-your-local-user-directory","text":"If you are in need of additional R packages which are not included in the R installation, you may intall them into your user directory. Start by launching an R session module purge module load GCC/11.2.0 OpenMPI/4.1.1 module load R/4.2.0 R Then, supposing you want to install a package from CRAN named \"A3\". If this is the first time you are attempting to install local packages for this R version then the steps look something like this. > library ( A3 ) # confirm that foo is not already available > install.packages ( 'A3' ) Warning in install.packages ( \"A3\" ) : 'lib = \"/home/software/EasyBuild/software/R/4.2.0-foss-2021b/lib64/R/library\"' is not writable Would you like to use a personal library instead ? ( yes / No / cancel ) yes Would you like to create a personal library \u2018 / nfs / home /< username >/ R / x86_64 - pc - linux - gnu - library / 4.2 \u2019 to install packages into ? ( yes / No / cancel ) yes --- Please select a CRAN mirror for use in this session --- Secure CRAN mirrors < long list of mirrors , the NZ mirror was number 54 in my list > Selection : 54 trying URL 'https://cran.stat.auckland.ac.nz/src/contrib/A3_1.0.0.tar.gz' < additional output from the installation steps... > In future, when you run this version of R, it should automatically check the local user directory created above for installed packages. Any other packages you install in future should automatically go into this directory as well (assuming you don't play around with .libPaths() ).","title":"Installing additional R packages/extensions in your local user directory"},{"location":"examples/#matlab-gpu-example","text":"Matlab has various built-in routines which are GPU accelerated. We will run a simple speed comparison between cpu and gpu tasks. In a sensible location create a file called matlab_gpu.m I used ~/examples/matlab/cuda/matlab_gpu.m . % Set an array which will calculate the Eigenvalues of A = rand ( 1000 ); % Copy the Array to the GPU memory - this process takes an erratic amount of time, so we will not time it. Agpu = gpuArray ( A ); tic B=eig(Agpu) ; t1 = toc % Let's compare the time with CPU tic B = eig ( A ); t2 = toc We will also need a Slurm submission script; we'll call this matlab_gpu.sh . Note that we will need to use the new Easybuild module files for our cuda libraries, so make sure to include the module use line module use /home/software/tools/eb_modulefiles/all/Core #!/bin/bash #SBATCH --job-name=matlab-gpu-example #SBATCH --output=out-gpu-example.out #SBATCH --error=out-gpu-example.err #SBATCH --time=00:05:00 #SBATCH --partition=gpu #SBATCH --gres=gpu:1 #SBATCH --ntasks=2 #SBATCH --mem=60G module use /home/software/tools/eb_modulefiles/all/Core module load MATLAB/2024a module load fosscuda/2020b matlab -nodisplay -nosplash -nodesktop -r \"run('matlab_gpu.m');exit;\" To submit this job to the Slurm queue sbatch matlab_gpu.sh . This job will take a few minutes to run - this is mostly the Matlab startup time. Examine the queue for your job squeue -u $USER . When your job is done, inspect the output file. You can use an editor like nano, vi or emacs, or you can just cat or less the file to see its contents on the terminal. cat out-gpu-example.out What do you notice about the output? Surely GPUs should be faster than the CPU! It takes time for the GPU to start processing your task, the CPU is able to start the task far more quickly. So for short operations, the CPU can be faster than the GPU - remember to benchmark your code for optimal performance! Just because you can use a GPU for your task doesn't mean it is necessarily faster! To get a better idea of the advantage of the GPU let's increase the size of the array from 1000 to 10000 matlab_gpu.m % Set an array which will calculate the Eigenvalues of A = rand ( 10000 ); % Copy the Array to the GPU memory - this process takes an erratic amount of time, so we will not time it. Agpu = gpuArray ( A ); tic B=eig(Agpu) ; t1 = toc % Let's compare the time with CPU tic B = eig ( A ); t2 = toc To make things fairer for the CPU in this case, we will also allocate half the CPUs on the node to Matlab. Half the CPUs, half the memory and half the GPUs, just to be fair. matlab_gpu.sh #!/bin/bash #SBATCH --job-name=matlab-gpu-example #SBATCH --output=out-gpu-example.out #SBATCH --error=out-gpu-example.err #SBATCH --time=00:05:00 #SBATCH --partition=gpu #SBATCH --gres=gpu:1 #SBATCH --ntasks=128 #SBATCH --mem=256G module use /home/software/tools/eb_modulefiles/all/Core module load MATLAB/2024a module load fosscuda/2020b matlab -nodisplay -nosplash -nodesktop -r \"run('matlab_gpu.m');exit;\" The output in my case was: < M A T L A B ( R ) > Copyright 1984 -2024 The MathWorks, Inc. R2024a ( 24 .1.0.2537033 ) 64 -bit ( glnxa64 ) February 21 , 2024 For online documentation, see https://www.mathworks.com/support For product information, visit www.mathworks.com. t1 = 62 .0212 t2 = 223 .0818 So in thise case the GPU was considerably faster. Matlab can do this a bit faster on the CPU if you give it fewer CPUs, the optimum appears to be around 20, but it still takes 177s. Again, optimise your resource requests for your problem, less can sometimes be more, however the GPU easily wins in this case.","title":"Matlab GPU example"},{"location":"examples/#job-arrays-running-many-similar-jobs","text":"Slurm makes it easy to run many jobs which are similar to each other. This could be one piece of code running over many datasets in parallel or running a set of simulations with a different set of parameters for each run.","title":"Job Arrays - running many similar jobs"},{"location":"examples/#simple-bash-job-array-example","text":"The following code will run the submission script 16 times as resources become available (i.e. they will not neccesarily run at the same time). It will just print out the Slurm array task ID and exit. submit.sh: #!/bin/bash #SBATCH --job-name=test_array #SBATCH --output=out_array_%A_%a.out #SBATCH --error=out_array_%A_%a.err #SBATCH --array=1-16 #SBATCH --time=00:00:20 #SBATCH --partition=parallel #SBATCH --ntasks=1 #SBATCH --mem=1G # Print the task id. echo \"My SLURM_ARRAY_TASK_ID: \" $SLURM_ARRAY_TASK_ID # Add lines here to run your computations. Run the example with the standard sbatch submit.sh","title":"Simple Bash Job Array example"},{"location":"examples/#a-simple-r-job-array-example","text":"As a slightly more practical example the following will run an R script 5 times as resources become available. The R script takes as an input the $SLURM_ARRAY_TASK_ID which then selects a parameter alpha out of a lookup table. This is one way you could run simulations or similar with a set parameters defined in a lookuop table in your code. To make outputs more tidy and to help organisation, instead of dumping all the outputs into the directory with our code and submission script, we will separate the outputs into directories. Dataframes saved from R will be saved to the output/ directory, and all output which would otherwise be printed to the commnd line (stdout and stderr) will be saved to the stdout/ directory. Both of these directories will need to be created before running the script. r_random_alpha.R: # get the arguments supplied to R. # trailingOnly = TRUE gets the user supplied # arguments, and for now we will only get the # first user supplied argument args <- commandArgs ( trailingOnly = TRUE ) inputparam <- args [ 1 ] # a vector with all our parameters. alpha_vec <- c ( 2.5 , 3.3 , 5.1 , 8.2 , 10.9 ) alpha <- alpha_vec [ as.integer ( inputparam )] # Generate a random number between 0 and alpha # store it in dataframe with the coresponding # alpha value randomnum <- runif ( 1 , min = 0 , max = as.double ( alpha )) df <- data.frame ( \"alpha\" = alpha , \"random_num\" = randomnum ) # Save the data frame to a file with the alpha value # Note that the output/ folder will need to be # manually created first! outputname <- paste ( \"output/\" , \"alpha_\" , alpha , \".Rda\" , sep = \"\" ) save ( df , file = outputname ) Next create the submision script. Which we will run on the parallel partition rather than quicktest. r_submit.sh: #!/bin/bash #SBATCH --job-name=test_R_array #SBATCH --output=stdout/array_%A_%a.out #SBATCH --error=stdout/array_%A_%a.err #SBATCH --array=1-5 #SBATCH --time=00:00:20 #SBATCH --partition=parallel #SBATCH --ntasks=1 #SBATCH --mem=1G module purge module load GCC/11.2.0 OpenMPI/4.1.1 module load R/4.2.0 # Print the task id. Rscript r_random_alpha.R $SLURM_ARRAY_TASK_ID Run the jobs with sbatch r_submit.sh","title":"A simple R job Array Example"},{"location":"examples/#singularity","text":"While there are many modules on R\u0101poi, sometimes you might want to install your own packages in your own way. Singularity allows you to do this. If you are familiar with Docker, Singularity is similar, except you can't get root (or sudo) once your container is running on the R\u0101poi. However, you can have sudo rights locally on your own machine, setup your container however you like, then run it without sudo on the cluster. Note As per updated singularity configuration, mount hostfs is set to false and mount home is set to true . As a result, filesystems available inside a singularity session by default are: devtmpfs /dev tmpfs /dev/shm /vg<id> /etc/hosts .. /tmp .. /var/tmp .. /etc/group $HOME /nfs/home/$USER This means any other file system such as nfs should be manually specified using --bind command. For example, singularity shell --bind /nfs/scratch/$USER:/nfs/scratch/$USER image_file.simg pwd See also: Using containers","title":"Singularity"},{"location":"examples/#singularitydocker-container-example","text":"Singularity allows you to use most (but not all!) docker images on R\u0101poi. On your local machine create the singularity definition file input_args_example.def BootStrap : library From : ubuntu: 16.04 %runscript exec echo \" $@ \" %labels Author JaneDoe This will build an ubuntu 16.04 container that will eventually run on R\u0101poi which runs Centos. This container has a runscript which just echos back any arguments sent to the container when your start it up. Build the container locally with sudo and singularity sudo singularity build inputexample.sif input_args_example.def OR Using Sylabs Remote Builder is another option to build containers remotely. A Sylabs account and access token are required to use this feature. To build the container remotely, use the following command: singularity build --remote inputexample.sif input_args_example.def This will build an image that you can't modify any further and is immediately suitable to run on R\u0101poi Copy this file to R\u0101poi via sftp sftp <username>@raapoi.vuw.ac.nz Create a submit script using singularity on the cluster singularity_submit.sh #!/bin/bash #SBATCH --job-name=singularity_test #SBATCH -o sing_test.out #SBATCH -e sing_test.err #SBATCH --time=00:00:20 #SBATCH --ntasks=1 #SBATCH --mem=1G module load singularity singularity run inputtest.sif \"hello from a container\" Run the script with the usual singularity_submit.sh","title":"Singularity/Docker container example"},{"location":"examples/#singularitytensorflow-example","text":"tensor.def Bootstrap: docker From: tensorflow/tensorflow:latest-py3 %post apt-get update && apt-get -y install wget build-essential %runscript exec python \" $@ \" compile this locally with sudo and singularity. sudo singularity build tensorflow.sif tensor.def Create a quick tensorflow test code tensortest.py import tensorflow as tf mnist = tf . keras . datasets . mnist ( x_train , y_train ),( x_test , y_test ) = mnist . load_data () x_train , x_test = x_train / 255.0 , x_test / 255.0 model = tf . keras . models . Sequential ([ tf . keras . layers . Flatten ( input_shape = ( 28 , 28 )), tf . keras . layers . Dense ( 512 , activation = tf . nn . relu ), tf . keras . layers . Dropout ( 0.2 ), tf . keras . layers . Dense ( 10 , activation = tf . nn . softmax ) ]) model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) model . fit ( x_train , y_train , epochs = 5 ) model . evaluate ( x_test , y_test ) Copy your files to R\u0101poi via sftp (or whatever you prefer) sftp <username>@raapoi.vuw.ac.nz cd <where you want to work> put * #put all files in your local directory onto R\u0101poi Lets quickly test the code via an interactive session on a node. Note I find the tensorflow container only runs properly on intel nodes, which we don't have many of at the moment, I'll investigate this further. srun --partition = \"parallel\" --constraint = \"Intel\" --pty bash #now on the remote node - note you might need to wait if nodes are busy module load singularity #load singularity singularity shell tensorflow.sif #now inside the tensorflow container on the remote node python tensortest.py #once that runs, exit the container exit #exit the container exit #exit the interactive session on the node Create a submit script using singularity on the cluster singularity_submit.sh #!/bin/bash #SBATCH --job-name=singularity_test #SBATCH -o sing_test.out #SBATCH -e sing_test.err #SBATCH --time=00:10:00 #SBATCH --partition=parallel #SBATCH --constraint=Intel #SBATCH --ntasks=1 #SBATCH --mem=4G module load singularity #run the container with the runscript defined when we created it singularity run tensorflow.sif tensortest.py","title":"Singularity/TensorFlow Example"},{"location":"examples/#singularitymaxbin2-example","text":"In a sensible location, either in your home directory or on the scratch: Get the maxbin2 container, there are a few places to get this, but will get the bioconda container as it is more recent than the one referenced on the official maxbin site. module load module load singularity singularity pull docker://quay.io/biocontainers/maxbin2:2.2.6--h14c3975_0 mv maxbin2_2.2.6--h14c3975_0.sif maxbin2_2.2.6.sif #rename for convenience Download some test data mkdir rawdata curl https://downloads.jbei.org/data/microbial_communities/MaxBin/getfile.php?20x.scaffold > rawdata/20x.scaffold curl https://downloads.jbei.org/data/microbial_communities/MaxBin/getfile.php?20x.abund > rawdata/20x.abund Create an output data location mkdir output Create a submit script using singularity on the cluster singularity_submit.sh #!/bin/bash #SBATCH --job-name=maxbin2_test #SBATCH -o sing_test.out #SBATCH -e sing_test.err #SBATCH --time=00:10:00 #SBATCH --partition=parallel #SBATCH --ntasks=4 #SBATCH --mem=4G module load singularity singularity exec maxbin2_2.2.6.sif run_MaxBin.pl -contig rawdata/20x.scaffold -abund rawdata/20x.abund -out output/20x.out -thread 4","title":"Singularity/MaxBin2 Example"},{"location":"examples/#singularitysandbox-example","text":"This lets you have root inside a container locally and make changes to it. This is really handy for determining how to setuop your container. While you can convert the sandbox container to one you can run on R\u0101poi, I suggest you don't do this . Use the sandbox to figure out how you need to configure your container, what packages to install, config files to change etc. Then create a .def file that contains all the nessesary steps without the need to use the sandbox - this will make your work more reproducable and easier to share with others. example.def BootStrap: library From: ubuntu:16.04 %post apt-get update && apt-get -y install wget build-essential %runscript exec echo \" $@ \" %labels Author JaneDoe Compile this locally with sudo and singularity. We are using the sandbox flag to create a writable container directory ( example/ ) on our local machine where we have sudo rights. sudo singularity build --sandbox example/ example.def Now we can run the container we just built, but with sudo rights inside the container. Your rights outside the container match the rights inside the container, so we need to do this with sudo. sudo singularity shell --writable example/ Inside the container we now have root and can install packages and modify files in the root directories Singularity example:~> apt update Singularity example:~> apt install sqlite Singularity example:~> touch /test.txt #create an empty file in root Singularity example:~> ls / Singularity example:~> exit #exit container To run the container on R\u0101poi we convert it to the default immutable image with build. We might need sudo for this as the prior use of sudo will have created a directory that your usual user can't see every file. sudo singularity build new-example-sif example/ You could now copy the new-example-sif file to R\u0101poi and run it there. However a better workflow is to use this to experiment, to find out what changes you need to make to the image and what packages you need to install. Once you've done that, I suggest starting afresh and putting everything in the.def file . That way when you return to your project in 6 months, or hand it over to someone else, there is a clear record of how the image was built.","title":"Singularity/Sandbox Example"},{"location":"examples/#singularitycustom-conda-container-idba-example","text":"In this example we'll build a singularity container using conda. The example is building a container for idba - a genome assembler. Idba is available in bioconda, but not as a biocontainer. We'll build this container locally to match a local conda environment, then run it on the HPC and do an example assembly.","title":"Singularity/Custom Conda Container - idba example"},{"location":"external_providers/","text":"Connecting to Cloud/Storage Providers \u00b6 Connecting to Cloud Providers \u00b6 AARNET Cloudstor \u00b6 NOTE Cloudstor service has been decommissioned since Dec 2023. For old accounts, please check access with the provider. All VUW researchers have access to the AARNET (Australia\u2019s Academic and Research Network) Cloudstor service which provides 1 TB of space to each researcher. To use this service first login and download an appropriate client to your laptop or desktop (or smarthone if you wish): Cloudstor Login NOTE: Within the Cloudstor web login settings you will need to create a Cloudstor Password, this is the password you will use to login on R\u0101poi, it does not use your VUW credentials for the command line login. We suggest setting up an App Password for Raapoi-rcopy rather than a global sync password. This way if your password is compromised you can easily just remove that app password. Setup an App Password by clicky on the settings gear on the top right and finding the App Password link . Once you have setup your cloudstor (aka ownCloud) credentials you can use them to sync data to and from R\u0101poi. For example, if I wanted to sync my project space to Cloudstor I would do the following from R\u0101poi login node: # Use Tmux to keep your session alive if you disconnect. You can reconnect to your Tmux session if you reconnect. See Tmux docs. tmux # Use our new module system module use /home/software/tools/eb_modulefiles/all/Core module load rclone/1.54.1 #check if cloudstor remote is already configured rclone listremotes The above sequence starts a tmux session to allow the transfer to continue even if I disconnect from the cluster (type tmux attach to reconnect) and then loads the rcopy module - which requires the use of our new module system. If you don't already have CloudStor configured as a remote (which you won't if this is your first time using it) follow the instructions on aarnet docs page . Once we have setup rclone to connect to CloudStor, we copy our data. In this case from <my scratch folder>/test to test on CloudStor rclone copy --progress --transfers 8 /nfs/scratch/<username>/test CloudStor:/test Amazon AWS \u00b6 Create a python environment and install awscli module. python3 -m pip install awscli Before you proceed you will need to configure your environment with your Access Key ID and Secret Access Key, both of which will be sent to you once your account is created or linked. The command to configure your environment is aws configure You only need to do this once, unless of course you use more than one user/Access Key. Most users can simply click through the region and profile questions (using the default of \"none\"). If you do have a specific region this should be relayed along with your access and secret keys. Once you have the appropriate environment in place and your configuration setup you can use the aws command, plus an appropriate sub-command (s3, emr, rds, dynamodb, etc) and supporting arguments. More information on the CLI can be found here: http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-using.html Transferring Data to/from Amazon (AWS) S3 \u00b6 To transfer data from S3 you first need to setup your AWS connect, instructions for that can be found above. Once that is done you should be able to use the aws commands to copy data to and from your S3 storage. For example if I wanted to copy data from my S3 storage to my project directory I could do the following: tmux module load amazon/aws/cli cd /nfs/scratch/<username>/project aws s3 cp s3://mybucket/mydata.dat mydata.dat To copy something to storage simply reverse the file paths, eg. aws s3 cp mydata.dat s3://mybucket/mydata.dat The above starts a tmux session to allow the transfer to continue even if I disconnect from the cluster (type tmux attach to reconnect). I then load the modules necessary to use the AWS commands. I change directory to my project space and use the aws s3 cp command to copy from S3. More information on using aws can be found here: http://docs.aws.amazon.com/cli/latest/reference/s3/index.html#cli-aws-s3 Working with AWS Data Analysis Tools \u00b6 Amazon has a number of data analytics and database services available. Using the command line utilities available in R\u0101poi, researchers can perform work on the eo cluster and transfer data to AWS to perform further analysis with tools such as MapReduce (aka Hadoop), RedShift or Quicksight. A listing of available services and documentation can be found at the following: https://aws.amazon.com/products/analytics/ Google Cloud (gcloud) Connections \u00b6 The Google Cloud SDK is available in R\u0101poi. This includes a command-line interface for connecting to gloud services. To get started, first load the environment module. You can find the path with the module spider command. As of this writing the current version can be loaded thusly: module load gcloud/481.0.0 This will give you access to the gcloud command. To setup a connection to your gcloud account use the init sub-command, eg. gcloud init --console-only Follow the instructions to authorize your gcloud account. Once on the Google website, you will be given an authorization code which you will copy/paste back into the R\u0101poi terminal window. Transferring Data to/from Google Cloud (gcloud) \u00b6 To transfer data from gcloud storage you first need to setup your gcloud credentials, instructions for that can be found above. Once that is done you should be able to use the gsutil command to copy data to and from your gcloud storage. For example, if I wanted to copy data from gcloud to my project directory I could do the following: tmux module load gcloud/481.0.0 cd /nfs/scratch/<username>/project gsutil cp gs://mybucket/mydata.dat mydata.dat To copy something to storage simply reverse the file paths, eg. gsutil cp mydata.dat gs://mybucket/mydata.dat The above starts a tmux session to allow the transfer to continue even if I disconnect from the cluster (type tmux attach to reconnect). I then load the modules necessary to use the gsutil commands. I change directory to my project space and use the gsutil cp command to copy from gcloud. More information on using gcloud can be found here: https://cloud.google.com/sdk/gcloud/ Working with GCloud Data Analysis Tools \u00b6 Google Cloud has a number of data analytics and database services available. Using the gcloud command line utilities available on R\u0101poi, researchers can perform work on the cluster and transfer data to gcloud to perform further analysis with tools such as Dataproc (Hadoop/Spark), BigQuery or Datalab (Visualization) A listing of available services and documentation can be found at the following: https://cloud.google.com/products/ DropBox Cloud Storage \u00b6 Upload/Download Limits with DropBox Dropbox has upload/download limitations and we have found that once your file gets above 50GB in size the transfer will have a better chance of timing out and failing. Configuring your Dropbox account on R\u0101poi Step A: On your local laptop or desktop start your browser and login to your Dropbox account Step B: On R\u0101poi type the following: module load dropbox Step C: Setup account credentials (You should only need to do this once): Run the following command from R\u0101poi dbxcli account You will now see something like the following: Go to https://www.dropbox.com/1/oauth2/authorize?client_id=X123... Click \"Allow\" (you might have to log in first). Copy the authorization code. Enter the authorization code here: Step D: Copy the URL link listed in Step C1 and paste it into the web browser that you started in Step A This will provide you with a long access code (aka hash). Now copy that access code and paste it into your R\u0101poi terminal after Step C3 where it is asking for Enter the authorization code here Now hit enter or return. You should see that you are now logged in with your Dropbox credentials Basic Dropbox commands \u00b6 Remember to load the dropbox environment module if you have not already (see module spider for the path) Now type dbx or dbxcli at a prompt. You will see a number of sub-commands, for instance ls, which will list the contents of your Dropbox, eg dbxcli ls Downloading from Dropbox \u00b6 Downloading uses the subcommand called: get. The basic format for get is: dbxcli get fileOnDropbox fileOnRaapoi For instance, if I have a datafile called 2018-financials.csv on Dropbox that I want to copy to my project folder I would type: dbxcli get 2018-financials.csv /nfs/scratch/<username>/projects\\ /finance_proj/2018-financials.csv Uploading to Dropbox \u00b6 Uploading is similar to downloading except now we use the subcommand: put. The basic format for put is: dbxcli put fileOnRaapoi fileOnDropbox For example I want to upload a PDF I generated from one of my jobs called final-report.pdf I would type: dbxcli put final-report.pdf final-report.pdf This will upload the PDF and name it the same thing, if I wanted to change the name on Dropbox I could: dbxcli put final-report.pdf analytics-class-final-report.pdf Microsoft OneDrive \u00b6 RClone can be used to connect to onedrive, google drive, etc. The steps below implement onedrive setup on R\u0101poi. module load rclone/1.54.1 rclone config Follow the on-screen instructions, e.g., make a new remote - enter \"n\"; to select storage type; \"onedrive\" and keep following default options until \"Use auto config\" and enter \"y\" here. This should display a url; copy and paste in the browser and it should get set up. To view your files on the remote onedrive, you can use: # For example, rclone lsd <remote>:<dir_name>, in my case , I would do : rclone lsd my_staff_onedrive:Documents GLOBUS \u00b6 External sharing coming soon! Presently, users can share/transfer data to and from locations including cluster, research storage, and personal devices. Options for enabling data sharing externally are being explored. First time users should be able to sign up on GLOBUS website using their university credentials by following this link: https://app.globus.org/ . Please install and start GLOBUS connect personal. Please find those instructions here: https://globus.org/globus-connect-personal You should now be able to share or transfer your data by following their guide: https://docs.globus.org/guides/tutorials/manage-files/share-files/ . Transfer files using Globus \u00b6 Globus works by seting up endpoints which are like locations for data transfer. This means, we will need two endpoints to transfer data; one - a source, and the other - a destination. The steps below will walk you through the Installation and then navigating the Globus File Manager . The installation process is same for both the globusconnectpersonal setup and for any of the R\u0101poi's compute nodes . Installation \u00b6 Working on a Windows machine? The steps below are for a non-windows based operating system. For Windows OS, please see https://globus.org/globus-connect-personal globusconnectpersonal setup \u00b6 Inside the termial of your device, please run the following command: wget https://downloads.globus.org/globus-connect-personal/\\ linux/stable/globusconnectpersonal-latest.tgz tar xzf globusconnectpersonal-latest.tgz # this will produce a versioned globusconnectpersonal directory # replace `x.y.z` in the line below with the version number you see cd globusconnectpersonal-x.y.z ./globusconnect -setup --no-gui Here, we'll be presented with a url . If it is our first time, it will ask for creating an account, if you haven't done it before. Once, the account is set up, you'll be presented with an authorization code to enter into the code prompt. An example of this process is shown below: $ ./globusconnect -setup --no-gui We will display a login URL. Copy it into any browser and log in to get a single-use code. Return to this command with the code to continue setup. Login here: ----- https://auth.globus.org/v2/oauth2/authorize... ----- Enter the auth code: 0ZaZ.... == starting endpoint setup Input a value for the Endpoint Name: <enter_relevant_name> registered new endpoint, id: 56d4c388.... setup completed successfully Will now start globusconnectpersonal in GUI mode Graphical environment not detected To launch Globus Connect Personal in CLI mode, use globusconnectpersonal -start Or, if you want to force the use of the GUI, use globusconnectpersonal -gui Finally, run globusconnect ./globusconnectpersonal -start & On the personal device it will display a window This completes our setup on the personal device. Now, we should be able to set it up on R\u0101poi . On R\u0101poi \u00b6 Launch an interactive session srun --pty bash Tip Request more time for the interactive session, if you are wanting to transfer huge amount of data. srun --time=0-05:00:00 --pty bash Once you are on a compute node, please follow the steps starting from globusconnectpersonal setup as above ./globusconnectpersonal -start & Changing default directory to put your data? echo \"/new/path/address/,0,1\" > /nfs/home/$USER/.globusonline/lta/config-paths Here, the /new/path/address will be the path of your choice, e.g., /nfs/scratch/$USER/ This should now makes our two endpoints of data transfer accessible from the Globus website. Globus File Manager \u00b6 Leave the above running, and login to Globus website. Open the File Manager Tab from the login page. You should now be able to browse the names of your personal device and R\u0101poi 's compute node to transfer your files. An example shown below: You are now ready to transfer your data. Storage for Learning and Research (SoLAR) - VUW High Capacity Storage \u00b6 The SoLAR Drive is the VUW High Capacity Storage system, allowing you to store all your research work. You can require many many terabytes of storage. It is also possible to connect your SoLAR drive to R\u0101poi, which is great! The following document will describe how to sign up for storage on the SoLAR Drive, as well as how to move and copy data between R\u0101poi and your SoLAR Drive. Signing up and getting storage on SoLAR \u00b6 To get your own space on SoLAR. Do the following: Login to your staff intranet. To do this, open https://intranet.wgtn.ac.nz/ in your web browser, and sign in to your staff intranet. In a new browser tab, open https://intranet.wgtn.ac.nz/staff/services-resources/digital-solutions/contact-us You should see the follow page below. Click the Staff Service Centre button You will now be directed to the Staff Service Centre, which will look like below. Hover your mouse above Digital Solution -> Access/permissions -> Additional drives You will now be sent to the ADDITIONAL DRIVE ACCESS page. Fill out the details on this page and click the **Submit** button at the bottom of the page to send your request space on SoLAR. Source: https://intranet.wgtn.ac.nz/staff/services-resources/digital-solutions/research-services/solar Accessing your SoLAR Drive on Windows/Mac \u00b6 Accessing the SoLAR Drive from off Campus \u00b6 You will want to sign up to the Uictoria University VPN to gain access to SoLAR. Click https://vpn.vuw.ac.nz/ to get access to the VPN and to download the Cisco AnyConnect program on to your computer Windows \u00b6 Open This PC (My Computer) and click Computer -> Map network drive at the top of the This PC explorer window. This will open a window as shown below. Enter the SoLAR path and the name of your patition on SoLAR, and click the Finish button. Enter in your username as STAFF\\username and your password if required Mac \u00b6 If you are off campus, login to your VPN using the Cisco AnyConnect program. In Finder, click Go -> Connect to Server... Write smb://vuwstocoissrin1.vuw.ac.nz/YourFolderName into the box, where YourFolderName is the name of your partition on SoLAR, and click connect. In username give: ``STAFF/username``; give your VUW password, and click connect. Moving/Copying files and folders between SoLAR and R\u0101poi \u00b6 There are several way to move/copy files and folder between SoLAR and R\u0101poi Best Way: Mounting SoLAR Partition in R\u0101poi \u00b6 Ask Digital Solutions for a service account to be created against your Research storage. Then a Raapoi admin will permanantly mount your storage on Raapoi - this process is time consuming and involves back and forth between DS and CAD. Second Way: RClone \u00b6 To do once I get it fixed Third Way: smbclient \u00b6 smbclient is specifically designed to transfer files and folders to and from smb clients. It is a bit cumbersome to use, but it is an alternative way for copying files between R\u0101poi and SoLAR To use smbclient , first cd into the directory that contains the folder you would like to copy from R\u0101poi to SoLAR. Then in the terminal give the following input: smbclient //vuwstocoissrin1.vuw.ac.nz/SoLAR_folder_name \\ --user username --workgroup STAFF --command \"prompt OFF;\\ recurse ON; cd remote/target/directory; mput \\ folder_on_Raapoi_you_want_to_copy_to_SoLAR \" You will then be asked to give your VUW password to copy data to SoLAR. Note: This may take a while if you are copying lots of files or large files. It is recommended that if you have lots of file in folders to copy (i.e. in the 100,000s of files) that you copy individually big folders rather than the whole directory at once so you can keep track of what has been copied if there are issues. Tip You may see it not doing anything for a while, and then all of a sudden it will show you that it is doing things. This is normal. If you want to copy files from SoLAR to R\u0101poi, first cd into the directory on R\u0101poi that you want to copy the SoLAR folder/file to. Then in the terminal give the following input: smbclient //vuwstocoissrin1.vuw.ac.nz/SoLAR_folder_name \\ --user username --workgroup STAFF --command \"prompt OFF; \\ recurse ON; cd remote/source/directory; mget \\ folder_on_SoLAR_you_want_to_copy_to_Raapoi \" Warning Don't run multiple smbclient at once, only a few at a time, if not one at a time. It can have problems if too many are running at one time.","title":"Connecting to Cloud/Storage Providers"},{"location":"external_providers/#connecting-to-cloudstorage-providers","text":"","title":"Connecting to Cloud/Storage Providers"},{"location":"external_providers/#connecting-to-cloud-providers","text":"","title":"Connecting to Cloud Providers"},{"location":"external_providers/#aarnet-cloudstor","text":"NOTE Cloudstor service has been decommissioned since Dec 2023. For old accounts, please check access with the provider. All VUW researchers have access to the AARNET (Australia\u2019s Academic and Research Network) Cloudstor service which provides 1 TB of space to each researcher. To use this service first login and download an appropriate client to your laptop or desktop (or smarthone if you wish): Cloudstor Login NOTE: Within the Cloudstor web login settings you will need to create a Cloudstor Password, this is the password you will use to login on R\u0101poi, it does not use your VUW credentials for the command line login. We suggest setting up an App Password for Raapoi-rcopy rather than a global sync password. This way if your password is compromised you can easily just remove that app password. Setup an App Password by clicky on the settings gear on the top right and finding the App Password link . Once you have setup your cloudstor (aka ownCloud) credentials you can use them to sync data to and from R\u0101poi. For example, if I wanted to sync my project space to Cloudstor I would do the following from R\u0101poi login node: # Use Tmux to keep your session alive if you disconnect. You can reconnect to your Tmux session if you reconnect. See Tmux docs. tmux # Use our new module system module use /home/software/tools/eb_modulefiles/all/Core module load rclone/1.54.1 #check if cloudstor remote is already configured rclone listremotes The above sequence starts a tmux session to allow the transfer to continue even if I disconnect from the cluster (type tmux attach to reconnect) and then loads the rcopy module - which requires the use of our new module system. If you don't already have CloudStor configured as a remote (which you won't if this is your first time using it) follow the instructions on aarnet docs page . Once we have setup rclone to connect to CloudStor, we copy our data. In this case from <my scratch folder>/test to test on CloudStor rclone copy --progress --transfers 8 /nfs/scratch/<username>/test CloudStor:/test","title":"AARNET Cloudstor"},{"location":"external_providers/#amazon-aws","text":"Create a python environment and install awscli module. python3 -m pip install awscli Before you proceed you will need to configure your environment with your Access Key ID and Secret Access Key, both of which will be sent to you once your account is created or linked. The command to configure your environment is aws configure You only need to do this once, unless of course you use more than one user/Access Key. Most users can simply click through the region and profile questions (using the default of \"none\"). If you do have a specific region this should be relayed along with your access and secret keys. Once you have the appropriate environment in place and your configuration setup you can use the aws command, plus an appropriate sub-command (s3, emr, rds, dynamodb, etc) and supporting arguments. More information on the CLI can be found here: http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-using.html","title":"Amazon AWS"},{"location":"external_providers/#google-cloud-gcloud-connections","text":"The Google Cloud SDK is available in R\u0101poi. This includes a command-line interface for connecting to gloud services. To get started, first load the environment module. You can find the path with the module spider command. As of this writing the current version can be loaded thusly: module load gcloud/481.0.0 This will give you access to the gcloud command. To setup a connection to your gcloud account use the init sub-command, eg. gcloud init --console-only Follow the instructions to authorize your gcloud account. Once on the Google website, you will be given an authorization code which you will copy/paste back into the R\u0101poi terminal window.","title":"Google Cloud (gcloud) Connections"},{"location":"external_providers/#dropbox-cloud-storage","text":"Upload/Download Limits with DropBox Dropbox has upload/download limitations and we have found that once your file gets above 50GB in size the transfer will have a better chance of timing out and failing. Configuring your Dropbox account on R\u0101poi Step A: On your local laptop or desktop start your browser and login to your Dropbox account Step B: On R\u0101poi type the following: module load dropbox Step C: Setup account credentials (You should only need to do this once): Run the following command from R\u0101poi dbxcli account You will now see something like the following: Go to https://www.dropbox.com/1/oauth2/authorize?client_id=X123... Click \"Allow\" (you might have to log in first). Copy the authorization code. Enter the authorization code here: Step D: Copy the URL link listed in Step C1 and paste it into the web browser that you started in Step A This will provide you with a long access code (aka hash). Now copy that access code and paste it into your R\u0101poi terminal after Step C3 where it is asking for Enter the authorization code here Now hit enter or return. You should see that you are now logged in with your Dropbox credentials","title":"DropBox Cloud Storage"},{"location":"external_providers/#microsoft-onedrive","text":"RClone can be used to connect to onedrive, google drive, etc. The steps below implement onedrive setup on R\u0101poi. module load rclone/1.54.1 rclone config Follow the on-screen instructions, e.g., make a new remote - enter \"n\"; to select storage type; \"onedrive\" and keep following default options until \"Use auto config\" and enter \"y\" here. This should display a url; copy and paste in the browser and it should get set up. To view your files on the remote onedrive, you can use: # For example, rclone lsd <remote>:<dir_name>, in my case , I would do : rclone lsd my_staff_onedrive:Documents","title":"Microsoft OneDrive"},{"location":"external_providers/#globus","text":"External sharing coming soon! Presently, users can share/transfer data to and from locations including cluster, research storage, and personal devices. Options for enabling data sharing externally are being explored. First time users should be able to sign up on GLOBUS website using their university credentials by following this link: https://app.globus.org/ . Please install and start GLOBUS connect personal. Please find those instructions here: https://globus.org/globus-connect-personal You should now be able to share or transfer your data by following their guide: https://docs.globus.org/guides/tutorials/manage-files/share-files/ .","title":"GLOBUS"},{"location":"external_providers/#storage-for-learning-and-research-solar-vuw-high-capacity-storage","text":"The SoLAR Drive is the VUW High Capacity Storage system, allowing you to store all your research work. You can require many many terabytes of storage. It is also possible to connect your SoLAR drive to R\u0101poi, which is great! The following document will describe how to sign up for storage on the SoLAR Drive, as well as how to move and copy data between R\u0101poi and your SoLAR Drive.","title":"Storage for Learning and Research (SoLAR) - VUW High Capacity Storage"},{"location":"external_providers/#signing-up-and-getting-storage-on-solar","text":"To get your own space on SoLAR. Do the following: Login to your staff intranet. To do this, open https://intranet.wgtn.ac.nz/ in your web browser, and sign in to your staff intranet. In a new browser tab, open https://intranet.wgtn.ac.nz/staff/services-resources/digital-solutions/contact-us You should see the follow page below. Click the Staff Service Centre button You will now be directed to the Staff Service Centre, which will look like below. Hover your mouse above Digital Solution -> Access/permissions -> Additional drives You will now be sent to the ADDITIONAL DRIVE ACCESS page. Fill out the details on this page and click the **Submit** button at the bottom of the page to send your request space on SoLAR. Source: https://intranet.wgtn.ac.nz/staff/services-resources/digital-solutions/research-services/solar","title":"Signing up and getting storage on SoLAR"},{"location":"external_providers/#accessing-your-solar-drive-on-windowsmac","text":"","title":"Accessing your SoLAR Drive on Windows/Mac"},{"location":"external_providers/#movingcopying-files-and-folders-between-solar-and-rapoi","text":"There are several way to move/copy files and folder between SoLAR and R\u0101poi","title":"Moving/Copying files and folders between SoLAR and R\u0101poi"},{"location":"faq/","text":"Frequently Asked Questions \u00b6 Login node \u00b6 Why can't I run programs directly on the login node? R\u0101poi login is a shared resource and running programs directly on the login node puts unnecessary strain. The recommended method is to use srun or sbatch . Please request an interactive session to work on the cluster, e.g., writing programs, debugging, or even data transfer. Requesting Resources \u00b6 I don't want to interfere with other people work, what does it mean \"Other users are prevented from using resources you request, even if you don't use them\"? The system is shared, you will very rarely have full use of a node. You need to be careful to request resources, leaving the extra space available to others. For example, say you submitted a job to bigmem which asked for 800GB of ram and 10 CPUs. Your job would end up on the node with 1000GB ram as only that one would fit it - if your job actually only used 300GB of ram - the extra 500GB of ram you requested would be \"wasted\" no one else could use it. So, another user with a job requesting 600GB of ram would have to wait for your job to end even if there was space for it to run alongside yours. The same issue occurs with CPU requests. It can be very hard to accurately estimate memory and cpu needs before running your job. If your job has a short run time (less than ~10 hours), you can just request more than you need and check the memory usage afterward to guide further jobs. If your job has a long run time (several days), you should run a test job with a short runtime (a few hours) to estimate your needs first. Job Requirements \u00b6 How do I know resource requirement for my job? The best and most reliable method of determining resource requirement is from testing. #Note: It is possible to find out a ballpark figure for a particular software in terms of memory, cpus, or time, for example: MATLAB (~16 GB memory to initialize), see documentation or consult in slack help channel. Select a test job carefully. As a rule of thumb, a test job should not run for more than 15 mins. Perhaps you could use smaller input, coarse parameters or use a subset of the calculations. Make sure your test job is quick to run and quick to start. The later can be ensured by keeping resource requirement to be small (mem or cpu). Often a good first test to run, is to execute your job serially e.g., using only 1 CPU. These jobs should be easier to debug, and quicker to run. Generally it is okay to request roughly 20% more time and memory than you think the job will use, any more than that can impact on other users. Visualisation \u00b6 How do I plot/visualise my results/data on R\u0101poi? Generally speaking, the best practice is to use R\u0101poi for your heavy computing workloads, then transfer your results/data to a local machine to do any plotting/visualisation. An exception to this might be if the visualisation process itself is computationally intensive, the dataset is large and difficult to move, and/or requires specialised hardware (like a gpu). The R\u0101poi operating system is primarily designed for command line use, and thus doesn't include most of the software libraries that support graphical interfaces. This typically makes the installation of visualisation software a time consuming process. Should you really need to plot/visualise on R\u0101poi, there are several things you probably need to do. The first will be to have a suitable ssh client (see the section on the Accessing the Cluster page) and enable X11 forwarding by adding the -X option when you ssh into R\u0101poi. Other steps can vary greatly depending on your local operating system and exactly what you want to do. If you need assistance, reach out on the raapoi-help slack channel . MPI Tips \u00b6 Allocating tasks, threads and MPI processes - or how to parse the meanings of \"ntasks\", \"cpus-per-task\" and \"nodes\" in my sbatch scripts? source: C.E.C.I hpc docs you use mpi and do not care about where those cores are distributed: --ntasks=16 you want to launch 16 independent processes (no communication): --ntasks=16 you want those cores to spread across distinct nodes: --ntasks=16 --ntasks-per-node=1 or --ntasks=16 --nodes=16 you want 16 processes to spread across 8 nodes to have two processes per node: --ntasks=16 --ntasks-per-node=2 you want 16 processes to stay on the same node: --ntasks=16 --ntasks-per-node=16 you want one process that can use 16 cores for multithreading: - -ntasks=1 --cpus-per-task=16 you want 4 processes that can use 4 cores each for multithreading: --ntasks=4 --cpus-per-task=4 How do I increase the memory available to Java based applications? Add the command line option -Xms<n> where <n> should be replaced with the desired initial size of the memory allocation pool. For example, -Xms512m will result in an initial memory allocation pool of 512MB. Add the command line option -Xmx<n> where <n> should be replaced with the desired maximum size of the memory allocation pool. For example, -Xmx4096m will set the maximum size of the memory allocation pool as 4096MB (i.e. 4GB). The value supplied to these options obviously should not exceed the memory requested for a job through the srun/sbatch arguments. See the java documentation for more details. How do I know if I should use ntasks, cpus-per-tasks, etc.? ( 1 ) For demo purposes, you could try this mpi example on R\u0101poi: example_mpi A little complicated as it depends on whether your program needs tasks or cores ( 1 , 2 ). A simple difference to understand between MPI and OpenMP is that an MPI based program will be launced several times and communicates via messgae passing, while an OpenMP based program will only be launched once and will then launch several threads which communicate via shared memory. In case of a shared memory job, it is required that tasks run on the same node while a message passing job doesn't care about nodes as long as the tasks can communicate (via Infiniband, Ethernet, etc.). A general guide could be: For multiprocessing (MPI, message parssing) use ntasks . For multithreading (OpenMP, pthreads) use cpus-per-task - use 1 for MPI. For parallelized applications benchmark this is the number of threads. For hybrid codes, you need both options and probably also want to tune ntasks-per-node ; refers to the number of (MPI) processes per node. For OpenMP, only 1 task is necessary. In nodes with hyper-threading enabled, use --ntasks-per-core=1 to avoid distributing cores across sockets. --nodes=<num_nodes> with more than 1 node is useful for jobs with distributed-memory (e.g. MPI). The --ntasks option of SLURM specifies how many tasks your program will launch, which could be threads of independent instances of the MPI program. However, SLURM assumes that when you say --ntasks you mean tasks which communicate by message passing and in case your machine has 12 cores but you requested 13 tasks, it will happily launch 12 tasks on one node and 1 on another node. (I don't think this behaviour is guaranteed. SLURM could also throw all 13 tasks on one node with 12 CPUs and let the CPU schedule the tasks. You can get more fine-grained control using --ntasks-per-core and --ntasks-per-node .) In case of a multithreaded program, then you want to use --cpus-per-task instead and set --ntasks = 1 (or leave it unspecified, as it defaults to 1). This way if you request 13 CPUs but the maximum available is 12, your job will just be rejected. Note Check if your script uses MPI, if it does, it would benefit from running on multiple nodes simultaneously. If not, it doesn't make sense to request more than one node. For example, you made a mistake in your MPI code, you are running non-optimised simple applications on HPC like python or R scripts without utilising parallelism. OpenMP is a multiprocessing library is often used for programs on shared memory systems. Shared memory describes systems which share the memory between all processing units (CPU cores), so that each process can access all data on that system. If your job requires more than the default available memory per core (2GB/core on a 256 core node) you should adjust this need with the following command: #SBATCH --mem-per-cpu=10GB . When doing this, the batch system will automatically allocate 50 cores or less per node (for a 500GB node). VS Code tips \u00b6 How do I clean up my VS Code Server processes on R\u0101poi? Unfortunately, VS Code seems to have somewhat poor process management when used to connect to R\u0101poi for remote code development. In particular, it tends to leave a large number of processes sitting on the cluster occupying resources (even after you leave/close the session on your local machine). This link from the vscode docs tells you how to cleanup your VS Code Server. In particular, they recommend running kill -9 $(ps aux | grep vscode-server | grep $USER | grep -v grep | awk '{print $2}') to kill server processes. If you are a user of VS Code, please do this whenever we you finish a session. Dependent Jobs \u00b6 How do I start a job that depends on the previous job finishing successfully? Submit your first job sbatch submit1.sl . This will print a job id on your terminal Submitted batch job 1042939 . Now submit the job you would like to start once the previous job has completed successfully, using: sbatch --depend=afterok:1042939 submit2.sl Similar can be achieved for array jobs. If you need assistance, reach out on the raapoi-help slack channel . Here is an example: $ sbatch submit.sl Submitted batch job 1042939 $ sbatch --depend=afterok:1042939 submit.sl Submitted batch job 1042940 $ squeue --me JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 1042940 quicktest dependen janedoe PD 0:00 1 (Dependency) 1042939 quicktest dependen janedoe R 0:37 1 itl02n01 From the Bioinformatics Workbook Argument Description after The job begins executing after the specified job have begun executing afterany The job begins executing after the specified job have terminated aftercorr A task of this job array can begin executing after the corresponding task ID in the specified job has completed successfully afternotok This job can begin execution after the specified jobs have terminated in some failed state afterok This job can begin execution after the specified jobs have successfully executed singleton This job can begin execution after any previously launched jobs sharing the same job name and user have terminated COMSOL \u00b6 How do I simulate COMSOL model? COMSOL Docs Parameter SLURM COMSOL --nn Total number of Slurm Tasks Total number of Compute Nodes across all Slurm Nodes across all Hosts --nnhost Number of Slurm Tasks per Number of compute nodes to Slurm Node run on each host (*A COMSOL instance resides in each Slurm Task, and communicates with other Slurm Tasks using MPI) -np Number of Cores / CPUs used Number of Cores / CPUs to be by each Slurm Task used by each compute node For e.g., let's assume a workload distribution of starting two COMSOL instances each using four CPUs/cores on a single node would look like: #SBATCH --nodes=1 #SBATCH --ntasks-per-node=2 #SBATCH --cpus-per-task=4 <your stuff here> # COMSOL execution command /path/to/executable/comsol batch -nn 2 -nnhost 2 -np 4 \\ -inputfile <your${INPUTFILE}> ..... >> rest of the arguments Job Start \u00b6 How do I know when will my job start? You can find start time for some of the jobs in pending state using: squeue --start -j <JobID> Full Node \u00b6 How do I request a whole node? We request users to be very careful with their requirements for such a job. However, here's a simple guide to follow [ 1 ]: Parameter Function --nodes= Start a parallel job for a distributed memory system on several nodes --ntasks-per-node= Number of (MPI) processes per node. Maximum number depends on cores per node --cpus-per-task=1 User one CPU core per task --exclusive Job will not share nodes with other running jobs. You don't need to specify memory as you will get all available on the node. To distribute your job: Parameter Function --ntasks= Total number of (MPI) processes. Equal to the number of cores. --mem-per-cpu= Memory (RAM) per requested CPU core. You should run a few tests to see that your job is requested cpus that it can actually utilise efficiently. Try to run your job on 1, 2, 4, 8, 16, etc. cores to see when the runtime for your job starts tailing off. Job Failed \u00b6 What should I do when my job fails (or hangs)? See if you can get your job to run by yourself when you follow these steps: Make sure the job is a scaled down version of a huge problem. For example, it can be a single parameter of a huge parameter sweep study. A rule of thumb should be a job that can finish in about 10 - 15 mins. Make sure you have added #SBATCH --error = slurm_%j.err to your submission script. Look through the file to get a hint of the possible error that caused your job to fail. Check slurm exit code by sacct -o Exitcode -j <JobId> , look up on the web for help around the exit code. Exit codes: 0:53 . For example, here error 0:53 often means that something wasn't readable or writable.docs Look for some common problems, e.g., OOM which means out-of-memory and try to increase memory request; permissions error - look for if files/folders referenced in your script exist by manually checking the directories. Resubmit your batch script but this time include #!/bin/bash -x at the top of your submission script. Now, the error file will produce debugging information for further inquiries to make into the program's behaviour and identifying where it failed. If you software utilises OpenMPI or OpenBLAS, please check the related faq's and user guides to ensure you are following best practice with these pieces of software. If nothing works, send the script file, JobId, error file, and other logs to raapoi-help slack channel to ask for help from the admins. They know the system and may have helped another user do something similar. Bug Report \u00b6 How do I ask for help? If you need assistance, reach out on the raapoi-help slack channel . Good request for help will include: A JobID that you tried with a short description of the problem like what are you trying to achieve and what went wrong. If possible, attach your script and error file.","title":"FAQ"},{"location":"faq/#frequently-asked-questions","text":"","title":"Frequently Asked Questions"},{"location":"faq/#login-node","text":"Why can't I run programs directly on the login node? R\u0101poi login is a shared resource and running programs directly on the login node puts unnecessary strain. The recommended method is to use srun or sbatch . Please request an interactive session to work on the cluster, e.g., writing programs, debugging, or even data transfer.","title":"Login node"},{"location":"faq/#requesting-resources","text":"I don't want to interfere with other people work, what does it mean \"Other users are prevented from using resources you request, even if you don't use them\"? The system is shared, you will very rarely have full use of a node. You need to be careful to request resources, leaving the extra space available to others. For example, say you submitted a job to bigmem which asked for 800GB of ram and 10 CPUs. Your job would end up on the node with 1000GB ram as only that one would fit it - if your job actually only used 300GB of ram - the extra 500GB of ram you requested would be \"wasted\" no one else could use it. So, another user with a job requesting 600GB of ram would have to wait for your job to end even if there was space for it to run alongside yours. The same issue occurs with CPU requests. It can be very hard to accurately estimate memory and cpu needs before running your job. If your job has a short run time (less than ~10 hours), you can just request more than you need and check the memory usage afterward to guide further jobs. If your job has a long run time (several days), you should run a test job with a short runtime (a few hours) to estimate your needs first.","title":"Requesting Resources"},{"location":"faq/#job-requirements","text":"How do I know resource requirement for my job? The best and most reliable method of determining resource requirement is from testing. #Note: It is possible to find out a ballpark figure for a particular software in terms of memory, cpus, or time, for example: MATLAB (~16 GB memory to initialize), see documentation or consult in slack help channel. Select a test job carefully. As a rule of thumb, a test job should not run for more than 15 mins. Perhaps you could use smaller input, coarse parameters or use a subset of the calculations. Make sure your test job is quick to run and quick to start. The later can be ensured by keeping resource requirement to be small (mem or cpu). Often a good first test to run, is to execute your job serially e.g., using only 1 CPU. These jobs should be easier to debug, and quicker to run. Generally it is okay to request roughly 20% more time and memory than you think the job will use, any more than that can impact on other users.","title":"Job Requirements"},{"location":"faq/#visualisation","text":"How do I plot/visualise my results/data on R\u0101poi? Generally speaking, the best practice is to use R\u0101poi for your heavy computing workloads, then transfer your results/data to a local machine to do any plotting/visualisation. An exception to this might be if the visualisation process itself is computationally intensive, the dataset is large and difficult to move, and/or requires specialised hardware (like a gpu). The R\u0101poi operating system is primarily designed for command line use, and thus doesn't include most of the software libraries that support graphical interfaces. This typically makes the installation of visualisation software a time consuming process. Should you really need to plot/visualise on R\u0101poi, there are several things you probably need to do. The first will be to have a suitable ssh client (see the section on the Accessing the Cluster page) and enable X11 forwarding by adding the -X option when you ssh into R\u0101poi. Other steps can vary greatly depending on your local operating system and exactly what you want to do. If you need assistance, reach out on the raapoi-help slack channel .","title":"Visualisation"},{"location":"faq/#mpi-tips","text":"Allocating tasks, threads and MPI processes - or how to parse the meanings of \"ntasks\", \"cpus-per-task\" and \"nodes\" in my sbatch scripts? source: C.E.C.I hpc docs you use mpi and do not care about where those cores are distributed: --ntasks=16 you want to launch 16 independent processes (no communication): --ntasks=16 you want those cores to spread across distinct nodes: --ntasks=16 --ntasks-per-node=1 or --ntasks=16 --nodes=16 you want 16 processes to spread across 8 nodes to have two processes per node: --ntasks=16 --ntasks-per-node=2 you want 16 processes to stay on the same node: --ntasks=16 --ntasks-per-node=16 you want one process that can use 16 cores for multithreading: - -ntasks=1 --cpus-per-task=16 you want 4 processes that can use 4 cores each for multithreading: --ntasks=4 --cpus-per-task=4 How do I increase the memory available to Java based applications? Add the command line option -Xms<n> where <n> should be replaced with the desired initial size of the memory allocation pool. For example, -Xms512m will result in an initial memory allocation pool of 512MB. Add the command line option -Xmx<n> where <n> should be replaced with the desired maximum size of the memory allocation pool. For example, -Xmx4096m will set the maximum size of the memory allocation pool as 4096MB (i.e. 4GB). The value supplied to these options obviously should not exceed the memory requested for a job through the srun/sbatch arguments. See the java documentation for more details. How do I know if I should use ntasks, cpus-per-tasks, etc.? ( 1 ) For demo purposes, you could try this mpi example on R\u0101poi: example_mpi A little complicated as it depends on whether your program needs tasks or cores ( 1 , 2 ). A simple difference to understand between MPI and OpenMP is that an MPI based program will be launced several times and communicates via messgae passing, while an OpenMP based program will only be launched once and will then launch several threads which communicate via shared memory. In case of a shared memory job, it is required that tasks run on the same node while a message passing job doesn't care about nodes as long as the tasks can communicate (via Infiniband, Ethernet, etc.). A general guide could be: For multiprocessing (MPI, message parssing) use ntasks . For multithreading (OpenMP, pthreads) use cpus-per-task - use 1 for MPI. For parallelized applications benchmark this is the number of threads. For hybrid codes, you need both options and probably also want to tune ntasks-per-node ; refers to the number of (MPI) processes per node. For OpenMP, only 1 task is necessary. In nodes with hyper-threading enabled, use --ntasks-per-core=1 to avoid distributing cores across sockets. --nodes=<num_nodes> with more than 1 node is useful for jobs with distributed-memory (e.g. MPI). The --ntasks option of SLURM specifies how many tasks your program will launch, which could be threads of independent instances of the MPI program. However, SLURM assumes that when you say --ntasks you mean tasks which communicate by message passing and in case your machine has 12 cores but you requested 13 tasks, it will happily launch 12 tasks on one node and 1 on another node. (I don't think this behaviour is guaranteed. SLURM could also throw all 13 tasks on one node with 12 CPUs and let the CPU schedule the tasks. You can get more fine-grained control using --ntasks-per-core and --ntasks-per-node .) In case of a multithreaded program, then you want to use --cpus-per-task instead and set --ntasks = 1 (or leave it unspecified, as it defaults to 1). This way if you request 13 CPUs but the maximum available is 12, your job will just be rejected. Note Check if your script uses MPI, if it does, it would benefit from running on multiple nodes simultaneously. If not, it doesn't make sense to request more than one node. For example, you made a mistake in your MPI code, you are running non-optimised simple applications on HPC like python or R scripts without utilising parallelism. OpenMP is a multiprocessing library is often used for programs on shared memory systems. Shared memory describes systems which share the memory between all processing units (CPU cores), so that each process can access all data on that system. If your job requires more than the default available memory per core (2GB/core on a 256 core node) you should adjust this need with the following command: #SBATCH --mem-per-cpu=10GB . When doing this, the batch system will automatically allocate 50 cores or less per node (for a 500GB node).","title":"MPI Tips"},{"location":"faq/#vs-code-tips","text":"How do I clean up my VS Code Server processes on R\u0101poi? Unfortunately, VS Code seems to have somewhat poor process management when used to connect to R\u0101poi for remote code development. In particular, it tends to leave a large number of processes sitting on the cluster occupying resources (even after you leave/close the session on your local machine). This link from the vscode docs tells you how to cleanup your VS Code Server. In particular, they recommend running kill -9 $(ps aux | grep vscode-server | grep $USER | grep -v grep | awk '{print $2}') to kill server processes. If you are a user of VS Code, please do this whenever we you finish a session.","title":"VS Code tips"},{"location":"faq/#dependent-jobs","text":"How do I start a job that depends on the previous job finishing successfully? Submit your first job sbatch submit1.sl . This will print a job id on your terminal Submitted batch job 1042939 . Now submit the job you would like to start once the previous job has completed successfully, using: sbatch --depend=afterok:1042939 submit2.sl Similar can be achieved for array jobs. If you need assistance, reach out on the raapoi-help slack channel . Here is an example: $ sbatch submit.sl Submitted batch job 1042939 $ sbatch --depend=afterok:1042939 submit.sl Submitted batch job 1042940 $ squeue --me JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 1042940 quicktest dependen janedoe PD 0:00 1 (Dependency) 1042939 quicktest dependen janedoe R 0:37 1 itl02n01 From the Bioinformatics Workbook Argument Description after The job begins executing after the specified job have begun executing afterany The job begins executing after the specified job have terminated aftercorr A task of this job array can begin executing after the corresponding task ID in the specified job has completed successfully afternotok This job can begin execution after the specified jobs have terminated in some failed state afterok This job can begin execution after the specified jobs have successfully executed singleton This job can begin execution after any previously launched jobs sharing the same job name and user have terminated","title":"Dependent Jobs"},{"location":"faq/#comsol","text":"How do I simulate COMSOL model? COMSOL Docs Parameter SLURM COMSOL --nn Total number of Slurm Tasks Total number of Compute Nodes across all Slurm Nodes across all Hosts --nnhost Number of Slurm Tasks per Number of compute nodes to Slurm Node run on each host (*A COMSOL instance resides in each Slurm Task, and communicates with other Slurm Tasks using MPI) -np Number of Cores / CPUs used Number of Cores / CPUs to be by each Slurm Task used by each compute node For e.g., let's assume a workload distribution of starting two COMSOL instances each using four CPUs/cores on a single node would look like: #SBATCH --nodes=1 #SBATCH --ntasks-per-node=2 #SBATCH --cpus-per-task=4 <your stuff here> # COMSOL execution command /path/to/executable/comsol batch -nn 2 -nnhost 2 -np 4 \\ -inputfile <your${INPUTFILE}> ..... >> rest of the arguments","title":"COMSOL"},{"location":"faq/#job-start","text":"How do I know when will my job start? You can find start time for some of the jobs in pending state using: squeue --start -j <JobID>","title":"Job Start"},{"location":"faq/#full-node","text":"How do I request a whole node? We request users to be very careful with their requirements for such a job. However, here's a simple guide to follow [ 1 ]: Parameter Function --nodes= Start a parallel job for a distributed memory system on several nodes --ntasks-per-node= Number of (MPI) processes per node. Maximum number depends on cores per node --cpus-per-task=1 User one CPU core per task --exclusive Job will not share nodes with other running jobs. You don't need to specify memory as you will get all available on the node. To distribute your job: Parameter Function --ntasks= Total number of (MPI) processes. Equal to the number of cores. --mem-per-cpu= Memory (RAM) per requested CPU core. You should run a few tests to see that your job is requested cpus that it can actually utilise efficiently. Try to run your job on 1, 2, 4, 8, 16, etc. cores to see when the runtime for your job starts tailing off.","title":"Full Node"},{"location":"faq/#job-failed","text":"What should I do when my job fails (or hangs)? See if you can get your job to run by yourself when you follow these steps: Make sure the job is a scaled down version of a huge problem. For example, it can be a single parameter of a huge parameter sweep study. A rule of thumb should be a job that can finish in about 10 - 15 mins. Make sure you have added #SBATCH --error = slurm_%j.err to your submission script. Look through the file to get a hint of the possible error that caused your job to fail. Check slurm exit code by sacct -o Exitcode -j <JobId> , look up on the web for help around the exit code. Exit codes: 0:53 . For example, here error 0:53 often means that something wasn't readable or writable.docs Look for some common problems, e.g., OOM which means out-of-memory and try to increase memory request; permissions error - look for if files/folders referenced in your script exist by manually checking the directories. Resubmit your batch script but this time include #!/bin/bash -x at the top of your submission script. Now, the error file will produce debugging information for further inquiries to make into the program's behaviour and identifying where it failed. If you software utilises OpenMPI or OpenBLAS, please check the related faq's and user guides to ensure you are following best practice with these pieces of software. If nothing works, send the script file, JobId, error file, and other logs to raapoi-help slack channel to ask for help from the admins. They know the system and may have helped another user do something similar.","title":"Job Failed"},{"location":"faq/#bug-report","text":"How do I ask for help? If you need assistance, reach out on the raapoi-help slack channel . Good request for help will include: A JobID that you tried with a short description of the problem like what are you trying to achieve and what went wrong. If possible, attach your script and error file.","title":"Bug Report"},{"location":"ganglia/","text":"Visit here to get real-time metrics and history of R\u0101poi's utilisation. Ganglia example:","title":"Ganglia"},{"location":"hpclayout/","text":"HPC layout \u00b6 NOTE Understanding the R\u0101poi hardware layout is not critical for most users! It is useful for users running big parallel MPI jobs and may be of interest to others. To a first approximation, a High Performance Computer (HPC) is a collection of large computers or servers (nodes) that are connected together. There will also be some attached storage. Rather than logging into the system and immediately running your program or code, it is organised into a job and submitted to a scheduler that takes your job and runs it on one of the nodes that has enough free resources (cpu and memory) to meet your job request. Most of the time you will be sharing a node with other users. It is important to try and not over request resources as requested resources are kept in reserve for you and not available to others, even if you don't use them. This is particularly important when requesting a lot of resources or running array jobs which can use up a lot of the HPCs resources. Hardware \u00b6 On R\u0101poi, the node you login into and submit your jobs to is called raapoi-login . The computers/servers making up the nodes are of several types, covered in partitions . Most of the processors in R\u0101poi are in the parallel AMD nodes such as AMD01n01, AMD01n02 etc. Figures 1-4 show more details of these nodes. Figure 1: Example of some of the computers making up R\u0101poi. This is the front panel in Rack 4 in the Datacentre - highlighted is one of the AMD chassis, which have 4 nodes each. Figure 2: Example of some of the computers making up R\u0101poi. This is the back in Rack 4 in the Datacentre. Here you can clearly see the 4 nodes in each chassis of the parallel partition Figure 3: An AMD compute node, one of 4 in a chassis. The 2 black rectangles are the processor heatsinks, on each side are the ram modules. Each ram module is 32GB for a total of 512GB. On the lower left, the green circuit board is the the InfiniBand network card. Opposite that, in black, is the 1.7TB NvMe storage we use as fast /tmp space. Figure 4: One of the CPUs with the heatsink removed. At 115.00 x 165.00 mm, it is physically much larger than the processor in a desktop Each AMD node has 2 of these 7702 processors. Each processor has 64Cores/128Threads (with SMT - symmetric multi-threading - enabled) for a total of 128Cores/256Threads per node. Network \u00b6 On R\u0101poi the nodes are connected to each other in two ways - via 10G ethernet and via 52G infiniband. Most of the time you can ignore this, but it is important for interconnected jobs running across multiple nodes like weather simulations. In figure 5 we can see the network layout of R\u0101poi from the perspective of the Login node. This is the node you ssh into, via the VUW intranet - either from a locally wired connection or via the VPN. The nodes are organised into groups mostly aligning with the partition the node is in. Ethernet \u00b6 The dashed lines indicate an Ethernet connection, all nodes are connected via ethernet at either 1G or 10G depending on the node. Most of the intel nodes are only connected at 1G due to their age. The newer nodes are all 10G connected. The ethernet connection can also reach out into the wider internet for downloading updates, datasets etc. Infiniband \u00b6 Many nodes are also connected by a solid line indicating an Infiniband network connection. This connection is faster than the ethernet connection but more importantly lower latency than the ethernet connection. This helps with large interconnected (eg MPI) jobs running across multiple nodes. The latency of the interprocess communication carried over the Infiniband link can have a dramatic affect on large scale calculations which for instance need to communicate grid boundary conditions across the nodes Where infiniband is available, the scratch storage is transmitted over the link as the latency helps with IO performance. classDiagram Parallel_AMD -- Login Parallel_AMD .. Login Quicktest -- Login Quicktest .. Login Bigmem .. Login GPU .. Login Login .. Internet Login .. Scratch Login -- Scratch Login .. Longrun Login -- Longrun class Internet { vuw intranet wider internet } class Scratch { 100 TB raapoizfs01 } class Longrun { bigtmp01 bigtmp02 } class Login { raapoi-login } class Parallel_AMD { amd01n01 amd01n02 amd01n03 amd01n04 - amd02n01 amd02n02 amd02n03 amd02n04 - amd03n01 amd03n02 amd03n03 amd03n04 - amd04n01 amd04n02 amd04n03 amd04n04 - amd05n01 amd05n02 amd05n03 amd05n04 - amd06n01 amd06n02 amd06n03 amd06n04 - amd07n01 amd07n02 amd07n03 amd07n04 } class Quicktest{ itl02n01 itl02n02 itl02n03 itl02n04 - itl03n01 itl03n02 } class Bigmem{ high01 high02 high03 high04 } class GPU{ gpu01 gpu02 gpu03 } Figure 5: Logical HPC layout from the perspective of the login node. Solid lines indicate ethernet connections, dashed Infiniband Looking at the HPC from the perspective of the ethernet and infiniband networks. The nodes in Figure 6 and 7 are the same as before, but we're just using the group container label to simplify the diagram. classDiagram Parallel_AMD .. Ethernet Quicktest .. Ethernet Bigmem .. Ethernet GPU .. Ethernet Ethernet .. Internet Ethernet .. Login Ethernet .. Scratch Ethernet .. Longrun class Ethernet{ 1-10Gig Connects to the wider internet } class Scratch { } class BeeGFS { } class Login { } class Parallel_AMD { } class Quicktest{ } class Bigmem{ } class GPU{ } Figure 6: Logical HPC layout from the perspective of the ethernet connections. Node layout is the same as in Figure 5, but only the group headings have been retained. classDiagram Parallel_AMD -- Login Parallel_AMD .. Login Quicktest -- Login Quicktest .. Login Bigmem .. Login GPU .. Login Login .. Internet Login .. Scratch Login -- Scratch Login .. Longrun Login -- Longrun class Internet { vuw intranet wider internet } class Scratch { 100 TB raapoizfs01 } class Longrun { bigtmp01 bigtmp02 } class Login { raapoi-login } class Parallel_AMD { } class Quicktest{ } class Bigmem{ } class GPU{ } Figure 7: Logical HPC layout from the perspective of the Infiniband connections. Note that not all nodes are connected via the infiniband link! Node layout is the same as in Figure 5, but only the group headings have been retained. The Infiniband nodes are connected to one of two SX6036 Infiniband switches. The intel and quicktest and login nodes are connected to one switch. Everything else is connected to the the other. The switches are broadly interconnected, but there is as small latency penalty for crossing the switch.","title":"HPC Hardware Layout"},{"location":"hpclayout/#hpc-layout","text":"NOTE Understanding the R\u0101poi hardware layout is not critical for most users! It is useful for users running big parallel MPI jobs and may be of interest to others. To a first approximation, a High Performance Computer (HPC) is a collection of large computers or servers (nodes) that are connected together. There will also be some attached storage. Rather than logging into the system and immediately running your program or code, it is organised into a job and submitted to a scheduler that takes your job and runs it on one of the nodes that has enough free resources (cpu and memory) to meet your job request. Most of the time you will be sharing a node with other users. It is important to try and not over request resources as requested resources are kept in reserve for you and not available to others, even if you don't use them. This is particularly important when requesting a lot of resources or running array jobs which can use up a lot of the HPCs resources.","title":"HPC layout"},{"location":"hpclayout/#hardware","text":"On R\u0101poi, the node you login into and submit your jobs to is called raapoi-login . The computers/servers making up the nodes are of several types, covered in partitions . Most of the processors in R\u0101poi are in the parallel AMD nodes such as AMD01n01, AMD01n02 etc. Figures 1-4 show more details of these nodes. Figure 1: Example of some of the computers making up R\u0101poi. This is the front panel in Rack 4 in the Datacentre - highlighted is one of the AMD chassis, which have 4 nodes each. Figure 2: Example of some of the computers making up R\u0101poi. This is the back in Rack 4 in the Datacentre. Here you can clearly see the 4 nodes in each chassis of the parallel partition Figure 3: An AMD compute node, one of 4 in a chassis. The 2 black rectangles are the processor heatsinks, on each side are the ram modules. Each ram module is 32GB for a total of 512GB. On the lower left, the green circuit board is the the InfiniBand network card. Opposite that, in black, is the 1.7TB NvMe storage we use as fast /tmp space. Figure 4: One of the CPUs with the heatsink removed. At 115.00 x 165.00 mm, it is physically much larger than the processor in a desktop Each AMD node has 2 of these 7702 processors. Each processor has 64Cores/128Threads (with SMT - symmetric multi-threading - enabled) for a total of 128Cores/256Threads per node.","title":"Hardware"},{"location":"hpclayout/#network","text":"On R\u0101poi the nodes are connected to each other in two ways - via 10G ethernet and via 52G infiniband. Most of the time you can ignore this, but it is important for interconnected jobs running across multiple nodes like weather simulations. In figure 5 we can see the network layout of R\u0101poi from the perspective of the Login node. This is the node you ssh into, via the VUW intranet - either from a locally wired connection or via the VPN. The nodes are organised into groups mostly aligning with the partition the node is in.","title":"Network"},{"location":"hpclayout/#ethernet","text":"The dashed lines indicate an Ethernet connection, all nodes are connected via ethernet at either 1G or 10G depending on the node. Most of the intel nodes are only connected at 1G due to their age. The newer nodes are all 10G connected. The ethernet connection can also reach out into the wider internet for downloading updates, datasets etc.","title":"Ethernet"},{"location":"hpclayout/#infiniband","text":"Many nodes are also connected by a solid line indicating an Infiniband network connection. This connection is faster than the ethernet connection but more importantly lower latency than the ethernet connection. This helps with large interconnected (eg MPI) jobs running across multiple nodes. The latency of the interprocess communication carried over the Infiniband link can have a dramatic affect on large scale calculations which for instance need to communicate grid boundary conditions across the nodes Where infiniband is available, the scratch storage is transmitted over the link as the latency helps with IO performance. classDiagram Parallel_AMD -- Login Parallel_AMD .. Login Quicktest -- Login Quicktest .. Login Bigmem .. Login GPU .. Login Login .. Internet Login .. Scratch Login -- Scratch Login .. Longrun Login -- Longrun class Internet { vuw intranet wider internet } class Scratch { 100 TB raapoizfs01 } class Longrun { bigtmp01 bigtmp02 } class Login { raapoi-login } class Parallel_AMD { amd01n01 amd01n02 amd01n03 amd01n04 - amd02n01 amd02n02 amd02n03 amd02n04 - amd03n01 amd03n02 amd03n03 amd03n04 - amd04n01 amd04n02 amd04n03 amd04n04 - amd05n01 amd05n02 amd05n03 amd05n04 - amd06n01 amd06n02 amd06n03 amd06n04 - amd07n01 amd07n02 amd07n03 amd07n04 } class Quicktest{ itl02n01 itl02n02 itl02n03 itl02n04 - itl03n01 itl03n02 } class Bigmem{ high01 high02 high03 high04 } class GPU{ gpu01 gpu02 gpu03 } Figure 5: Logical HPC layout from the perspective of the login node. Solid lines indicate ethernet connections, dashed Infiniband Looking at the HPC from the perspective of the ethernet and infiniband networks. The nodes in Figure 6 and 7 are the same as before, but we're just using the group container label to simplify the diagram. classDiagram Parallel_AMD .. Ethernet Quicktest .. Ethernet Bigmem .. Ethernet GPU .. Ethernet Ethernet .. Internet Ethernet .. Login Ethernet .. Scratch Ethernet .. Longrun class Ethernet{ 1-10Gig Connects to the wider internet } class Scratch { } class BeeGFS { } class Login { } class Parallel_AMD { } class Quicktest{ } class Bigmem{ } class GPU{ } Figure 6: Logical HPC layout from the perspective of the ethernet connections. Node layout is the same as in Figure 5, but only the group headings have been retained. classDiagram Parallel_AMD -- Login Parallel_AMD .. Login Quicktest -- Login Quicktest .. Login Bigmem .. Login GPU .. Login Login .. Internet Login .. Scratch Login -- Scratch Login .. Longrun Login -- Longrun class Internet { vuw intranet wider internet } class Scratch { 100 TB raapoizfs01 } class Longrun { bigtmp01 bigtmp02 } class Login { raapoi-login } class Parallel_AMD { } class Quicktest{ } class Bigmem{ } class GPU{ } Figure 7: Logical HPC layout from the perspective of the Infiniband connections. Note that not all nodes are connected via the infiniband link! Node layout is the same as in Figure 5, but only the group headings have been retained. The Infiniband nodes are connected to one of two SX6036 Infiniband switches. The intel and quicktest and login nodes are connected to one switch. Everything else is connected to the the other. The switches are broadly interconnected, but there is as small latency penalty for crossing the switch.","title":"Infiniband"},{"location":"managing_jobs/","text":"Managing Jobs \u00b6 Cancelling a Job \u00b6 To cancel a job, first find the jobID, you can use the vuw-myjobs (or squeue ) command to see a list of your jobs, including jobIDs. Once you have that you can use the scancel command, eg scancel 236789 To cancel all of your jobs you can use the -u flag followed by your username: scancel -u <username> Note: Before cancelling your jobs, please make sure it runs for at least 2 mins, including the jobs submitted in error. Viewing Job information \u00b6 Job History \u00b6 If you want to get a quick view of all the jobs completed within the last 5 days you can use the vuw-job-history command, for example: $ vuw-job-history MY JOBS WITHIN LAST 5 days JobID State JobName MaxVMSize CPUTime ------------ ---------- ---------- ---------- ---------- 2645 COMPLETED bash 00:00:22 2645.extern COMPLETED extern 0.15G 00:00:22 2645.0 COMPLETED bash 0.22G 00:00:20 2734 COMPLETED bash 00:07:40 2734.extern COMPLETED extern 0.15G 00:07:40 2734.0 COMPLETED bash 0.22G 00:07:40 Job Reports \u00b6 To view a report of your past jobs you can run vuw-job-report : $ vuw-job-report 162711 JOB REPORT FOR JOB 162711 JobName Nodes ReqMem UsedMem(GB) ReqCPUs CPUTime State Completed test-schro 1 64Gn 24 00:02.513 COMPLETED 2019-05-28T16:17:10 batch 1 64Gn 0.15G 24 00:00.210 COMPLETED 2019-05-28T16:17:10 extern 1 64Gn 0.15G 24 00:00.002 COMPLETED 2019-05-28T16:17:10 NOTE: In this example you see that I requested 64 GigaBytes of memory but only used 0.15 GB. This means that 63 GB of memory went unused, which was a waste of resources. You can also get a report of your completed jobs using the sacct command. For example if I wanted to get a report on how much memory my job used I could do the following: sacct --units=G --format=\"MaxVMSize\" -j 2156 MaxVMSize will report the maximum virtual memory (RAM plus swap space) used by my job in GigBytes ( --units=G ) -j 2156 shows the information for job ID 2156 type man sacct at a prompt in engaging to see the documentation on the sacct command Viewing jobs in the Queue \u00b6 To view your running jobs you can type vuw-myjobs eg: $ vuw-myjobs JOBID PARTITION CPUS MIN_MEM GPUs RUN_TIME TIME_LIMIT PRIORITY STATE NODELIST JOB_NAME 2039592_[9-10] parallel 9 180G 0 2025-04-28T16:27:25 10-00:00:00 27365 PENDING (Resources) MLMAP 2042371_[11-20] parallel 9 180G 0 2025-04-29T15:46:36 10-00:00:00 24560 PENDING (Priority) MLMAP 2042372_[11-20] parallel 9 180G 0 2025-04-29T15:46:36 10-00:00:00 24560 PENDING (Priority) MLMAP 2039591_1 parallel 10 200G 0 11:17:24 10-00:00:00 24484 RUNNING amd07n02 MLMAP 2039591_2 parallel 10 200G 0 11:17:24 10-00:00:00 24484 RUNNING amd06n03 MLMAP 2039591_3 parallel 10 200G 0 11:17:24 10-00:00:00 24484 RUNNING amd06n04 MLMAP The vuw-myjobs command will show you all your jobs - running and pending , the partition they are in, the requested resources, the priority of the job and the state of the job. For the jobs in PENDING state, the reason for the job being in that state is also shown and an expectation of when the job will start. You can see all the jobs in the queues by running the vuw-alljobs command. This will produce a very long list of jobs if the cluster is busy. Note By default, each user is allowed 1,024 CPU cores , 2,048 GB of memory and 3 GPUs at any one time. Once you reach these limits, your jobs will be queued until resources are available. These limits are set to ensure that all users have fair access to the cluster. If you need more resources, please contact the R\u0101poi support team . Job Queuing (aka Why isn't my job running?) \u00b6 When a partition is busy, jobs will be placed in a queue. You can observe this in the vuw-myjobs and vuw-alljobs commands. The STATE of your job will be PENDING, this means it is waiting for resources or your job has been re-prioritized to allow other users access to run their jobs (this is called fair-share queueing). The resource manager will list a reason the job is pending, these reasons can include: Priority - Your job priority has been reduced to allow other users access to the cluster. If no other user with normal priority is also pending then your job will start once resources are available. Possible reasons why your priority has been lowered can include: the number of jobs you have run in the past 24-48 hours; the duration of the job and the amount of resources requested. The Slurm manager uses fair-share queuing to ensure the best use of the cluster. You can google fair-share queuing if you want to know more Resources - There are insufficient resources to start your job. Some combination of CPU, Memory, Time or other specialized resource are unavailable. Once resources are freed up your job will begin to run. Time: If you request more time than the max run-time of a partition, your job will be queued indefinitely (in other words: it will never run). Your time request must be less than or equal to the Partition Max Run-Time. Also if a special reservation is placed on the cluster, for instance prior to a scheduled maintenance, this too will reduce the available time to run your job. You can see Max Run-Time for our partitions described in this document. CAD or ITS Staff will alert all users prior to any scheduled maintenance and advise them of the time restrictions. QOSGrpCPULimit - This is a Quality of Service configuration to limit the number of CPUs per user. The QOSMax is the maximum that can be requested for any single job. If a user requests more CPUs than the QOSMax for a single job then the job will not run. If the user requests more than QOSMax in 2 or more jobs then the subsequent jobs will queue until the users running jobs complete. PartitionTimeLimit - This means you have requested more time than the maximum runtime of the partition. This document contains information about the different partitions, including max run-time. Typing vuw-partitions will also show the max run-time for the partitions available to you. ReqNodeNotAvail - 99% of the time you will receive this code if you have asked for too much time. This frequently occurs when the cluster is about to go into maintenance and a reservation has been placed on the cluster, which reduces the maximum run-time of all jobs. For example, if maintenance on the cluster is 1 week away, the maximum run-time on all jobs needs to be less than 1 week, regardless if the configured maximum run-time on a partition is greater than 1 week. To request time you can use the --time parameter. Another issue is if you request too much memory or a CPU configuration that does not exist on any node in a partition. Required node not available (down, drained or reserved) - This is related to ReqNodeNotAvail, see above.","title":"Managing Jobs"},{"location":"managing_jobs/#managing-jobs","text":"","title":"Managing Jobs"},{"location":"managing_jobs/#cancelling-a-job","text":"To cancel a job, first find the jobID, you can use the vuw-myjobs (or squeue ) command to see a list of your jobs, including jobIDs. Once you have that you can use the scancel command, eg scancel 236789 To cancel all of your jobs you can use the -u flag followed by your username: scancel -u <username> Note: Before cancelling your jobs, please make sure it runs for at least 2 mins, including the jobs submitted in error.","title":"Cancelling a Job"},{"location":"managing_jobs/#viewing-job-information","text":"","title":"Viewing Job information"},{"location":"managing_jobs/#viewing-jobs-in-the-queue","text":"To view your running jobs you can type vuw-myjobs eg: $ vuw-myjobs JOBID PARTITION CPUS MIN_MEM GPUs RUN_TIME TIME_LIMIT PRIORITY STATE NODELIST JOB_NAME 2039592_[9-10] parallel 9 180G 0 2025-04-28T16:27:25 10-00:00:00 27365 PENDING (Resources) MLMAP 2042371_[11-20] parallel 9 180G 0 2025-04-29T15:46:36 10-00:00:00 24560 PENDING (Priority) MLMAP 2042372_[11-20] parallel 9 180G 0 2025-04-29T15:46:36 10-00:00:00 24560 PENDING (Priority) MLMAP 2039591_1 parallel 10 200G 0 11:17:24 10-00:00:00 24484 RUNNING amd07n02 MLMAP 2039591_2 parallel 10 200G 0 11:17:24 10-00:00:00 24484 RUNNING amd06n03 MLMAP 2039591_3 parallel 10 200G 0 11:17:24 10-00:00:00 24484 RUNNING amd06n04 MLMAP The vuw-myjobs command will show you all your jobs - running and pending , the partition they are in, the requested resources, the priority of the job and the state of the job. For the jobs in PENDING state, the reason for the job being in that state is also shown and an expectation of when the job will start. You can see all the jobs in the queues by running the vuw-alljobs command. This will produce a very long list of jobs if the cluster is busy. Note By default, each user is allowed 1,024 CPU cores , 2,048 GB of memory and 3 GPUs at any one time. Once you reach these limits, your jobs will be queued until resources are available. These limits are set to ensure that all users have fair access to the cluster. If you need more resources, please contact the R\u0101poi support team .","title":"Viewing jobs in the Queue"},{"location":"managing_jobs/#job-queuing-aka-why-isnt-my-job-running","text":"When a partition is busy, jobs will be placed in a queue. You can observe this in the vuw-myjobs and vuw-alljobs commands. The STATE of your job will be PENDING, this means it is waiting for resources or your job has been re-prioritized to allow other users access to run their jobs (this is called fair-share queueing). The resource manager will list a reason the job is pending, these reasons can include: Priority - Your job priority has been reduced to allow other users access to the cluster. If no other user with normal priority is also pending then your job will start once resources are available. Possible reasons why your priority has been lowered can include: the number of jobs you have run in the past 24-48 hours; the duration of the job and the amount of resources requested. The Slurm manager uses fair-share queuing to ensure the best use of the cluster. You can google fair-share queuing if you want to know more Resources - There are insufficient resources to start your job. Some combination of CPU, Memory, Time or other specialized resource are unavailable. Once resources are freed up your job will begin to run. Time: If you request more time than the max run-time of a partition, your job will be queued indefinitely (in other words: it will never run). Your time request must be less than or equal to the Partition Max Run-Time. Also if a special reservation is placed on the cluster, for instance prior to a scheduled maintenance, this too will reduce the available time to run your job. You can see Max Run-Time for our partitions described in this document. CAD or ITS Staff will alert all users prior to any scheduled maintenance and advise them of the time restrictions. QOSGrpCPULimit - This is a Quality of Service configuration to limit the number of CPUs per user. The QOSMax is the maximum that can be requested for any single job. If a user requests more CPUs than the QOSMax for a single job then the job will not run. If the user requests more than QOSMax in 2 or more jobs then the subsequent jobs will queue until the users running jobs complete. PartitionTimeLimit - This means you have requested more time than the maximum runtime of the partition. This document contains information about the different partitions, including max run-time. Typing vuw-partitions will also show the max run-time for the partitions available to you. ReqNodeNotAvail - 99% of the time you will receive this code if you have asked for too much time. This frequently occurs when the cluster is about to go into maintenance and a reservation has been placed on the cluster, which reduces the maximum run-time of all jobs. For example, if maintenance on the cluster is 1 week away, the maximum run-time on all jobs needs to be less than 1 week, regardless if the configured maximum run-time on a partition is greater than 1 week. To request time you can use the --time parameter. Another issue is if you request too much memory or a CPU configuration that does not exist on any node in a partition. Required node not available (down, drained or reserved) - This is related to ReqNodeNotAvail, see above.","title":"Job Queuing (aka Why isn't my job running?)"},{"location":"mods_admins/","text":"Moderating a Slurm Cluster \u00b6 Expectations of moderators \u00b6 If you are responding to a user query in the raapoi-help or software-request channels, please leave a message indicating you are looking into the problem, and subsequently post a message when it has been completed. This ensures that other moderators/admins are aware that a request is currently being handled so that they need not double handle it. If you get stuck on a problem, let other moderators/admins know in the mod-chat channel so they can jump in and help. If you are taking action with respect to something not directly related to a request in the raapoi-help or software-request channels, please make sure you make a note/record of this in the mod-chat channel on slack. This might include making unsolicited software updates or limiting a user's access to resources to address a fair use issue in the queue. Posting a clear message indicating what you have done and why helps to ensure transparency with regards to response to various issues on the cluster and, once more, reduce instances of double handling. Dealing with jobs in the queue \u00b6 This a list of common cluster moderator actions, provided as reference. Users without moderator privileges might find some of this of interest, but you won't be able to perform the actions that affect other users. These commands will require you to be logged in with your moderator-specific account. Dealing with normal jobs \u00b6 Extending jobs \u00b6 Jobs are normally given a limit of 10 days to run. If a little longer is needed and there is no reason such as upcoming maintenance then jobs can be given a bit longer to complete: # To set a new total run time for a job of 13 days, 23 hours, 59 minutes and 59 seconds scontrol update jobid =< jobid > TimeLimit = 13 - 23 : 59 : 59 # Extend the job by 3 days, 23 hours, 59 minutes and 59 seconds (the + extends the time) scontrol update jobid =< jobid > TimeLimit =+ 3 - 23 : 59 : 59 Dealing with badly behaved jobs \u00b6 Holding jobs \u00b6 Users will occasionally run jobs which consume an unfair amount of resources, if a single user is causes problems, you can hold their jobs. This won't stop their current jobs, but will prevent more from starting # hold some jobs scontrol hold jobid1,jobid2,etc # Allow the jobs back onto the queue scontrol requeue jobid1,jobid2,etc ## previous step sets priority to zero \\ ## so they won't actually start now # Release the jobs to run again scontrol release jobid1,jobid2,etc Alterativly you can reduce their priority to a low setting squeue -p gpu -u <username> -t pending --format \\ \"scontrol update jobid=%i nice=1000000\" | sh Cancelling jobs \u00b6 If a users jobs are causing too many problems, you can cancel their jobs. Note this is drastic and can throw away many days of compute, it's best to try get hold of a user first. Get them to cancel their own jobs. If needed though: scancel <jobid> # be careful to get the correct job id! # to cancel all their running jobs on parallel squeue -p parallel -u <username> -t running --format \"scancel %i\" | sh Limiting user's resource allowance on Raapoi \u00b6 Limiting a user's access to resources should generally be a last resort, i.e. after an educative approach has been unsuccessful. Note that placing a limit on resources will not affect jobs which are already running on the cluster (i.e. even if they exceed any newly imposed limits), it will only affect any pending or newly submitted jobs to the queue. Set maxjobs \u00b6 sacctmgr modify user where name = bob set MaxJobs = 2 After a few minutes you should be able to see the results on squeue squeue -u bob -o \"%i %r\" # returns something like JOBID REASON 20582 AssocMaxJobsLimit 20583 Dependency Show User CPU restruction details \u00b6 sacctmgr show assoc where user = write_username_here Limiting CPU resources \u00b6 sacctmgr modify user <user> set MaxTRES = cpu = 1026 Limiting Memory (RAM) resources \u00b6 This is a reference, but note that this may have unintended consequences. Please consult other moderators on Slack before proceeding with this. sacctmgr modify user <user> set MaxTRES = mem = 1000G # This is 1TB of Ram Limiting GPU resources \u00b6 sacctmgr modify user bob set MaxTRES = cpu = -1,mem = -1,gres/gpu = 4 # -1 means no restriction. # check result sacctmgr list assoc User = bob Using reservations \u00b6 If a research group has a good need and the other moderators agree, you can give them a reservation that only they can use. This is usually done for a specific time period. This is also one of the steps when we put the cluster into maintenance Create a month-long reservation on amd01n01 and amd01n02 scontrol create reservationname = MyReservation starttime = 2021 -03-01T11:00:00 \\ duration = 30 -00:00:00 user = user1,user2,user3 nodes = amd01n01,amd01n02 Users will use the reservation with #SBATCH --reservation=MyReservation Building software with EasyBuild \u00b6 Caution: EasyBuild v5.0 and above includes a number of backwards-incompatible changes, see easybuild-v5/#breaking-changes . One issue is related to where easybuild looks for config files. As a workaround, first check whether easybuild can find our config files using eb --show-default-configfiles . If the output indicates that no config files are found, you may need to add the argument --configfiles=/etc/easybuild.d/config.cfg when building with easybuild to ensure software is correctly built into the correct directories on the cluster. If in doubt, check with an experienced moderator/admin. (You could also try loading and using a prior easybuild version, such as module load EasyBuild/4.9.4 .) Use a terminal multiplexer like screen, tmux or byobu to keep your ssh session alive and get a interactive session on a node. Build a simple program \u00b6 Here we will build a simple program called velvet - it's a genome assembler. srun -c 10 --mem = 10G -p parallel --time = 6 :00:00 --pty bash # now on the node module purge # just in case module load EasyBuild # Search for the package eb -S velvet # Returns * $CFGS1 /v/Velvet/Velvet-1.2.10-GCC-8.3.0-mt-kmer_191.eb * $CFGS1 /v/Velvet/Velvet-1.2.10-GCC-11.2.0-mt-kmer_191.eb * $CFGS1 /v/Velvet/Velvet-1.2.10-foss-2018a-mt-kmer_191.eb * $CFGS1 /v/Velvet/Velvet-1.2.10-foss-2018b-mt-kmer_191.eb * $CFGS1 /v/Velvet/Velvet-1.2.10-intel-2017a-mt-kmer_37.eb # We want to pick one that won't need to build a whole new toolchain if we can avoid it # Let's have a look at what would get built with a Dry (D) run. The r is for robot to # find all the dependancies eb -Dr Velvet-1.2.10-intel-2017a-mt-kmer_37.eb # Partial return * [ x ] $CFGS /m/M4/M4-1.4.17.eb ( module: Core | M4/1.4.17 ) * [ x ] $CFGS /b/Bison/Bison-3.0.4.eb ( module: Core | Bison/3.0.4 ) * [ x ] $CFGS /f/flex/flex-2.6.0.eb ( module: Core | flex/2.6.0 ) * [ x ] $CFGS /z/zlib/zlib-1.2.8.eb ( module: Core | zlib/1.2.8 ) * [ ] $CFGS /b/binutils/binutils-2.27.eb ( module: Core | binutils/2.27 ) * [ ] $CFGS /g/GCCcore/GCCcore-6.3.0.eb ( module: Core | GCCcore/6.3.0 ) * [ ] $CFGS /z/zlib/zlib-1.2.11-GCCcore-6.3.0.eb * [ ] $CFGS /h/help2man/help2man-1.47.4-GCCcore-6.3.0.eb * [ ] $CFGS /m/M4/M4-1.4.18-GCCcore-6.3.0.eb * [ ] $CFGS /b/Bison/Bison-3.0.4-GCCcore-6.3.0.eb * [ ] $CFGS /f/flex/flex-2.6.3-GCCcore-6.3.0.eb * [ ] $CFGS /i/icc/icc-2017.1.132-GCC-6.3.0-2.27.eb * [ ] $CFGS /i/ifort/ifort-2017.1.132-GCC-6.3.0-2.27.eb * [ ] $CFGS /i/iccifort/iccifort-2017.1.132-GCC-6.3.0-2.27.eb * [ ] $CFGS /i/impi/impi-2017.1.132-iccifort-2017.1.132-GCC-6.3.0-2.27.eb * [ ] $CFGS /i/iimpi/iimpi-2017a.eb ( module: Core | iimpi/2017a ) * [ ] $CFGS /i/imkl/imkl-2017.1.132-iimpi-2017a.eb * [ ] $CFGS /i/intel/intel-2017a.eb ( module: Core | intel/2017a ) * [ ] $CFGS /v/Velvet/Velvet-1.2.10-intel-2017a-mt-kmer_37.eb # Packages wiht a [x] are already built, [ ] will need to be built. This is a lot of building,, including a \"new\" compiler - intel-2017a.eb, let's avoid that and try another eb -Dr Velvet-1.2.10-foss-2018b-mt-kmer_191.eb # Partially returns, all [x] except for velvet ... * [ x ] $CFGS /f/FFTW/FFTW-3.3.8-gompi-2018b.eb * [ x ] $CFGS /s/ScaLAPACK/ScaLAPACK-2.0.2-gompi-2018b-OpenBLAS-0.3.1.eb * [ x ] $CFGS /f/foss/foss-2018b.eb ( module: Core | foss/2018b ) * [ ] $CFGS /v/Velvet/Velvet-1.2.10-foss-2018b-mt-kmer_191.eb # Before you proceed to build, make sure appropriate folder permissions will be set umask 0002 # To build this we would eb -r --parallel = $SLURM_CPUS_PER_TASK Velvet-1.2.10-foss-2018b-mt-kmer_191.eb # Afterwards, reset the default permissions for anything else you go on to do umask 0022 Remember to close the interactive session when done to stop it consuming resources. Building a new toolchain \u00b6 Below we ask for 10cpus, 10G memory and 6 hours. Really long rebuilds might need more time and or cpu/memory. srun -c 10 --mem = 10G -p parallel --time = 6 :00:00 --pty bash # now on the node module purge # just in case module load EasyBuild # Search for the package eb -S foss-2022a # There is a long output as that toochain gets used in many packages, but we can see: $CFGS1 /f/foss/foss-2022a.eb # Check what will be built # BE CAUTIOUS OF OPENMPI builds - the .eb file needs to be changed to use pmi2 rather than pmix each time! eb -Dr foss-2022a.eb # Trigger the build - this might take a long time, you could add more cpus or time if needed # (making sure permissions will be set appropriately, as described above) umask 0002 eb -r --parallel = $SLURM_CPUS_PER_TASK foss-2022a.eb umask 0022 Rebuilding an existing package \u00b6 This might be needed for some old packages after the move to Rocky 8 Below we ask for 10cpus, 10G memory and 6 hours. Really long rebuilds might need more time and or cpu/memory. srun -c 10 --mem = 10G -p parallel --time = 6 :00:00 --pty bash # now on the node module purge # just in case module load EasyBuild # Example of finding the problem ldd /home/software/apps/samtools/1.10/bin/samtools | grep found libncursesw.so.5 = > not found libtinfo.so.5 = > not found # See the what got build for samtools eb -Dr /home/software/EasyBuild/ebfiles_repo/SAMtools/SAMtools-1.10-GCC-8.3.0.eb # There are a few options # ncurses-6.0.eb # ncurses-6.1-GCCcore-8.3.0.eb # let's try one (making sure permissions will be set appropriately, as described above) umask 0002 eb -r --parallel = 10 --rebuild ncurses-6.0.eb umask 0022 # test once done Upgrading easybuild with Easybuild \u00b6 Get an interactice session on a node, then module load EasyBuild # will load latest version by default eb --version # see version eb --install-latest-eb-release # upgrade - will create new module file for new version Building New Version of Schrodinger Suite \u00b6 Schr\u00f6dinger Suite releases new versions quarterly, it's good practice to keep up to date with the latest version of the software. To build the new version, first download the tar file from the Schr\u00f6dinger website (www.schrodinger.com), then move the installation tar file to the directory /home/software/src/Schrodinger on R\u0101poi. Quick Installation \u00b6 First, extract the tar file tar -xvf Schrodinger_Download.tar Change to the top-level directory of the download cd Schrodinger_Download Then, run the instalaation script sh ./INSTALL Answer y/n to prompts from the INSTALL script, then all packages should be installed. Note During installation, you will be asked to confirm the installation directory , this is /home/software/apps/Schrodinger/2023-3 , '2023-3' should be replaced with the current version being installed. The scratch directory should be /nfs/scratch/projects/tmp . Select no where it prompts for creating application launchers. The installation file will check for dependencies in the last stage, missing dependencies will be reported, and will need to be installed for Schr\u00f6dinger Suite to run properly. Contact R\u0101poi admin to install the missing dependencies. Modify the hosts file \u00b6 Change directory to the installation folder cd /home/software/apps/Schrodinger/2023-3 open the schrodinger.hosts file with vi , modify the contents to add hostnames. The hosts and settings can be found in the schrodinger.hosts file from the installation directory of older versions, such as /home/software/apps/Schrodinger/2023-1 . Add all the remote hosts to the new host file. For example, Name: parallel Host: raapoi-login Queue: SLURM2.1 Qargs: \"-p parallel --mem-per-cpu=2G --time=5-00:00 --constraint=AMD\" processors: 1608 tmpdir: /nfs/scratch/projects/tmp Add new module file \u00b6 Once installation is complete, add a new module file so that the new version can be loaded. Module files for existing Schrodinger versions can be found in /home/software/tools/eb_modulefiles/all/Core/Schrodinger . The module files are named with .lua extensions. Make a new module file by copying one of the older module files, for example, cp 2023 -1.lua 2023 -3.lua Then edit the new module file (in this case, 2023-3.lua ) to match the new version installed. Fields that will need to be updated include the Whatis section, and the root . For example: local root = \"/home/software/apps/Schrodinger/2023-3\" You can check if the module has been properly installed by module --ignore_cache avail Installing non-easybuild software for other users \u00b6 Caution: Before you proceed, ask yourself: how likely is it another user will want to use this software, even if they do will they potentially want different/newer version, etc. For software which is unlikely to be used by many users, and/or different users may want to install different versions, then it is probably not worth the hassle to install centrally (i.e. users can install local copies as needed). This is something you should only attempt if you are confident in using/modifying build scripts and associated tools. If you decide it is worth the hassle to proceed, you should first carefully examine the entire installation documentation of the desired software. For any required dependencies, check if modules for these already exist and pre-load them to minimise the amount of stuff you need to compile (otherwise you will need to install any remaining dependencies first, following these same broad steps). Depending on what's available, this may help inform which compiler toolchain you will use during the installation (e.g. typically one of the foss/YEAR<a/b> toolchains). You'll also need to modify the installation steps so that the sofware is ultimately installed into /home/software/apps/<software name>/<version number>/ taking care that no existing software is overwritten. However, before installing software in this directory, it is recommended you install a copy into your home/local directory and test that it works properly first. Lastly, to allow others to load the software you'll need to write a .lua file to be copied into /home/software/tools/modulefiles/<software name>/<version number>/ . The .lua file contains some basic module information, help information, and instructions on what modules need to be loaded and environment variables which need to be set/modified when the software is loaded. You should have a good idea of what is required based on the installation steps, but you may also want to examine (and possibly copy+modify) the .lua file for a similar piece of software. Pruning unattended vscode-server processes \u00b6 If the login node is behaving erratically or unresponse it may be running out of memory due to left over vscode-server processes which don't exit when the user has finished with them. Caution: Killing processes may result in lost work, you may need an Admin to perform the tasks as root user. Use these commands to identify that this is happening: This will output a sorted list of processes by memory usage. Commands named \"node\" or \"pvserver-real\" belonging to users may show large memory usage. top -o VIRT You may also see mysql, slurmctld, and other slum things as well as polkitd and httpd are system related and are expected to use lots of memory to run the cluster. This command will list all the processes by username once you have identified the top memory users ps -ef | grep username To proceed to kill the resource hungry processes, you need to identify the numeric uid (user id) of the user to whom the process belongs. id username This will output something like this: uid=1001(username) gid=100(users) groups=100(users) Make a note of the uid - in this case 1001 To kill all the processes related to that uid run this Note Note the use of sudo so this needs to be done with a moderator or admin account Caution: Don't kill processes belonging to slurm, apache or polkitd - they are needed to run the cluster sudo pkill --uid 1001","title":"Notes for Moderators"},{"location":"mods_admins/#moderating-a-slurm-cluster","text":"","title":"Moderating a Slurm Cluster"},{"location":"mods_admins/#expectations-of-moderators","text":"If you are responding to a user query in the raapoi-help or software-request channels, please leave a message indicating you are looking into the problem, and subsequently post a message when it has been completed. This ensures that other moderators/admins are aware that a request is currently being handled so that they need not double handle it. If you get stuck on a problem, let other moderators/admins know in the mod-chat channel so they can jump in and help. If you are taking action with respect to something not directly related to a request in the raapoi-help or software-request channels, please make sure you make a note/record of this in the mod-chat channel on slack. This might include making unsolicited software updates or limiting a user's access to resources to address a fair use issue in the queue. Posting a clear message indicating what you have done and why helps to ensure transparency with regards to response to various issues on the cluster and, once more, reduce instances of double handling.","title":"Expectations of moderators"},{"location":"mods_admins/#dealing-with-jobs-in-the-queue","text":"This a list of common cluster moderator actions, provided as reference. Users without moderator privileges might find some of this of interest, but you won't be able to perform the actions that affect other users. These commands will require you to be logged in with your moderator-specific account.","title":"Dealing with jobs in the queue"},{"location":"mods_admins/#dealing-with-normal-jobs","text":"","title":"Dealing with normal jobs"},{"location":"mods_admins/#dealing-with-badly-behaved-jobs","text":"","title":"Dealing with badly behaved jobs"},{"location":"mods_admins/#limiting-users-resource-allowance-on-raapoi","text":"Limiting a user's access to resources should generally be a last resort, i.e. after an educative approach has been unsuccessful. Note that placing a limit on resources will not affect jobs which are already running on the cluster (i.e. even if they exceed any newly imposed limits), it will only affect any pending or newly submitted jobs to the queue.","title":"Limiting user's resource allowance on Raapoi"},{"location":"mods_admins/#set-maxjobs","text":"sacctmgr modify user where name = bob set MaxJobs = 2 After a few minutes you should be able to see the results on squeue squeue -u bob -o \"%i %r\" # returns something like JOBID REASON 20582 AssocMaxJobsLimit 20583 Dependency","title":"Set maxjobs"},{"location":"mods_admins/#show-user-cpu-restruction-details","text":"sacctmgr show assoc where user = write_username_here","title":"Show User CPU restruction details"},{"location":"mods_admins/#limiting-cpu-resources","text":"sacctmgr modify user <user> set MaxTRES = cpu = 1026","title":"Limiting CPU resources"},{"location":"mods_admins/#limiting-memory-ram-resources","text":"This is a reference, but note that this may have unintended consequences. Please consult other moderators on Slack before proceeding with this. sacctmgr modify user <user> set MaxTRES = mem = 1000G # This is 1TB of Ram","title":"Limiting Memory (RAM) resources"},{"location":"mods_admins/#limiting-gpu-resources","text":"sacctmgr modify user bob set MaxTRES = cpu = -1,mem = -1,gres/gpu = 4 # -1 means no restriction. # check result sacctmgr list assoc User = bob","title":"Limiting GPU resources"},{"location":"mods_admins/#using-reservations","text":"If a research group has a good need and the other moderators agree, you can give them a reservation that only they can use. This is usually done for a specific time period. This is also one of the steps when we put the cluster into maintenance Create a month-long reservation on amd01n01 and amd01n02 scontrol create reservationname = MyReservation starttime = 2021 -03-01T11:00:00 \\ duration = 30 -00:00:00 user = user1,user2,user3 nodes = amd01n01,amd01n02 Users will use the reservation with #SBATCH --reservation=MyReservation","title":"Using reservations"},{"location":"mods_admins/#building-software-with-easybuild","text":"Caution: EasyBuild v5.0 and above includes a number of backwards-incompatible changes, see easybuild-v5/#breaking-changes . One issue is related to where easybuild looks for config files. As a workaround, first check whether easybuild can find our config files using eb --show-default-configfiles . If the output indicates that no config files are found, you may need to add the argument --configfiles=/etc/easybuild.d/config.cfg when building with easybuild to ensure software is correctly built into the correct directories on the cluster. If in doubt, check with an experienced moderator/admin. (You could also try loading and using a prior easybuild version, such as module load EasyBuild/4.9.4 .) Use a terminal multiplexer like screen, tmux or byobu to keep your ssh session alive and get a interactive session on a node.","title":"Building software with EasyBuild"},{"location":"mods_admins/#build-a-simple-program","text":"Here we will build a simple program called velvet - it's a genome assembler. srun -c 10 --mem = 10G -p parallel --time = 6 :00:00 --pty bash # now on the node module purge # just in case module load EasyBuild # Search for the package eb -S velvet # Returns * $CFGS1 /v/Velvet/Velvet-1.2.10-GCC-8.3.0-mt-kmer_191.eb * $CFGS1 /v/Velvet/Velvet-1.2.10-GCC-11.2.0-mt-kmer_191.eb * $CFGS1 /v/Velvet/Velvet-1.2.10-foss-2018a-mt-kmer_191.eb * $CFGS1 /v/Velvet/Velvet-1.2.10-foss-2018b-mt-kmer_191.eb * $CFGS1 /v/Velvet/Velvet-1.2.10-intel-2017a-mt-kmer_37.eb # We want to pick one that won't need to build a whole new toolchain if we can avoid it # Let's have a look at what would get built with a Dry (D) run. The r is for robot to # find all the dependancies eb -Dr Velvet-1.2.10-intel-2017a-mt-kmer_37.eb # Partial return * [ x ] $CFGS /m/M4/M4-1.4.17.eb ( module: Core | M4/1.4.17 ) * [ x ] $CFGS /b/Bison/Bison-3.0.4.eb ( module: Core | Bison/3.0.4 ) * [ x ] $CFGS /f/flex/flex-2.6.0.eb ( module: Core | flex/2.6.0 ) * [ x ] $CFGS /z/zlib/zlib-1.2.8.eb ( module: Core | zlib/1.2.8 ) * [ ] $CFGS /b/binutils/binutils-2.27.eb ( module: Core | binutils/2.27 ) * [ ] $CFGS /g/GCCcore/GCCcore-6.3.0.eb ( module: Core | GCCcore/6.3.0 ) * [ ] $CFGS /z/zlib/zlib-1.2.11-GCCcore-6.3.0.eb * [ ] $CFGS /h/help2man/help2man-1.47.4-GCCcore-6.3.0.eb * [ ] $CFGS /m/M4/M4-1.4.18-GCCcore-6.3.0.eb * [ ] $CFGS /b/Bison/Bison-3.0.4-GCCcore-6.3.0.eb * [ ] $CFGS /f/flex/flex-2.6.3-GCCcore-6.3.0.eb * [ ] $CFGS /i/icc/icc-2017.1.132-GCC-6.3.0-2.27.eb * [ ] $CFGS /i/ifort/ifort-2017.1.132-GCC-6.3.0-2.27.eb * [ ] $CFGS /i/iccifort/iccifort-2017.1.132-GCC-6.3.0-2.27.eb * [ ] $CFGS /i/impi/impi-2017.1.132-iccifort-2017.1.132-GCC-6.3.0-2.27.eb * [ ] $CFGS /i/iimpi/iimpi-2017a.eb ( module: Core | iimpi/2017a ) * [ ] $CFGS /i/imkl/imkl-2017.1.132-iimpi-2017a.eb * [ ] $CFGS /i/intel/intel-2017a.eb ( module: Core | intel/2017a ) * [ ] $CFGS /v/Velvet/Velvet-1.2.10-intel-2017a-mt-kmer_37.eb # Packages wiht a [x] are already built, [ ] will need to be built. This is a lot of building,, including a \"new\" compiler - intel-2017a.eb, let's avoid that and try another eb -Dr Velvet-1.2.10-foss-2018b-mt-kmer_191.eb # Partially returns, all [x] except for velvet ... * [ x ] $CFGS /f/FFTW/FFTW-3.3.8-gompi-2018b.eb * [ x ] $CFGS /s/ScaLAPACK/ScaLAPACK-2.0.2-gompi-2018b-OpenBLAS-0.3.1.eb * [ x ] $CFGS /f/foss/foss-2018b.eb ( module: Core | foss/2018b ) * [ ] $CFGS /v/Velvet/Velvet-1.2.10-foss-2018b-mt-kmer_191.eb # Before you proceed to build, make sure appropriate folder permissions will be set umask 0002 # To build this we would eb -r --parallel = $SLURM_CPUS_PER_TASK Velvet-1.2.10-foss-2018b-mt-kmer_191.eb # Afterwards, reset the default permissions for anything else you go on to do umask 0022 Remember to close the interactive session when done to stop it consuming resources.","title":"Build a simple program"},{"location":"mods_admins/#building-a-new-toolchain","text":"Below we ask for 10cpus, 10G memory and 6 hours. Really long rebuilds might need more time and or cpu/memory. srun -c 10 --mem = 10G -p parallel --time = 6 :00:00 --pty bash # now on the node module purge # just in case module load EasyBuild # Search for the package eb -S foss-2022a # There is a long output as that toochain gets used in many packages, but we can see: $CFGS1 /f/foss/foss-2022a.eb # Check what will be built # BE CAUTIOUS OF OPENMPI builds - the .eb file needs to be changed to use pmi2 rather than pmix each time! eb -Dr foss-2022a.eb # Trigger the build - this might take a long time, you could add more cpus or time if needed # (making sure permissions will be set appropriately, as described above) umask 0002 eb -r --parallel = $SLURM_CPUS_PER_TASK foss-2022a.eb umask 0022","title":"Building a new toolchain"},{"location":"mods_admins/#rebuilding-an-existing-package","text":"This might be needed for some old packages after the move to Rocky 8 Below we ask for 10cpus, 10G memory and 6 hours. Really long rebuilds might need more time and or cpu/memory. srun -c 10 --mem = 10G -p parallel --time = 6 :00:00 --pty bash # now on the node module purge # just in case module load EasyBuild # Example of finding the problem ldd /home/software/apps/samtools/1.10/bin/samtools | grep found libncursesw.so.5 = > not found libtinfo.so.5 = > not found # See the what got build for samtools eb -Dr /home/software/EasyBuild/ebfiles_repo/SAMtools/SAMtools-1.10-GCC-8.3.0.eb # There are a few options # ncurses-6.0.eb # ncurses-6.1-GCCcore-8.3.0.eb # let's try one (making sure permissions will be set appropriately, as described above) umask 0002 eb -r --parallel = 10 --rebuild ncurses-6.0.eb umask 0022 # test once done","title":"Rebuilding an existing package"},{"location":"mods_admins/#upgrading-easybuild-with-easybuild","text":"Get an interactice session on a node, then module load EasyBuild # will load latest version by default eb --version # see version eb --install-latest-eb-release # upgrade - will create new module file for new version","title":"Upgrading easybuild with Easybuild"},{"location":"mods_admins/#building-new-version-of-schrodinger-suite","text":"Schr\u00f6dinger Suite releases new versions quarterly, it's good practice to keep up to date with the latest version of the software. To build the new version, first download the tar file from the Schr\u00f6dinger website (www.schrodinger.com), then move the installation tar file to the directory /home/software/src/Schrodinger on R\u0101poi.","title":"Building New Version of Schrodinger Suite"},{"location":"mods_admins/#quick-installation","text":"First, extract the tar file tar -xvf Schrodinger_Download.tar Change to the top-level directory of the download cd Schrodinger_Download Then, run the instalaation script sh ./INSTALL Answer y/n to prompts from the INSTALL script, then all packages should be installed. Note During installation, you will be asked to confirm the installation directory , this is /home/software/apps/Schrodinger/2023-3 , '2023-3' should be replaced with the current version being installed. The scratch directory should be /nfs/scratch/projects/tmp . Select no where it prompts for creating application launchers. The installation file will check for dependencies in the last stage, missing dependencies will be reported, and will need to be installed for Schr\u00f6dinger Suite to run properly. Contact R\u0101poi admin to install the missing dependencies.","title":"Quick Installation"},{"location":"mods_admins/#modify-the-hosts-file","text":"Change directory to the installation folder cd /home/software/apps/Schrodinger/2023-3 open the schrodinger.hosts file with vi , modify the contents to add hostnames. The hosts and settings can be found in the schrodinger.hosts file from the installation directory of older versions, such as /home/software/apps/Schrodinger/2023-1 . Add all the remote hosts to the new host file. For example, Name: parallel Host: raapoi-login Queue: SLURM2.1 Qargs: \"-p parallel --mem-per-cpu=2G --time=5-00:00 --constraint=AMD\" processors: 1608 tmpdir: /nfs/scratch/projects/tmp","title":"Modify the hosts file"},{"location":"mods_admins/#add-new-module-file","text":"Once installation is complete, add a new module file so that the new version can be loaded. Module files for existing Schrodinger versions can be found in /home/software/tools/eb_modulefiles/all/Core/Schrodinger . The module files are named with .lua extensions. Make a new module file by copying one of the older module files, for example, cp 2023 -1.lua 2023 -3.lua Then edit the new module file (in this case, 2023-3.lua ) to match the new version installed. Fields that will need to be updated include the Whatis section, and the root . For example: local root = \"/home/software/apps/Schrodinger/2023-3\" You can check if the module has been properly installed by module --ignore_cache avail","title":"Add new module file"},{"location":"mods_admins/#installing-non-easybuild-software-for-other-users","text":"Caution: Before you proceed, ask yourself: how likely is it another user will want to use this software, even if they do will they potentially want different/newer version, etc. For software which is unlikely to be used by many users, and/or different users may want to install different versions, then it is probably not worth the hassle to install centrally (i.e. users can install local copies as needed). This is something you should only attempt if you are confident in using/modifying build scripts and associated tools. If you decide it is worth the hassle to proceed, you should first carefully examine the entire installation documentation of the desired software. For any required dependencies, check if modules for these already exist and pre-load them to minimise the amount of stuff you need to compile (otherwise you will need to install any remaining dependencies first, following these same broad steps). Depending on what's available, this may help inform which compiler toolchain you will use during the installation (e.g. typically one of the foss/YEAR<a/b> toolchains). You'll also need to modify the installation steps so that the sofware is ultimately installed into /home/software/apps/<software name>/<version number>/ taking care that no existing software is overwritten. However, before installing software in this directory, it is recommended you install a copy into your home/local directory and test that it works properly first. Lastly, to allow others to load the software you'll need to write a .lua file to be copied into /home/software/tools/modulefiles/<software name>/<version number>/ . The .lua file contains some basic module information, help information, and instructions on what modules need to be loaded and environment variables which need to be set/modified when the software is loaded. You should have a good idea of what is required based on the installation steps, but you may also want to examine (and possibly copy+modify) the .lua file for a similar piece of software.","title":"Installing non-easybuild software for other users"},{"location":"mods_admins/#pruning-unattended-vscode-server-processes","text":"If the login node is behaving erratically or unresponse it may be running out of memory due to left over vscode-server processes which don't exit when the user has finished with them. Caution: Killing processes may result in lost work, you may need an Admin to perform the tasks as root user. Use these commands to identify that this is happening: This will output a sorted list of processes by memory usage. Commands named \"node\" or \"pvserver-real\" belonging to users may show large memory usage. top -o VIRT You may also see mysql, slurmctld, and other slum things as well as polkitd and httpd are system related and are expected to use lots of memory to run the cluster. This command will list all the processes by username once you have identified the top memory users ps -ef | grep username To proceed to kill the resource hungry processes, you need to identify the numeric uid (user id) of the user to whom the process belongs. id username This will output something like this: uid=1001(username) gid=100(users) groups=100(users) Make a note of the uid - in this case 1001 To kill all the processes related to that uid run this Note Note the use of sudo so this needs to be done with a moderator or admin account Caution: Don't kill processes belonging to slurm, apache or polkitd - they are needed to run the cluster sudo pkill --uid 1001","title":"Pruning unattended vscode-server processes"},{"location":"new_mod/","text":"New Module System \u00b6 In 2020 we started building packages and organising them into modules 1 in a new way. In the new system modules are organised into toolchains 2 . These toolchains were used to build the software. For most users the most important thing is these these toolchains act like software \"silos\". In general this restricts you to using using programs in one toolchain \"silo\". This is on the face of it, is annoying. However, it resolves some very hard to diagnose and subtle bugs that occur when you load programs that are built with different compilers - in the old system this was not transparent to you. Before you module load a toolchain the software contained within will not be visible to module loading (except via module spider). You first need to load the compiler and MPI version the software was built with. For example, if you wanted to load BioPython/1.79 you would first need to load GCC/10.3.0 and OpenMPI/4.1.1 To save you needing to load both a Compiler and MPI version, the compiler and MPI versions are bundled into half yearly packs. For example GCC/10.3.0 and OpenMPI/4.1.1 are bundled in the meta module foss/2021a graph TD; LMOD[\"Module System\"] --toolchain --- foss2020b[\"foss2020b GCC/10.2.0 OpenMPI/4.0.5\"] LMOD --toolchain --- foss2021a[\"foss2021a GCC/10.3.0 OpenMPI/4.1.1\"] foss2020b --- id3[\"ORCA/4.2.1\"] foss2020b --- id4[\"Singularity/3.7.3\"] foss2021a --- id5[\"Biopython/1.79\"] foss2021a --- id6[\"Haploflow/1.0\"] Example loading BioPython/1.7.9 # loading the modules module load foss/2021a # Needed for biopython to be loadable module load BioPython/1.7.9 Searching for Software with Spider \u00b6 As the software is siloed into toolchains methods for finding software like module avail are less useful than in the old system - it will only show software loadable in the current toolchain silo, or if no toolchain silos are loaded it'll show you all the toolchains as well as software that's not tied to a toolchain. We can use module spider to search for software modules more broadly - it will also show what toolchain needs to be loaded first. If we wanted to load a version of netCDF, we could search for it with spider. Note spider searches in a case sensitive manner - but it will suggest other capitalisation if other search options exist. module spider netCDF This returns a lot of information, but the important bit is: ------------- netCDF: ------------- Description: NetCDF ( network Common Data Form ) is a set of software libraries and machine-independent data formats that support the creation, access, and sharing of array-oriented scientific data. Versions: netCDF/4.7.1 netCDF/4.7.4 netCDF/4.8.0 -------------- For detailed information about a specific \"netCDF\" module ( including how to load the modules ) use the modules full name. For example: $ module spider netCDF/4.7.1 -------------- We have 3 versions of netCDF available in the new module system, 4.7.1 , 4.7.4 and 4.8.0 . To see which toolchain needs to be loaded, we spider that particular version module spider netCDF/4.7.1 #returns ---------- netCDF: netCDF/4.7.1 ---------- Description: NetCDF ( network Common Data Form ) is a set of software libraries and machine-independent data formats that support the creation, access, and sharing of array-oriented scientific data. You will need to load all module ( s ) on any one of the lines below before the \"netCDF/4.7.1\" module is available to load. GCC/8.3.0 OpenMPI/3.1.4 Help: Description =========== NetCDF ( network Common Data Form ) is a set of software libraries and machine-independent data formats that support the creation, access, and sharing of array-oriented scientific data. More information ================ - Homepage: https://www.unidata.ucar.edu/software/netcdf/ This gives some information about the software as well as what we need to load it, in this case GCC/8.3.0 and OpenMPI/3.1.4 . We can just use that and load netcdf - #Load prerequisites - the \"silo\" module load GCC/8.3.0 module load OpenMPI/3.1.4 #Load NetCDF module load netCDF Alternatively you could load the toolchain containing gcc/8.3.0 and OpenMPI/3.1.4 - foss/2019b #Load prerequisites - the \"silo\" module load foss/2019b #Load NetCDF module load netCDF Toolchain \"silo\" table \u00b6 Toolchains currently on R\u0101poi as of April 2024. Toolchain Compiler MPI foss/2018b GCC/7.3.0 OpenMPI/3.1.1 foss/2019b GCC/8.3.0 OpenMPI/3.1.4 foss/2020a GCC/9.3.0 OpenMPI/4.0.3 foss/2020b GCC/10.2.0 OpenMPI/4.0.5 foss/2021a GCC/10.3.0 OpenMPI/4.1.1 foss/2021b GCC/11.2.0 OpenMPI/4.1.1 foss/2022a GCC/11.3.0 OpenMPI/4.1.4 foss/2022b GCC/12.2.0 OpenMPI/4.1.4 foss/2023a GCC/12.3.0 OpenMPI/4.1.5 foss/2023b GCC/13.2.0 OpenMPI/4.1.6 There are also toolchain versions with CUDA for use on the GPU nodes - they contain the same compiler and OpenMPI but also include a CUDA version Toolchain Compiler MPI CUDA fosscuda/2019b GCC/8.3.0 OpenMPI/3.1.4 CUDA/10.1.243 fosscuda/2020b GCC/10.2.0 OpenMPI/4.0.5 CUDA/11.1.1 Lastly we have some intel compiler toolchains built. This might work on the AMD nodes, but you'll have an easier time with the intel nodes. Toolchain Compiler Intel Compiler MPI MKL intel/2021b GCC/11.2.0 2021.4.0 impi/2021.4.0 imkl/2021.4.0 intel/2022a GCC/11.3.0 2022.1.0 impi/2021.6.0 imkl/2022.1.0 intel/2022.00 GCC/11.2.0 2022.0.1 impi/2021.5.0 imkl/2022.0.1 intel/2022.05 GCC/11.3.0 2022.1.0 impi/2021.6.0 imkl/2022.1.0 You can also just experimentally module load the various toolchains and list to see what the module loads to see what it contains. #load toolchain module load foss/2020b # List what it loads module list # Returns Currently Loaded Modules: 1 ) config 6 ) libfabric/1.11.0 11 ) libxml2/2.9.10 16 ) FFTW/3.3.8 2 ) GCCcore/10.2.0 7 ) libevent/2.1.12 12 ) libpciaccess/0.16 17 ) OpenBLAS/0.3.12 3 ) binutils/2.35 8 ) numactl/2.0.13 13 ) hwloc/2.2.0 18 ) ScaLAPACK/2.1.0 4 ) <b>GCC/10.2.0</b> 9 ) XZ/5.2.5 14 ) PMIx/3.1.5 19 ) foss/2020b 5 ) UCX/1.9.0 10 ) zlib/1.2.11 15 ) <b>OpenMPI/4.0.5</b> Add new module system for accounts prior to March 2022 \u00b6 Users accounts setup prior to March 2022 will not automatically have the new module system loaded. You can automatically use this new module system (in parallel with the old one) by adding a line to your .bashrc file. Users of zsh can make a similar change to their .zshrc file. Login to R\u0101poi and backup your .bashrc file, then edit it to add the needed line in. cd ~ # change to your home directory cp .bashrc .bashrc_backup #create backup of your bashrc file nano .bashrc #open the nano editor to edit your file #in nano find the line # module use -a /home/software/tools/modulefiles # it will be near the end of the file. After that line, add the line: module use /home/software/tools/eb_modulefiles/all/Core #press control-x to exit nano. It will ask if you want to save the modified buffer. Type Y to save the change. # It'll ask for the filename, just press enter to accept the name # .bashrc After you have made that change logout and log back in to have the new module system loaded. You can test it's working by loading a toolchain. module load foss/2020a If you run into problems, copy your .bashrc backup back to the original and try again with cp .bashrc_backup .bashrc Also feel free to ask for help on slack Modules are the way programs are packaged up for you to use. We can't just install them system wide as we have hundreds of programs installed in modules, often with many versions of the same thing, they would conflict with each other. Modules let you load just what you need. See Preparing your environment \u21a9 The toolchain is comprised of a compiler and a version of MPI that was used to build the software. For instance the toolchain foss/2021a uses GCC/10.3.0 and OpenMPI/4.1.1 \u21a9","title":"New module system"},{"location":"new_mod/#new-module-system","text":"In 2020 we started building packages and organising them into modules 1 in a new way. In the new system modules are organised into toolchains 2 . These toolchains were used to build the software. For most users the most important thing is these these toolchains act like software \"silos\". In general this restricts you to using using programs in one toolchain \"silo\". This is on the face of it, is annoying. However, it resolves some very hard to diagnose and subtle bugs that occur when you load programs that are built with different compilers - in the old system this was not transparent to you. Before you module load a toolchain the software contained within will not be visible to module loading (except via module spider). You first need to load the compiler and MPI version the software was built with. For example, if you wanted to load BioPython/1.79 you would first need to load GCC/10.3.0 and OpenMPI/4.1.1 To save you needing to load both a Compiler and MPI version, the compiler and MPI versions are bundled into half yearly packs. For example GCC/10.3.0 and OpenMPI/4.1.1 are bundled in the meta module foss/2021a graph TD; LMOD[\"Module System\"] --toolchain --- foss2020b[\"foss2020b GCC/10.2.0 OpenMPI/4.0.5\"] LMOD --toolchain --- foss2021a[\"foss2021a GCC/10.3.0 OpenMPI/4.1.1\"] foss2020b --- id3[\"ORCA/4.2.1\"] foss2020b --- id4[\"Singularity/3.7.3\"] foss2021a --- id5[\"Biopython/1.79\"] foss2021a --- id6[\"Haploflow/1.0\"] Example loading BioPython/1.7.9 # loading the modules module load foss/2021a # Needed for biopython to be loadable module load BioPython/1.7.9","title":"New Module System"},{"location":"new_mod/#searching-for-software-with-spider","text":"As the software is siloed into toolchains methods for finding software like module avail are less useful than in the old system - it will only show software loadable in the current toolchain silo, or if no toolchain silos are loaded it'll show you all the toolchains as well as software that's not tied to a toolchain. We can use module spider to search for software modules more broadly - it will also show what toolchain needs to be loaded first. If we wanted to load a version of netCDF, we could search for it with spider. Note spider searches in a case sensitive manner - but it will suggest other capitalisation if other search options exist. module spider netCDF This returns a lot of information, but the important bit is: ------------- netCDF: ------------- Description: NetCDF ( network Common Data Form ) is a set of software libraries and machine-independent data formats that support the creation, access, and sharing of array-oriented scientific data. Versions: netCDF/4.7.1 netCDF/4.7.4 netCDF/4.8.0 -------------- For detailed information about a specific \"netCDF\" module ( including how to load the modules ) use the modules full name. For example: $ module spider netCDF/4.7.1 -------------- We have 3 versions of netCDF available in the new module system, 4.7.1 , 4.7.4 and 4.8.0 . To see which toolchain needs to be loaded, we spider that particular version module spider netCDF/4.7.1 #returns ---------- netCDF: netCDF/4.7.1 ---------- Description: NetCDF ( network Common Data Form ) is a set of software libraries and machine-independent data formats that support the creation, access, and sharing of array-oriented scientific data. You will need to load all module ( s ) on any one of the lines below before the \"netCDF/4.7.1\" module is available to load. GCC/8.3.0 OpenMPI/3.1.4 Help: Description =========== NetCDF ( network Common Data Form ) is a set of software libraries and machine-independent data formats that support the creation, access, and sharing of array-oriented scientific data. More information ================ - Homepage: https://www.unidata.ucar.edu/software/netcdf/ This gives some information about the software as well as what we need to load it, in this case GCC/8.3.0 and OpenMPI/3.1.4 . We can just use that and load netcdf - #Load prerequisites - the \"silo\" module load GCC/8.3.0 module load OpenMPI/3.1.4 #Load NetCDF module load netCDF Alternatively you could load the toolchain containing gcc/8.3.0 and OpenMPI/3.1.4 - foss/2019b #Load prerequisites - the \"silo\" module load foss/2019b #Load NetCDF module load netCDF","title":"Searching for Software with Spider"},{"location":"new_mod/#toolchain-silo-table","text":"Toolchains currently on R\u0101poi as of April 2024. Toolchain Compiler MPI foss/2018b GCC/7.3.0 OpenMPI/3.1.1 foss/2019b GCC/8.3.0 OpenMPI/3.1.4 foss/2020a GCC/9.3.0 OpenMPI/4.0.3 foss/2020b GCC/10.2.0 OpenMPI/4.0.5 foss/2021a GCC/10.3.0 OpenMPI/4.1.1 foss/2021b GCC/11.2.0 OpenMPI/4.1.1 foss/2022a GCC/11.3.0 OpenMPI/4.1.4 foss/2022b GCC/12.2.0 OpenMPI/4.1.4 foss/2023a GCC/12.3.0 OpenMPI/4.1.5 foss/2023b GCC/13.2.0 OpenMPI/4.1.6 There are also toolchain versions with CUDA for use on the GPU nodes - they contain the same compiler and OpenMPI but also include a CUDA version Toolchain Compiler MPI CUDA fosscuda/2019b GCC/8.3.0 OpenMPI/3.1.4 CUDA/10.1.243 fosscuda/2020b GCC/10.2.0 OpenMPI/4.0.5 CUDA/11.1.1 Lastly we have some intel compiler toolchains built. This might work on the AMD nodes, but you'll have an easier time with the intel nodes. Toolchain Compiler Intel Compiler MPI MKL intel/2021b GCC/11.2.0 2021.4.0 impi/2021.4.0 imkl/2021.4.0 intel/2022a GCC/11.3.0 2022.1.0 impi/2021.6.0 imkl/2022.1.0 intel/2022.00 GCC/11.2.0 2022.0.1 impi/2021.5.0 imkl/2022.0.1 intel/2022.05 GCC/11.3.0 2022.1.0 impi/2021.6.0 imkl/2022.1.0 You can also just experimentally module load the various toolchains and list to see what the module loads to see what it contains. #load toolchain module load foss/2020b # List what it loads module list # Returns Currently Loaded Modules: 1 ) config 6 ) libfabric/1.11.0 11 ) libxml2/2.9.10 16 ) FFTW/3.3.8 2 ) GCCcore/10.2.0 7 ) libevent/2.1.12 12 ) libpciaccess/0.16 17 ) OpenBLAS/0.3.12 3 ) binutils/2.35 8 ) numactl/2.0.13 13 ) hwloc/2.2.0 18 ) ScaLAPACK/2.1.0 4 ) <b>GCC/10.2.0</b> 9 ) XZ/5.2.5 14 ) PMIx/3.1.5 19 ) foss/2020b 5 ) UCX/1.9.0 10 ) zlib/1.2.11 15 ) <b>OpenMPI/4.0.5</b>","title":"Toolchain \"silo\" table"},{"location":"new_mod/#add-new-module-system-for-accounts-prior-to-march-2022","text":"Users accounts setup prior to March 2022 will not automatically have the new module system loaded. You can automatically use this new module system (in parallel with the old one) by adding a line to your .bashrc file. Users of zsh can make a similar change to their .zshrc file. Login to R\u0101poi and backup your .bashrc file, then edit it to add the needed line in. cd ~ # change to your home directory cp .bashrc .bashrc_backup #create backup of your bashrc file nano .bashrc #open the nano editor to edit your file #in nano find the line # module use -a /home/software/tools/modulefiles # it will be near the end of the file. After that line, add the line: module use /home/software/tools/eb_modulefiles/all/Core #press control-x to exit nano. It will ask if you want to save the modified buffer. Type Y to save the change. # It'll ask for the filename, just press enter to accept the name # .bashrc After you have made that change logout and log back in to have the new module system loaded. You can test it's working by loading a toolchain. module load foss/2020a If you run into problems, copy your .bashrc backup back to the original and try again with cp .bashrc_backup .bashrc Also feel free to ask for help on slack Modules are the way programs are packaged up for you to use. We can't just install them system wide as we have hundreds of programs installed in modules, often with many versions of the same thing, they would conflict with each other. Modules let you load just what you need. See Preparing your environment \u21a9 The toolchain is comprised of a compiler and a version of MPI that was used to build the software. For instance the toolchain foss/2021a uses GCC/10.3.0 and OpenMPI/4.1.1 \u21a9","title":"Add new module system for accounts prior to March 2022"},{"location":"parallel_processing/","text":"Parallel processing \u00b6 Running a job in parallel is a great way to utilize the power of the cluster. So what is a parallel job/workflow? Loosely-coupled jobs (sometimes referred to as embarrassingly or naively parallel jobs) are processes in which multiple instances of the same program execute on multiple data files simultaneously, with each instance running independently from others on its own allocated resources (i.e. CPUs and memory). Slurm job arrays offer a simple mechanism for achieving this. Multi-threaded programs that include explicit support for shared memory processing via multiple threads of execution (e.g. Posix Threads or OpenMP) running across multiple CPU cores. Distributed memory programs that include explicit support for message passing between processes (e.g. MPI). These processes execute across multiple CPU cores and/or nodes, these are often referred to as tightly-coupled jobs. GPU (graphics processing unit) programs including explicit support for offloading to the device via languages like CUDA or OpenCL. It is important to understand the capabilities and limitations of an application in order to fully leverage the parallel processing options available on the cluster. For instance, many popular scientific computing languages like Python, R, and Matlab now offer packages that allow for GPU, multi-core or multithreaded processing, especially for matrix and vector operations. If you need help with the design of your parallel workflow, send us a message in the raapoi-help Slack channel. Job Array Example \u00b6 Here is an example of running a job array to run 50 simultaneous processes: sbatch array.sh The contents of the array.sh batch script looks like this: #!/bin/bash #SBATCH -a 1-50 #SBATCH --cpus-per-task=1 #SBATCH --mem-per-cpu=2G #SBATCH --time=00:10:00 #SBATCH --partition=parallel #SBATCH --mail-type=BEGIN,END,FAIL #SBATCH --mail-user=me@email.com module load fastqc/0.11.7 fastqc --nano -o $TMPDIR/output_dir seqfile_${SLURM_ARRAY_TASK_ID} So what do these parameter mean?: -a sets this up as a parallel array job (this sets up the \"loop\" that will be run --cpus-per-task requests the number of CPUs per array task, in this case I just want one CPU per task, we will use 50 in total --mem-per-cpu request 2GB of RAM per CPU, for this parallel job I will request a total of 100GB RAM (50 CPUs * 2GB RAM) --time is the max run time for this job, 10 minutes in this case --partition assigns this job to a partition module load fastqc/0.11.7 : Load software into my environment, in this case fastqc fastqc --nano -o $TMPDIR/output_dir seqfile ${SLURM_ARRAY_TASK_ID}_ Run fastqc on each input data file with the filenames seqfile_1, seqfile_2...seqfile_50 Running the array.sh script will cause the SLURM manager to spawn 50 parallel jobs. Multi-threaded or Multi-processing Job Example \u00b6 Multi-threaded or multi-processing programs are applications that are able to execute in parallel across multiple CPU cores within a single node using a shared memory execution model. In general, a multi-threaded application uses a single process (aka \u201ctask\u201d in Slurm) which then spawns multiple threads of execution. By default, Slurm allocates 1 CPU core per task. In order to make use of multiple CPU cores in a multi-threaded program, one must include the --cpus-per-task option. Below is an example of a multi-threaded program requesting 12 CPU cores per task and a total of 256GB of memory. The program itself is responsible for spawning the appropriate number of threads. #!/bin/bash #SBATCH --nodes=1 #SBATCH --ntasks=1 #SBATCH --cpus-per-task=12 # 12 threads per task #SBATCH --time=02:00:00 # two hours #SBATCH --mem=256G #SBATCH -p bigmem #SBATCH --output=threaded.out #SBATCH --job-name=threaded #SBATCH --mail-type=BEGIN,END,FAIL #SBATCH --mail-user=me@email.com # Run multi-threaded application module load java/1.8.0-91 java -jar threaded-app.jar MPI Jobs \u00b6 Most users do not require MPI to run their jobs but many do. Please read on if you want to learn more about using MPI for tightly-coupled jobs. See also the OpenMPI Users Guide MPI (Message Passing Interface) code require special attention within Slurm. Slurm allocates and launches MPI jobs differently depending on the version of MPI used (e.g. OpenMPI, MPICH2). We recommend using OpenMPI version 2.1.1 or later to compile your C code (using mpicc) and then using the mpirun command in a batch submit script to launch parallel MPI jobs. The example below runs MPI code compiled by OpenMPI 2.1.1: #!/bin/bash #SBATCH --nodes=3 #SBATCH --tasks-per-node=8 # 8 MPI processes per node #SBATCH --time=3-00:00:00 #SBATCH --mem=4G # 4 GB RAM per node #SBATCH --output=mpi_job.log #SBATCH --partition=parallel #SBATCH --constraint=\"IB\" #SBATCH --mail-type=BEGIN,END,FAIL #SBATCH --mail-user=me@email.com module load openmpi echo $SLURM_JOB_NODELIST mpirun -np 24 mpiscript.o This example requests 3 nodes and 8 tasks (i.e. processes) per node, for a total of 24 tasks. I use this number to tell mpirun how many processes to start, -np 24 NOTE: We highly recomend adding the --constraint=\"IB\" parameter to your MPI job as this will ensure the job is run on nodes with an Infiniband interconnect. ALSO NOTE: If using python or another language you will also need to add the --oversubscribe parameter to mpirun, eg. mpirun --oversubscribe -np 24 mpiscript.py More information about running MPI jobs within Slurm can be found here: http://slurm.schedmd.com/mpi_guide.html . OpenMPI users guide \u00b6 Which versions of OpenMPI are working on R\u0101poi? \u00b6 There are a number of versions of OpenMPI on R\u0101poi, although many of these are old installations (prior to an OS update and changes to the module system) and may no longer work. Generally speaking, your best bet is to try a version which appears when you search via module spider OpenMPI (noting that the capital 'O M P I' is important here). A few examples of relatively recent version of OpenMPI which are available (as of April 2024) are OpenMPI/4.1.1 , OpenMPI/4.1.4 and OpenMPI/4.1.6 . Each of these OpenMPI modules has one or more of pre-requisite modules that need to be loaded first (generally a specific version of GCC compilers). To find out what you need to load first for a specific version of OpenMPI you just need to check the output of module spider OpenMPI/x.y.z (with the appropriate values for x,y,z). One of the examples below shows how to use OpenMPI/4.1.6 . In cases where your code utilises software from another module which also requires a specific GCC module, that will dictate which version of OpenMPI to load (i.e. whichever one depends on the same GCC version). Otherwise, you are free to use any desired OpenMPI module. Known issues and workarounds \u00b6 There is a known issue with the communication/networking interfaces with several of the installations of OpenMPI. The error/warning messages occur sporadically, making it difficult to pin down and resolve, but it is likely there is a combination of internal and external factors that cause this (OpenMPI is a very complex beast). The warning messages take the form: Failed to modify UD QP to INIT on mlx5_0: Operation not permitted A workaround is described below, this page will be updated in the future when a more permanent solution is found. Exectute your mpi jobs using the additional arguments: mpirun -mca pml ucx -mca btl '^uct,ofi' -mca mtl '^ofi' -np $SLURM_NTASKS <your executable> This will ensure OpenMPI avoids trying to use the communication libraries which are problematic. If your executable is launched without using mpirun (i.e. it implements its own wrapper/launcher), you will instead need to set the following environment variables: export OMPI_MCA_btl = '^uct,ofi' export OMPI_MCA_pml = 'ucx' export OMPI_MCA_mtl = '^ofi' Software module ORCA users, sometimes, come across an error message: PML ucx cannot be selected To address this, update mpirun variables to: mpirun --mca btl ^openib --mca pml ob1 --mca osc ucx -np ...<remaining params> `","title":"Parallel Processing"},{"location":"parallel_processing/#parallel-processing","text":"Running a job in parallel is a great way to utilize the power of the cluster. So what is a parallel job/workflow? Loosely-coupled jobs (sometimes referred to as embarrassingly or naively parallel jobs) are processes in which multiple instances of the same program execute on multiple data files simultaneously, with each instance running independently from others on its own allocated resources (i.e. CPUs and memory). Slurm job arrays offer a simple mechanism for achieving this. Multi-threaded programs that include explicit support for shared memory processing via multiple threads of execution (e.g. Posix Threads or OpenMP) running across multiple CPU cores. Distributed memory programs that include explicit support for message passing between processes (e.g. MPI). These processes execute across multiple CPU cores and/or nodes, these are often referred to as tightly-coupled jobs. GPU (graphics processing unit) programs including explicit support for offloading to the device via languages like CUDA or OpenCL. It is important to understand the capabilities and limitations of an application in order to fully leverage the parallel processing options available on the cluster. For instance, many popular scientific computing languages like Python, R, and Matlab now offer packages that allow for GPU, multi-core or multithreaded processing, especially for matrix and vector operations. If you need help with the design of your parallel workflow, send us a message in the raapoi-help Slack channel.","title":"Parallel processing"},{"location":"parallel_processing/#job-array-example","text":"Here is an example of running a job array to run 50 simultaneous processes: sbatch array.sh The contents of the array.sh batch script looks like this: #!/bin/bash #SBATCH -a 1-50 #SBATCH --cpus-per-task=1 #SBATCH --mem-per-cpu=2G #SBATCH --time=00:10:00 #SBATCH --partition=parallel #SBATCH --mail-type=BEGIN,END,FAIL #SBATCH --mail-user=me@email.com module load fastqc/0.11.7 fastqc --nano -o $TMPDIR/output_dir seqfile_${SLURM_ARRAY_TASK_ID} So what do these parameter mean?: -a sets this up as a parallel array job (this sets up the \"loop\" that will be run --cpus-per-task requests the number of CPUs per array task, in this case I just want one CPU per task, we will use 50 in total --mem-per-cpu request 2GB of RAM per CPU, for this parallel job I will request a total of 100GB RAM (50 CPUs * 2GB RAM) --time is the max run time for this job, 10 minutes in this case --partition assigns this job to a partition module load fastqc/0.11.7 : Load software into my environment, in this case fastqc fastqc --nano -o $TMPDIR/output_dir seqfile ${SLURM_ARRAY_TASK_ID}_ Run fastqc on each input data file with the filenames seqfile_1, seqfile_2...seqfile_50 Running the array.sh script will cause the SLURM manager to spawn 50 parallel jobs.","title":"Job Array Example"},{"location":"parallel_processing/#multi-threaded-or-multi-processing-job-example","text":"Multi-threaded or multi-processing programs are applications that are able to execute in parallel across multiple CPU cores within a single node using a shared memory execution model. In general, a multi-threaded application uses a single process (aka \u201ctask\u201d in Slurm) which then spawns multiple threads of execution. By default, Slurm allocates 1 CPU core per task. In order to make use of multiple CPU cores in a multi-threaded program, one must include the --cpus-per-task option. Below is an example of a multi-threaded program requesting 12 CPU cores per task and a total of 256GB of memory. The program itself is responsible for spawning the appropriate number of threads. #!/bin/bash #SBATCH --nodes=1 #SBATCH --ntasks=1 #SBATCH --cpus-per-task=12 # 12 threads per task #SBATCH --time=02:00:00 # two hours #SBATCH --mem=256G #SBATCH -p bigmem #SBATCH --output=threaded.out #SBATCH --job-name=threaded #SBATCH --mail-type=BEGIN,END,FAIL #SBATCH --mail-user=me@email.com # Run multi-threaded application module load java/1.8.0-91 java -jar threaded-app.jar","title":"Multi-threaded or Multi-processing Job Example"},{"location":"parallel_processing/#mpi-jobs","text":"Most users do not require MPI to run their jobs but many do. Please read on if you want to learn more about using MPI for tightly-coupled jobs. See also the OpenMPI Users Guide MPI (Message Passing Interface) code require special attention within Slurm. Slurm allocates and launches MPI jobs differently depending on the version of MPI used (e.g. OpenMPI, MPICH2). We recommend using OpenMPI version 2.1.1 or later to compile your C code (using mpicc) and then using the mpirun command in a batch submit script to launch parallel MPI jobs. The example below runs MPI code compiled by OpenMPI 2.1.1: #!/bin/bash #SBATCH --nodes=3 #SBATCH --tasks-per-node=8 # 8 MPI processes per node #SBATCH --time=3-00:00:00 #SBATCH --mem=4G # 4 GB RAM per node #SBATCH --output=mpi_job.log #SBATCH --partition=parallel #SBATCH --constraint=\"IB\" #SBATCH --mail-type=BEGIN,END,FAIL #SBATCH --mail-user=me@email.com module load openmpi echo $SLURM_JOB_NODELIST mpirun -np 24 mpiscript.o This example requests 3 nodes and 8 tasks (i.e. processes) per node, for a total of 24 tasks. I use this number to tell mpirun how many processes to start, -np 24 NOTE: We highly recomend adding the --constraint=\"IB\" parameter to your MPI job as this will ensure the job is run on nodes with an Infiniband interconnect. ALSO NOTE: If using python or another language you will also need to add the --oversubscribe parameter to mpirun, eg. mpirun --oversubscribe -np 24 mpiscript.py More information about running MPI jobs within Slurm can be found here: http://slurm.schedmd.com/mpi_guide.html .","title":"MPI Jobs"},{"location":"parallel_processing/#openmpi-users-guide","text":"","title":"OpenMPI users guide"},{"location":"parallel_processing/#which-versions-of-openmpi-are-working-on-rapoi","text":"There are a number of versions of OpenMPI on R\u0101poi, although many of these are old installations (prior to an OS update and changes to the module system) and may no longer work. Generally speaking, your best bet is to try a version which appears when you search via module spider OpenMPI (noting that the capital 'O M P I' is important here). A few examples of relatively recent version of OpenMPI which are available (as of April 2024) are OpenMPI/4.1.1 , OpenMPI/4.1.4 and OpenMPI/4.1.6 . Each of these OpenMPI modules has one or more of pre-requisite modules that need to be loaded first (generally a specific version of GCC compilers). To find out what you need to load first for a specific version of OpenMPI you just need to check the output of module spider OpenMPI/x.y.z (with the appropriate values for x,y,z). One of the examples below shows how to use OpenMPI/4.1.6 . In cases where your code utilises software from another module which also requires a specific GCC module, that will dictate which version of OpenMPI to load (i.e. whichever one depends on the same GCC version). Otherwise, you are free to use any desired OpenMPI module.","title":"Which versions of OpenMPI are working on R\u0101poi?"},{"location":"parallel_processing/#known-issues-and-workarounds","text":"There is a known issue with the communication/networking interfaces with several of the installations of OpenMPI. The error/warning messages occur sporadically, making it difficult to pin down and resolve, but it is likely there is a combination of internal and external factors that cause this (OpenMPI is a very complex beast). The warning messages take the form: Failed to modify UD QP to INIT on mlx5_0: Operation not permitted A workaround is described below, this page will be updated in the future when a more permanent solution is found. Exectute your mpi jobs using the additional arguments: mpirun -mca pml ucx -mca btl '^uct,ofi' -mca mtl '^ofi' -np $SLURM_NTASKS <your executable> This will ensure OpenMPI avoids trying to use the communication libraries which are problematic. If your executable is launched without using mpirun (i.e. it implements its own wrapper/launcher), you will instead need to set the following environment variables: export OMPI_MCA_btl = '^uct,ofi' export OMPI_MCA_pml = 'ucx' export OMPI_MCA_mtl = '^ofi' Software module ORCA users, sometimes, come across an error message: PML ucx cannot be selected To address this, update mpirun variables to: mpirun --mca btl ^openib --mca pml ob1 --mca osc ucx -np ...<remaining params> `","title":"Known issues and workarounds"},{"location":"partitions/","text":"Partitions \u00b6 Using Partitions \u00b6 A partition is a collection of compute nodes, think of it as a sub-cluster or slice of the larger cluster. Each partition has its own rules and configurations. For example, the quicktest partition has a maximum job run-time of 5 hours, whereas the partition bigmem has a maximum runtime of 10 days. Partitions can also limit who can run a job. Currently any user can use any partition but there may come a time when certain research groups purchase their own nodes and they are given exclusive access. To view the partitions available to use you can type the vuw-partitions command, eg <user>@raapoi-login:~$ vuw-partitions VUW CLUSTER PARTITIONS PARTITION AVAIL TIMELIMIT NODES STATE NODELIST quicktest* up 5:00:00 1 down* amd01n01 quicktest* up 5:00:00 4 mix amd01n[02-03] quicktest* up 5:00:00 1 idle amd01n04 PARTITION AVAIL TIMELIMIT NODES STATE NODELIST parallel up 10-00:00:0 1 resv spj01 parallel up 10-00:00:0 24 mix amd02n[01-04],amd03n[01-04],amd04n[01-04],amd05n[01-04],amd06n[01-04] parallel up 10-00:00:0 2 alloc amd07n[03-04] PARTITION AVAIL TIMELIMIT NODES STATE NODELIST gpu up 1-00:00:00 1 mix gpu02 gpu up 1-00:00:00 2 idle gpu[01,03] PARTITION AVAIL TIMELIMIT NODES STATE NODELIST bigmem up 10-00:00:0 3 mix high[01-02,04] bigmem up 10-00:00:0 1 alloc high03 PARTITION AVAIL TIMELIMIT NODES STATE NODELIST longrun up 30-00:00:0 2 idle bigtmp[01-02] NOTE: This utility is a wrapper for the Slurm command: sinfo -p PARTITION Notice the STATE field, this describes the current condition of nodes within the partition, the most common states are defined as: idle - nodes in an idle state have no jobs running, all resources are available for work mix - nodes in a mixed state have some jobs running, but still have some resources available for work alloc - nodes in an alloc state are completely full, all resources are in use. drain - nodes in a drain state have some running jobs, but no new jobs can be run. This is typically done before the node goes into maintenance maint - node is in maintenance mode, no jobs can be submitted resv - node is in a reservation. A reservation is setup for future maintenance or for special purposes such as temporary dedicated access down - node is down, either for maitnenance or due to failure Also notice the TIMELIMIT field, this describes the maximum runtime of a partition. For example, the quicktest partition has a maximum runtime of 1 hour and the parallel partition has a max runtime of 10 days. Partition Descriptions \u00b6 Partition: quicktest \u00b6 This partition is for quick tests of code, environment, software builds or similar short-run jobs. Since the max time limit is 5 hours it should not take long for your job to run. This can also be used for near-on-demand interactive jobs. Note that unlike the other partitions, these nodes have intel cpus. Quicktest nodes available: 6 Maximum CPU available per task: 64 Maximum memory available per task: 128G Optimal cpu/mem ratio: 1 cpu/2G ram Minimum allocated cpus: 2 - Slurm won't split an SMT core between users/jobs Maximum Runtime: 5 hours Partition: parallel \u00b6 This partition is useful for parallel workflows, either loosely coupled or jobs requiring MPI or other message passing protocols for tightly bound jobs. The total number of CPU's in this partition is 6816 with 2GB ram per CPU. AMD nodes - amdXXnXX AMD nodes available: 28 Maximum CPU available per task: 256 Maximum memory available per task: 512G Optimal cpu/mem ratio: 1 cpu/2G ram Minimum allocated cpus: 2 - Slurm won't split an SMT core between users/jobs Maximum Runtime: 10 days Partition: gpu \u00b6 This partition is for those jobs that require GPUs or those software that work with the CUDA platform and API (tensorflow, pytorch, MATLAB, etc) GPU nodes available: 3 GPUs available per node: 2 (A100's) Maximum CPU available per task: 256 Maximum memory available per task: 512G Optimal cpu/mem ratio: 1 cpu/2G ram Minimum allocated cpus: 2 - Slurm won't split an SMT core between users/jobs Maximum Runtime: 24 hours Note : To request GPU add the parameter, --gres=gpu:X Where X is the number of GPUs required, typically 1: --gres=gpu:1 - Partition: bigmem \u00b6 This partition is primarily useful for jobs that require very large shared memory (greater than 125 GB). These are known as memory-bound jobs. NOTE: Please do not schedule jobs of less than 125GB of memory on the bigmem partition. Bigmem nodes available: 4 (4x1024G ram) Maximum CPU available per task: 128 Maximum memory available per task: 1 TB Optimal cpu/mem ratio: 1 cpu/8G ram - note jobs here often use much more ram than this. Minimum allocated cpus: 1 - These cpus are not currently SMT enabled. Maximum Runtime: 10 days Note : The bigmem nodes also each have one NVIDIA Tesla T4 GPU. These are not as powerful as the A100's in the gpu nodes, but may still be of use at times when (a) the gpu partition is particularly busy or a gpu node is down, and (b) the bigmem node is being under-utilised. (I.e. please try to avoid using these gpus when there is a high demand for jobs requiring lots of memory in the bigmem partition, at other time, please go ahead.) Partition: longrun \u00b6 This partition is useful for long running jobs (with modest resource requirements). The total number of CPU's in this partition is 512 with 2GB ram per CPU. AMD nodes - bigtmp## AMD nodes available: 2 Maximum CPU available per task: 256 Maximum memory available per task: 512G Optimal cpu/mem ratio: 1 cpu/2G ram Minimum allocated cpus: 2 - Slurm won't split an SMT core between users/jobs Maximum Runtime: 30 days Cluster Default Resources \u00b6 Please note that if you do not specify the Partition, CPU, Memory or Time in your job request (via srun or sbatch ) you will be assigned the corresponding cluster defaults. The defaults are: Default Partition: quicktest Default CPUs: 2 Default Memory: 2 GB Default Time: 1 hour You can change these with the --partition , -c, --mem and --time parameters, respectively, to the srun and sbatch commands. Please see this section of the documentation for more information on how to run jobs using srun and sbatch .","title":"Using Partitions"},{"location":"partitions/#partitions","text":"","title":"Partitions"},{"location":"partitions/#using-partitions","text":"A partition is a collection of compute nodes, think of it as a sub-cluster or slice of the larger cluster. Each partition has its own rules and configurations. For example, the quicktest partition has a maximum job run-time of 5 hours, whereas the partition bigmem has a maximum runtime of 10 days. Partitions can also limit who can run a job. Currently any user can use any partition but there may come a time when certain research groups purchase their own nodes and they are given exclusive access. To view the partitions available to use you can type the vuw-partitions command, eg <user>@raapoi-login:~$ vuw-partitions VUW CLUSTER PARTITIONS PARTITION AVAIL TIMELIMIT NODES STATE NODELIST quicktest* up 5:00:00 1 down* amd01n01 quicktest* up 5:00:00 4 mix amd01n[02-03] quicktest* up 5:00:00 1 idle amd01n04 PARTITION AVAIL TIMELIMIT NODES STATE NODELIST parallel up 10-00:00:0 1 resv spj01 parallel up 10-00:00:0 24 mix amd02n[01-04],amd03n[01-04],amd04n[01-04],amd05n[01-04],amd06n[01-04] parallel up 10-00:00:0 2 alloc amd07n[03-04] PARTITION AVAIL TIMELIMIT NODES STATE NODELIST gpu up 1-00:00:00 1 mix gpu02 gpu up 1-00:00:00 2 idle gpu[01,03] PARTITION AVAIL TIMELIMIT NODES STATE NODELIST bigmem up 10-00:00:0 3 mix high[01-02,04] bigmem up 10-00:00:0 1 alloc high03 PARTITION AVAIL TIMELIMIT NODES STATE NODELIST longrun up 30-00:00:0 2 idle bigtmp[01-02] NOTE: This utility is a wrapper for the Slurm command: sinfo -p PARTITION Notice the STATE field, this describes the current condition of nodes within the partition, the most common states are defined as: idle - nodes in an idle state have no jobs running, all resources are available for work mix - nodes in a mixed state have some jobs running, but still have some resources available for work alloc - nodes in an alloc state are completely full, all resources are in use. drain - nodes in a drain state have some running jobs, but no new jobs can be run. This is typically done before the node goes into maintenance maint - node is in maintenance mode, no jobs can be submitted resv - node is in a reservation. A reservation is setup for future maintenance or for special purposes such as temporary dedicated access down - node is down, either for maitnenance or due to failure Also notice the TIMELIMIT field, this describes the maximum runtime of a partition. For example, the quicktest partition has a maximum runtime of 1 hour and the parallel partition has a max runtime of 10 days.","title":"Using Partitions"},{"location":"partitions/#partition-descriptions","text":"","title":"Partition Descriptions"},{"location":"partitions/#partition-quicktest","text":"This partition is for quick tests of code, environment, software builds or similar short-run jobs. Since the max time limit is 5 hours it should not take long for your job to run. This can also be used for near-on-demand interactive jobs. Note that unlike the other partitions, these nodes have intel cpus. Quicktest nodes available: 6 Maximum CPU available per task: 64 Maximum memory available per task: 128G Optimal cpu/mem ratio: 1 cpu/2G ram Minimum allocated cpus: 2 - Slurm won't split an SMT core between users/jobs Maximum Runtime: 5 hours","title":"Partition: quicktest"},{"location":"partitions/#partition-parallel","text":"This partition is useful for parallel workflows, either loosely coupled or jobs requiring MPI or other message passing protocols for tightly bound jobs. The total number of CPU's in this partition is 6816 with 2GB ram per CPU. AMD nodes - amdXXnXX AMD nodes available: 28 Maximum CPU available per task: 256 Maximum memory available per task: 512G Optimal cpu/mem ratio: 1 cpu/2G ram Minimum allocated cpus: 2 - Slurm won't split an SMT core between users/jobs Maximum Runtime: 10 days","title":"Partition: parallel"},{"location":"partitions/#partition-gpu","text":"This partition is for those jobs that require GPUs or those software that work with the CUDA platform and API (tensorflow, pytorch, MATLAB, etc) GPU nodes available: 3 GPUs available per node: 2 (A100's) Maximum CPU available per task: 256 Maximum memory available per task: 512G Optimal cpu/mem ratio: 1 cpu/2G ram Minimum allocated cpus: 2 - Slurm won't split an SMT core between users/jobs Maximum Runtime: 24 hours Note : To request GPU add the parameter, --gres=gpu:X Where X is the number of GPUs required, typically 1: --gres=gpu:1 -","title":"Partition: gpu"},{"location":"partitions/#partition-bigmem","text":"This partition is primarily useful for jobs that require very large shared memory (greater than 125 GB). These are known as memory-bound jobs. NOTE: Please do not schedule jobs of less than 125GB of memory on the bigmem partition. Bigmem nodes available: 4 (4x1024G ram) Maximum CPU available per task: 128 Maximum memory available per task: 1 TB Optimal cpu/mem ratio: 1 cpu/8G ram - note jobs here often use much more ram than this. Minimum allocated cpus: 1 - These cpus are not currently SMT enabled. Maximum Runtime: 10 days Note : The bigmem nodes also each have one NVIDIA Tesla T4 GPU. These are not as powerful as the A100's in the gpu nodes, but may still be of use at times when (a) the gpu partition is particularly busy or a gpu node is down, and (b) the bigmem node is being under-utilised. (I.e. please try to avoid using these gpus when there is a high demand for jobs requiring lots of memory in the bigmem partition, at other time, please go ahead.)","title":"Partition: bigmem"},{"location":"partitions/#partition-longrun","text":"This partition is useful for long running jobs (with modest resource requirements). The total number of CPU's in this partition is 512 with 2GB ram per CPU. AMD nodes - bigtmp## AMD nodes available: 2 Maximum CPU available per task: 256 Maximum memory available per task: 512G Optimal cpu/mem ratio: 1 cpu/2G ram Minimum allocated cpus: 2 - Slurm won't split an SMT core between users/jobs Maximum Runtime: 30 days","title":"Partition: longrun"},{"location":"partitions/#cluster-default-resources","text":"Please note that if you do not specify the Partition, CPU, Memory or Time in your job request (via srun or sbatch ) you will be assigned the corresponding cluster defaults. The defaults are: Default Partition: quicktest Default CPUs: 2 Default Memory: 2 GB Default Time: 1 hour You can change these with the --partition , -c, --mem and --time parameters, respectively, to the srun and sbatch commands. Please see this section of the documentation for more information on how to run jobs using srun and sbatch .","title":"Cluster Default Resources"},{"location":"running_jobs/","text":"Running Jobs \u00b6 Job Basics \u00b6 R\u0101poi uses a scheduler and resource manager called Slurm that requires researchers to submit jobs for processing. There are 2 main types of jobs: batch and interactive . More details about submitting these types of jobs are below, but in general interactive jobs allow a user to interact with the application, for example a researcher can start a MATLAB session and can type MATLAB commands at a prompt or within a GUI. Batch jobs can work in the background and require no user interaction, they will start when resources are available and can be configured to email once a job completes. Job resources \u00b6 Jobs require resources. Basic resources are CPU, memory (aka RAM) and time. If the researcher does not specify the number of CPUs, RAM and time, the defaults will be given (currently 2CPU, 2 GB RAM and 1 hour of runtime.) Details on requesting the basic resources are included in the Batch and Interactive sections below. Along with basic resources there can be other resources defined, such as GPU, license tokens, or even specific types of CPUs and CPU instruction sets. Special resources can be requested using the parameters --gres or --constraint For example, to request an Intel processor one can use the parameter: --constraint=\"Intel\" Currently defined constraints \u00b6 Below is a list of constraints that have need defined and a brief description: AMD - AMD processor IB - Infiniband network for tightly coupled and MPI processing Intel - Intel processor 10GE - 10 Gigabit Ethernet SSE41 - Streaming SIMD Extensions version 4.1 AVX - Advanced Vector Extensions For example, if you want to request a compute node with AMD processors you can add --constraint=\"AMD\" in your submit script or srun request. Interactive jobs \u00b6 One of the basic job submittal tools is the command srun . It is a useful command that let's us a) work on a compute node ( a.k.a request an interactive session ) and b) run your program written in python , R , etc. Interactive sessions will take me to one of the compute nodes where I can perform tasks such as writing my program, debugging, and even data transfer. a) To request an interactive session: username@raapoi-login:~$ srun --pty bash Depending on the node assigned by the scheduler (in this case - amd01n01 ), the prompt ( raapoi-login ) will change to: username@amd01n01:~$ #start_your_applications_here Tip You could playaround with flags for the srun command, e.g., srun -J<Job_Name> --cpus-per-task=2 --mem=4G --time=0-00:10:00 --pty bash . b) To run a small quick program I can use hello.py program available on R\u0101poi to test a quick example on my own. I can then use the method to run my other programs that finish under 5 hours. username@raapoi-login:~$ srun --mem=100M --time=00:10:00 --partition=quicktest python3 /home/software/tools/examples/python_venv/hello.py To run my own python program, I can do: username@raapoi-login:~$ srun --mem = 100M --time = 00 :10:00 --partition = quicktest python3 <your_program_name> In case I use a programming language other than python, the method is slightly different. For example, say I want to start a job to run an interactive R session. Once logged into the cluster I can: module purge # clean/reset your environment module load config # reload utilities such as vuw-job-report module load GCC/11.2.0 OpenMPI/4.1.1 # pre-requisites for the new R module module load R/4.2.0 srun --pty --cpus-per-task = 2 --mem = 2G --time = 05 :00:00 --partition = quicktest R Info It may take a few seconds until the prompt appears when the cluster is busy. Please wait for at least 2 mins before interreputing this step to avoid any unexpected behaviour. Note MATLAB on X server should use matlab -softwareopengl to run the application. Batch jobs \u00b6 To run a batch job (aka a job that runs unattended) you use the sbatch command. A simple example would look something like this: sbatch myjob.sl In this example the sbatch command runs the file myjob.sh, the contents of this file, also known as a \"batch submit script\" could look something like this: #!/bin/bash #SBATCH --cpus-per-task=2 #SBATCH --mem=2G #SBATCH --partition=parallel #SBATCH --constraint=AVX #SBATCH --output=slurm-%j.out #SBATCH --error=slurm-%j.err #SBATCH --mail-type=BEGIN,END,FAIL #SBATCH --mail-user=me@email.com module load python/3.6.8 python3 project1.py This will request 2 CPUs and 4GB of memory (2GB per CPU) and a runtime of 3 days 12 hours. We are requesting that this job be run on the parallel partition, it will then load the environment module for python version 3.6.3 and run a python script called project1.py. Any output from the script will be placed in your home directory in a file named project1.out and any error information in a file called project1.err. If you do not specify an output or error file, the default files will have the form of Slurm-jobID.o and Slurm-jobID.e and will be located in the directory from which you ran sbatch . NOTE: We have this example script available to copy on the cluster, you can type the following to copy it to your home directory: cp /home/software/tools/examples/batch/myjob.sh ~/myjob.sl The ~/ in front of the file is a short-cut to your home directory path. You will want to edit this file accordingly. For more information on the sbatch command, please use the manpages, eg: man sbatch So what does this all mean? The module load commands will introduce the environment necessary to run a particular program, in this case R version 4.2.0 The srun command will submit the job to the cluster. The srun command has many parameter available, some of the most common are in this example and explained below --pty - Required to run interactively --cpus-per-task=2 - requests 2 CPUs, can also use the -c flag, eg. -c 2 --mem=2G - requests 2 GigaBytes (GB) of RAM. --time=08:00:00 - requests a runtime of up to 8 hours (format is DAYS-HOURS:MINUTES:SECONDS), this is important in case the cluster or partition has a limited run-time, for example if an outage window is approaching. Keep in mind time is a resource along with CPU and Memory. --partition=quicktest - requests a certain partition, in this case it requests the quicktest partition, see the section on cluster partitions for more information. R - the command you wish to run, this could also be matlab, python, etc. (just remember to load the module first) For more information on the srun command, please use the manpages, eg: man srun Cluster Default Resources \u00b6 Please note that if you do not specify the Partition, CPU, Memory or Time in your job request (via srun or sbatch ) you will be assigned the corresponding cluster defaults. The defaults are: Default Partition: quicktest Default CPUs: 2 Default Memory: 2 GB Default Time: 1 hour You can change these with the --partition , -c, --mem and --time parameters, respectively, to the srun and sbatch commands. Please see this section of the documentation for more information on partitions.","title":"Running Jobs"},{"location":"running_jobs/#running-jobs","text":"","title":"Running Jobs"},{"location":"running_jobs/#job-basics","text":"R\u0101poi uses a scheduler and resource manager called Slurm that requires researchers to submit jobs for processing. There are 2 main types of jobs: batch and interactive . More details about submitting these types of jobs are below, but in general interactive jobs allow a user to interact with the application, for example a researcher can start a MATLAB session and can type MATLAB commands at a prompt or within a GUI. Batch jobs can work in the background and require no user interaction, they will start when resources are available and can be configured to email once a job completes.","title":"Job Basics"},{"location":"running_jobs/#job-resources","text":"Jobs require resources. Basic resources are CPU, memory (aka RAM) and time. If the researcher does not specify the number of CPUs, RAM and time, the defaults will be given (currently 2CPU, 2 GB RAM and 1 hour of runtime.) Details on requesting the basic resources are included in the Batch and Interactive sections below. Along with basic resources there can be other resources defined, such as GPU, license tokens, or even specific types of CPUs and CPU instruction sets. Special resources can be requested using the parameters --gres or --constraint For example, to request an Intel processor one can use the parameter: --constraint=\"Intel\"","title":"Job resources"},{"location":"running_jobs/#currently-defined-constraints","text":"Below is a list of constraints that have need defined and a brief description: AMD - AMD processor IB - Infiniband network for tightly coupled and MPI processing Intel - Intel processor 10GE - 10 Gigabit Ethernet SSE41 - Streaming SIMD Extensions version 4.1 AVX - Advanced Vector Extensions For example, if you want to request a compute node with AMD processors you can add --constraint=\"AMD\" in your submit script or srun request.","title":"Currently defined constraints"},{"location":"running_jobs/#interactive-jobs","text":"One of the basic job submittal tools is the command srun . It is a useful command that let's us a) work on a compute node ( a.k.a request an interactive session ) and b) run your program written in python , R , etc. Interactive sessions will take me to one of the compute nodes where I can perform tasks such as writing my program, debugging, and even data transfer. a) To request an interactive session: username@raapoi-login:~$ srun --pty bash Depending on the node assigned by the scheduler (in this case - amd01n01 ), the prompt ( raapoi-login ) will change to: username@amd01n01:~$ #start_your_applications_here Tip You could playaround with flags for the srun command, e.g., srun -J<Job_Name> --cpus-per-task=2 --mem=4G --time=0-00:10:00 --pty bash . b) To run a small quick program I can use hello.py program available on R\u0101poi to test a quick example on my own. I can then use the method to run my other programs that finish under 5 hours. username@raapoi-login:~$ srun --mem=100M --time=00:10:00 --partition=quicktest python3 /home/software/tools/examples/python_venv/hello.py To run my own python program, I can do: username@raapoi-login:~$ srun --mem = 100M --time = 00 :10:00 --partition = quicktest python3 <your_program_name> In case I use a programming language other than python, the method is slightly different. For example, say I want to start a job to run an interactive R session. Once logged into the cluster I can: module purge # clean/reset your environment module load config # reload utilities such as vuw-job-report module load GCC/11.2.0 OpenMPI/4.1.1 # pre-requisites for the new R module module load R/4.2.0 srun --pty --cpus-per-task = 2 --mem = 2G --time = 05 :00:00 --partition = quicktest R Info It may take a few seconds until the prompt appears when the cluster is busy. Please wait for at least 2 mins before interreputing this step to avoid any unexpected behaviour. Note MATLAB on X server should use matlab -softwareopengl to run the application.","title":"Interactive jobs"},{"location":"running_jobs/#batch-jobs","text":"To run a batch job (aka a job that runs unattended) you use the sbatch command. A simple example would look something like this: sbatch myjob.sl In this example the sbatch command runs the file myjob.sh, the contents of this file, also known as a \"batch submit script\" could look something like this: #!/bin/bash #SBATCH --cpus-per-task=2 #SBATCH --mem=2G #SBATCH --partition=parallel #SBATCH --constraint=AVX #SBATCH --output=slurm-%j.out #SBATCH --error=slurm-%j.err #SBATCH --mail-type=BEGIN,END,FAIL #SBATCH --mail-user=me@email.com module load python/3.6.8 python3 project1.py This will request 2 CPUs and 4GB of memory (2GB per CPU) and a runtime of 3 days 12 hours. We are requesting that this job be run on the parallel partition, it will then load the environment module for python version 3.6.3 and run a python script called project1.py. Any output from the script will be placed in your home directory in a file named project1.out and any error information in a file called project1.err. If you do not specify an output or error file, the default files will have the form of Slurm-jobID.o and Slurm-jobID.e and will be located in the directory from which you ran sbatch . NOTE: We have this example script available to copy on the cluster, you can type the following to copy it to your home directory: cp /home/software/tools/examples/batch/myjob.sh ~/myjob.sl The ~/ in front of the file is a short-cut to your home directory path. You will want to edit this file accordingly. For more information on the sbatch command, please use the manpages, eg: man sbatch So what does this all mean? The module load commands will introduce the environment necessary to run a particular program, in this case R version 4.2.0 The srun command will submit the job to the cluster. The srun command has many parameter available, some of the most common are in this example and explained below --pty - Required to run interactively --cpus-per-task=2 - requests 2 CPUs, can also use the -c flag, eg. -c 2 --mem=2G - requests 2 GigaBytes (GB) of RAM. --time=08:00:00 - requests a runtime of up to 8 hours (format is DAYS-HOURS:MINUTES:SECONDS), this is important in case the cluster or partition has a limited run-time, for example if an outage window is approaching. Keep in mind time is a resource along with CPU and Memory. --partition=quicktest - requests a certain partition, in this case it requests the quicktest partition, see the section on cluster partitions for more information. R - the command you wish to run, this could also be matlab, python, etc. (just remember to load the module first) For more information on the srun command, please use the manpages, eg: man srun","title":"Batch jobs"},{"location":"running_jobs/#cluster-default-resources","text":"Please note that if you do not specify the Partition, CPU, Memory or Time in your job request (via srun or sbatch ) you will be assigned the corresponding cluster defaults. The defaults are: Default Partition: quicktest Default CPUs: 2 Default Memory: 2 GB Default Time: 1 hour You can change these with the --partition , -c, --mem and --time parameters, respectively, to the srun and sbatch commands. Please see this section of the documentation for more information on partitions.","title":"Cluster Default Resources"},{"location":"storage/","text":"Storage and quotas \u00b6 Shared Storage \u00b6 Currently users have 3 main storage areas share across every node. Each node has access to to this storage at all times and data is shared. Be careful with parallel jobs trying to write to the same filename! /nfs/home/USERNAME - This is your Home Directory, each user has a 50 GB quota limit. The data is replicated off site and backed up regularly by Digital Solutions. Home directory tips /nfs/scratch/USERNAME - This is your scratch space, each user has a 5 TB quota limit. This data is not backed up! Scratch directory tips /nfs/scratch/noquota-volatile - This is an additional filesystem (previously referred to as beegfs). There is no quota enforcement here. There is 100TB of total space. This data is not backed up! All data on this storage is periodically deleted Note: Home directory quotas cannot be increased, however if you need more space in your scratch folder let us know. To view your current quota and usage use the vuw-quota command, for example: <username@raapoi-login:~$ vuw-quota User Quotas Storage Usage ( GB ) Quota ( GB ) % Used /nfs/home/<username> 18 .32 50 .00 36 .63% /nfs/scratch/<username> 0 .00 5000 .00 0 .00% Per Node Storage \u00b6 Each compute node has local storage you can use at /tmp . This storage is not shared so a program running on amd01n02 will not be able to see data stored on node amd01n04 's /tmp storage. Additionally, you can only access /tmp on any given node via a job running on that node. On the AMD nodes and GPU nodes the /tmp storage is very fast nvme storage with 1.7TB total space. On the Intel and highmem nodes this storage is slower and 1.7TB is not always available. Temp Disk Tips If you use the /tmp storage it is your responsibility to copy data to the /tmp and clean it up when your job is done. For more info see Temp Disk Tips . Storage Performance \u00b6 graph TD A(Home and Research Storage) --> B B[Scratch] --> D D[local tmp on AMD nodes] Figure 1: Storage speed hierarchy. The slowest storage is your user home directory as well as any mounted research storage. The trade off for this is that this data is replicated off site as well as backed up by Digital Solutions. The fastest is the local tmp space on the AMD nodes - it is usually deleted shortly after you logout and only visible to the node it's on, but it is extremely fast with excellent IO performance. Storage tips \u00b6 Home Directory Tips \u00b6 Home directories have a small quota and are on fairly slow storage. The data here is backed up . It is replicated off site live as well as periodically backed up to off site tape. In theory data here is fairly safe, even in a catastophic event it should be recoverable eventually. If you accidentally delete something here it can be recovered with a service desk request. While this storage is not performant, is is quite safe and is a good place for you scripts and code to live. Your data sets can also be on your home if they fit and the performance doesn't cause you any problems. For bigger or faster storage see the Scratch page. Scratch Tips \u00b6 The scratch storage is on a large storage node with 2 raid arrays with 50TB of storage each. Your scratch will always be available at /nfs/scratch/<username> . Your scratch storage could be on scratch or scratch2, to find out run vuw-quota . Each user has a quota of 5TB on scratch - you can ask the support team to increase it if needed. While each user has a quota of 5TB, we don't actually have enough storage for each user to fill 5TB of storage! This is a shared resource and we will occasionally ask on the slack channel for users to clean up their storage to make space for others. Individuals using a large amount of scratch space may recieve an email. To check how much space is free on the scratch storage for all users, on R\u0101poi: df -h | grep scratch #df -h is disk free with human units, | pipes the output to grep, which shows lines which contain the word scratch On the slack channel in any DM or channel type /df-scratch The output should only be visible to you This storage is not backed up at all. It is on a raid array so if a hard drive fails your data is safe. However in the event of a more dramatic hardware failure, earthquakes or fire - your data is gone forever. If you accidentally delete something, it's gone forever. If an Admin misconfigures something, your data is gone (we try not to do this!). It is your responsiblilty to backup your data here - a good place is to use Digital Solutions Research Storage (see Connecting to SoLAR ). Scratch is also not a place for your old data to live forever, please clean up datasets you're no longer using! Temp Disk Tips \u00b6 This storage is very fast on the AMD nodes and GPU nodes. It is your job to move data to the tmp space and clean it up when done. There is very little management of this space and currently it is not visible to slurm for fair use scheduling - in other words someone else might have used up most of the temp space on the node! This is generally not the case though. A rough example of how you could use this in an sbatch script #!/bin/bash # #SBATCH --job-name=bash_test # #SBATCH --partition=quicktest # #SBATCH --cpus-per-task=2 #Note: you are always allocated an even number of cpus #SBATCH --mem=1G #SBATCH --time=10:00 # Do the needed module loading for your use case module load etc #make a temporary directory with your usename so you don't tread on others mkdir /tmp/<username> #Copy dataset from scratch to the local tmp on the node (could also use rsync) cp -r /nfs/scratch/<user>/dataset /tmp/<user>/dataset Process data against /tmp/<user>/dataset Lets say the data is output to /tmp/<user>/dataoutput/ # Copy data from output to your scratch - I suggest not overwriting your original dataset! cp -r /tmp/<user>/dataoutput/* /nfs/scratch/<user>/dataset/dataoutput/ # Delete the data you copy to and created on tmp rm -rf /tmp/<user> #DANGER!! BeeGFS Tips \u00b6 The BeeGFS storage is spread across 3 nodes with SSD disks. The aggregate storage is 100TB. We don't enforce quotas here as some projects have large storage needs. However, you do need to be a good HPC citizen and respect the rights of others. Don't needlessly fill up this storage. We regularly delete all data here every 3-6 months. We only post warnings on the slack channel . If after 3 months the storage is not full and still performning well, we delay the wipe another 3 months. This storage is not backed up at all. It is on a raid array so if a hard drive fails your data is safe. However in the event of a more dramatic hardware failure, earthquakes or fire - your data is gone forever. If you accidentally delete something, it's gone forever. If an Admin misconfigures something, your data is gone (we try not to do this!). It is your responsiblilty to backup your data here - a good place is to use Digital Solutions Research Storage. BeeGFS should have better IO performance than the scratch storage - however it does depend what other users are doing. No filesystem likes small files, BeeGFS likes small files even less than scratch. Filesizes over 1MB are best. If you have a large amount of files you can improve performance by splitting your files accross many directories - the load balancing accross the metadata and storage servers is by directory, not file.","title":"Storage and Quotas"},{"location":"storage/#storage-and-quotas","text":"","title":"Storage and quotas"},{"location":"storage/#shared-storage","text":"Currently users have 3 main storage areas share across every node. Each node has access to to this storage at all times and data is shared. Be careful with parallel jobs trying to write to the same filename! /nfs/home/USERNAME - This is your Home Directory, each user has a 50 GB quota limit. The data is replicated off site and backed up regularly by Digital Solutions. Home directory tips /nfs/scratch/USERNAME - This is your scratch space, each user has a 5 TB quota limit. This data is not backed up! Scratch directory tips /nfs/scratch/noquota-volatile - This is an additional filesystem (previously referred to as beegfs). There is no quota enforcement here. There is 100TB of total space. This data is not backed up! All data on this storage is periodically deleted Note: Home directory quotas cannot be increased, however if you need more space in your scratch folder let us know. To view your current quota and usage use the vuw-quota command, for example: <username@raapoi-login:~$ vuw-quota User Quotas Storage Usage ( GB ) Quota ( GB ) % Used /nfs/home/<username> 18 .32 50 .00 36 .63% /nfs/scratch/<username> 0 .00 5000 .00 0 .00%","title":"Shared Storage"},{"location":"storage/#per-node-storage","text":"Each compute node has local storage you can use at /tmp . This storage is not shared so a program running on amd01n02 will not be able to see data stored on node amd01n04 's /tmp storage. Additionally, you can only access /tmp on any given node via a job running on that node. On the AMD nodes and GPU nodes the /tmp storage is very fast nvme storage with 1.7TB total space. On the Intel and highmem nodes this storage is slower and 1.7TB is not always available. Temp Disk Tips If you use the /tmp storage it is your responsibility to copy data to the /tmp and clean it up when your job is done. For more info see Temp Disk Tips .","title":"Per Node Storage"},{"location":"storage/#storage-performance","text":"graph TD A(Home and Research Storage) --> B B[Scratch] --> D D[local tmp on AMD nodes] Figure 1: Storage speed hierarchy. The slowest storage is your user home directory as well as any mounted research storage. The trade off for this is that this data is replicated off site as well as backed up by Digital Solutions. The fastest is the local tmp space on the AMD nodes - it is usually deleted shortly after you logout and only visible to the node it's on, but it is extremely fast with excellent IO performance.","title":"Storage Performance"},{"location":"storage/#storage-tips","text":"","title":"Storage tips"},{"location":"storage/#home-directory-tips","text":"Home directories have a small quota and are on fairly slow storage. The data here is backed up . It is replicated off site live as well as periodically backed up to off site tape. In theory data here is fairly safe, even in a catastophic event it should be recoverable eventually. If you accidentally delete something here it can be recovered with a service desk request. While this storage is not performant, is is quite safe and is a good place for you scripts and code to live. Your data sets can also be on your home if they fit and the performance doesn't cause you any problems. For bigger or faster storage see the Scratch page.","title":"Home Directory Tips"},{"location":"storage/#scratch-tips","text":"The scratch storage is on a large storage node with 2 raid arrays with 50TB of storage each. Your scratch will always be available at /nfs/scratch/<username> . Your scratch storage could be on scratch or scratch2, to find out run vuw-quota . Each user has a quota of 5TB on scratch - you can ask the support team to increase it if needed. While each user has a quota of 5TB, we don't actually have enough storage for each user to fill 5TB of storage! This is a shared resource and we will occasionally ask on the slack channel for users to clean up their storage to make space for others. Individuals using a large amount of scratch space may recieve an email. To check how much space is free on the scratch storage for all users, on R\u0101poi: df -h | grep scratch #df -h is disk free with human units, | pipes the output to grep, which shows lines which contain the word scratch On the slack channel in any DM or channel type /df-scratch The output should only be visible to you This storage is not backed up at all. It is on a raid array so if a hard drive fails your data is safe. However in the event of a more dramatic hardware failure, earthquakes or fire - your data is gone forever. If you accidentally delete something, it's gone forever. If an Admin misconfigures something, your data is gone (we try not to do this!). It is your responsiblilty to backup your data here - a good place is to use Digital Solutions Research Storage (see Connecting to SoLAR ). Scratch is also not a place for your old data to live forever, please clean up datasets you're no longer using!","title":"Scratch Tips"},{"location":"storage/#temp-disk-tips","text":"This storage is very fast on the AMD nodes and GPU nodes. It is your job to move data to the tmp space and clean it up when done. There is very little management of this space and currently it is not visible to slurm for fair use scheduling - in other words someone else might have used up most of the temp space on the node! This is generally not the case though. A rough example of how you could use this in an sbatch script #!/bin/bash # #SBATCH --job-name=bash_test # #SBATCH --partition=quicktest # #SBATCH --cpus-per-task=2 #Note: you are always allocated an even number of cpus #SBATCH --mem=1G #SBATCH --time=10:00 # Do the needed module loading for your use case module load etc #make a temporary directory with your usename so you don't tread on others mkdir /tmp/<username> #Copy dataset from scratch to the local tmp on the node (could also use rsync) cp -r /nfs/scratch/<user>/dataset /tmp/<user>/dataset Process data against /tmp/<user>/dataset Lets say the data is output to /tmp/<user>/dataoutput/ # Copy data from output to your scratch - I suggest not overwriting your original dataset! cp -r /tmp/<user>/dataoutput/* /nfs/scratch/<user>/dataset/dataoutput/ # Delete the data you copy to and created on tmp rm -rf /tmp/<user> #DANGER!!","title":"Temp Disk Tips"},{"location":"storage/#beegfs-tips","text":"The BeeGFS storage is spread across 3 nodes with SSD disks. The aggregate storage is 100TB. We don't enforce quotas here as some projects have large storage needs. However, you do need to be a good HPC citizen and respect the rights of others. Don't needlessly fill up this storage. We regularly delete all data here every 3-6 months. We only post warnings on the slack channel . If after 3 months the storage is not full and still performning well, we delay the wipe another 3 months. This storage is not backed up at all. It is on a raid array so if a hard drive fails your data is safe. However in the event of a more dramatic hardware failure, earthquakes or fire - your data is gone forever. If you accidentally delete something, it's gone forever. If an Admin misconfigures something, your data is gone (we try not to do this!). It is your responsiblilty to backup your data here - a good place is to use Digital Solutions Research Storage. BeeGFS should have better IO performance than the scratch storage - however it does depend what other users are doing. No filesystem likes small files, BeeGFS likes small files even less than scratch. Filesizes over 1MB are best. If you have a large amount of files you can improve performance by splitting your files accross many directories - the load balancing accross the metadata and storage servers is by directory, not file.","title":"BeeGFS Tips"},{"location":"support/","text":"Check out the Documentation section for a comprehensive overview of how to use R\u0101poi. Visit the R\u0101poi Slack Channel for help, software requests or to communicate with others in the R\u0101poi community. Need an account on R\u0101poi or access to the Slack channel? Log a request on the Service Desk Portal or Contact the R\u0101poi Admins: Research Compute Specialist \u00b6 Our research compute specialist is usually ready to assist you in getting started with your work on R\u0101poi and effectively handling your research data within the cluster. The Research Computing Specialist role\u2014previously held by Rohit Duggal in ECS\u2014is currently vacant. While a replacement is being sought, please direct any support queries to the #raapoi-help channel in UWRC Slack . This space is actively used by experienced cluster users who are likely to have the knowledge and expertise to assist you. We appreciate your understanding and encourage peer collaboration in the meantime. ????? ????? Research Computing Specialist - R\u0101poi Position Vacant R\u0101poi Systems Administrators \u00b6 Our admins from Digital Solutions build the cluster nodes and keep an eye on the hardware as well as provisioning new users. The listed folk in the team have other roles and are very kindly helping out on a best effort as time permits basis. David Alderman Systems Administrator in Digital Solutions. Email Me Nick Bray Technical Specialist - Storage in Digital Solutions. Email Me R\u0101poi Moderators \u00b6 Moderators are members of the research community who have kindly volunteered their time to help support Raapoi - they can help build software and limit users who are causing problems on the cluster. They're busy people, but they might find time to build your weird software package - on a best effort and as time permits basis. Wanting Jiao Senior Scientist Ferrier Research Institute, Schr\u00f6dinger Suite Email Me Geoffrey Weal Postdoctorial Fellowship MacDiarmid Institute and iCeMS (Kyoto University, Japan) Email Me Kelly Styles PhD Student School of Biological Sciences Microbial genomics and pipeline creation Email Me Brendan Harding Lecturer School of Mathematics and Statistics Faculty of Engineering Email Me R\u0101poi Admin Alumni \u00b6 Folk who used to look after Raapoi. Rohit Duggal Research Computing Specialist - R\u0101poi Left September 2025 Andre Geldenhuis Research Computing Specialist in the Center for Academic Development. Left August 2023 Matt Plummer Senior Research Partner. Email Me Wes Harrell Research Software Engineer Alice Fage Research Assistant who helped out greatly with cluster admin R\u0101poi Moderator Alumni \u00b6 Folk who were moderators in the past. SNAP Community \u00b6 Connect with SNAP community here: SNAP - VUW-hub for Simulation, Numerics, Analytics and Programming","title":"Support"},{"location":"support/#research-compute-specialist","text":"Our research compute specialist is usually ready to assist you in getting started with your work on R\u0101poi and effectively handling your research data within the cluster. The Research Computing Specialist role\u2014previously held by Rohit Duggal in ECS\u2014is currently vacant. While a replacement is being sought, please direct any support queries to the #raapoi-help channel in UWRC Slack . This space is actively used by experienced cluster users who are likely to have the knowledge and expertise to assist you. We appreciate your understanding and encourage peer collaboration in the meantime.","title":"Research Compute Specialist"},{"location":"support/#rapoi-systems-administrators","text":"Our admins from Digital Solutions build the cluster nodes and keep an eye on the hardware as well as provisioning new users. The listed folk in the team have other roles and are very kindly helping out on a best effort as time permits basis.","title":"R\u0101poi Systems Administrators"},{"location":"support/#rapoi-moderators","text":"Moderators are members of the research community who have kindly volunteered their time to help support Raapoi - they can help build software and limit users who are causing problems on the cluster. They're busy people, but they might find time to build your weird software package - on a best effort and as time permits basis.","title":"R\u0101poi Moderators"},{"location":"support/#rapoi-admin-alumni","text":"Folk who used to look after Raapoi.","title":"R\u0101poi Admin Alumni"},{"location":"support/#rapoi-moderator-alumni","text":"Folk who were moderators in the past.","title":"R\u0101poi Moderator Alumni"},{"location":"support/#snap-community","text":"Connect with SNAP community here: SNAP - VUW-hub for Simulation, Numerics, Analytics and Programming","title":"SNAP Community"},{"location":"training/","text":"Training \u00b6 These are longer worked examples. If you have domain specific training you'd like to provide for your students or peers, contact someone on the support team , or make a pull request against this repo. GPU example with neural style in pytorch \u00b6 We'll do a quick python example using neural style implemented in pytorch. We will be using modules rather than conda/virtualenvs but there is nothing stopping you from loading the modules and creating a virtualenv/conda enviroment to install additional python packages. The code we use will come from the pytorch example git repo. Clone the pytorch example repo \u00b6 In a sensible location, clone the rep. git clone https://github.com/pytorch/examples.git cd examples/fast_neural_style # change to the example we will be running. Load the modules \u00b6 We are using the new Easybuild based modules, to ensure we don't have conflicts with the old modules, it will be best to unuse them first and then use the new system. At somepoint we may automatically add the new modules to your bashrc file - but currently you'll have to do this yourself or manually unuse and use the new module system. module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 module list #see all the dependencies we have loaded, in particular which version of python we're using now. Currently Python 3.8.6 PyTorch note: When running jobs which utilise PyTorch, make sure you allocate sufficient memory for the job. If you encounter error messages which are vague, it is possible that you don't have enough memory allocated. Just to import torch it is recommended to have 4GB (or you may see errors such as ImportError: <library>.so: failed to map segment from shared object ). Optional: Setup a virtualenv \u00b6 python3 -m venv env # create a virtualenv folder called env. Note! This will likely only work with the python version listed above! source env/bin/activate # activate the virtualenv Now that we've activated the virtual environment, we can install any additional packages we need. In this case we don't need any additional packages. Download some images to use as content as well as for training. \u00b6 In your examples/fast_neural_style/ directory. # Download an image of an octopus to images/content-images. ## CC BY-SA 3.0 H. Zell wget https://upload.wikimedia.org/wikipedia/commons/0/0c/Octopus_vulgaris_02.JPG -P images/content-images/ # Download an image of The Great Wave off Kanagawa - public domain wget https://upload.wikimedia.org/wikipedia/commons/a/a5/Tsunami_by_hokusai_19th_century.jpg -O images/style-images/wave.jpg Depending on the GPU we are using, we may need to resize the image to ensure it fits in memory. On an RTX6000 we would need to resize the image to 70% of its full size to fit in memroy. Thankfully the GPUs on R\u0101poi are A100's with 40GB of ram, so we can skip this step. We will also need to download the pre-trained models for our initial inference runs. python download_saved_models.py Style some images - inference \u00b6 We'll initially just use pretrained models to generate styled images - this is known as model inference and is much less intensive than training the model, we'll do this on both CPU and GPU. submit_cpu.sh #!/bin/bash #SBATCH --job-name=pytorch_test #SBATCH -o _test.out #SBATCH -e _test.err #SBATCH --time=00:15:00 #SBATCH --partition=parallel #SBATCH --ntasks=12 #SBATCH --mem=6G module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 #Optional source env/bin/activate #activate the virtualenv # Run our job --cuda 0 means run on the CPU and we'll save the output image as test1.jpg # python neural_style/neural_style.py eval --content-image images/content-images/Octopus_vulgaris_02.JPG --model saved_models/mosaic.pth --output-image ./test1.jpg --cuda 0 You can check how long the job took to run with vuw-job-history . The last lines are your last run job, in my case: 332281 COMPLETED pytorch_t+ 00 :02:36 332281 .batch COMPLETED batch 0 .15G 00 :02:36 332281 .exte+ COMPLETED extern 0 .15G 00 :02:36 the job took 2:36. Let's run the inference job again on GPU to see the speedup. submit_gpu.sh #!/bin/bash #SBATCH --job-name=pytorch_test #SBATCH -o _test.out #SBATCH -e _test.err #SBATCH --time=00:15:00 #SBATCH --partition=gpu #SBATCH --gres=gpu:1 #SBATCH --ntasks=2 #SBATCH --mem=60G module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 #optional source env/bin/activate #activate the virtualenv # Run our job --cuda 1 means run on the GPU and we'll save the output image as test2.jpg # python neural_style/neural_style.py eval --content-image images/content-images/Octopus_vulgaris_02.JPG --model saved_models/mosaic.pth --output-image ./test2.jpg --cuda 1 In this case vuw-job-history the job took: 692973 COMPLETED pytorch_t+ 00 :00:16 692973 .batch COMPLETED batch 0 .15G 00 :00:16 692973 .exte+ COMPLETED extern 0 .15G 00 :00:16 but the time varies a lot with short GPU runs, some are nearly 2 min long and some runs are 16s with the same data. The memory usage with pytorch is also hard to estimate, running vuw-job-report 332320 shows: Nodes: 1 Cores per node: 2 CPU Utilized: 00 :00:07 CPU Efficiency: 43 .75% of 00 :00:16 core-walltime Job Wall-clock time: 00 :00:08 Memory Utilized: 1 .38 MB Memory Efficiency: 0 .00% of 60 .00 GB The memory usage is very low, but there is a very brief spike in memory at the end of the run as the image is generated that vuw-job-report doesn't quite capture. 60G of memory is needed to ensure this completes - a good rule of thumb is to allocate at least as much system memory as GPU memory. The A100's have 40G of ram. Train a new style - computationally expensive. \u00b6 Training a new image style is where we will get the greatest speedup using a GPU. We will use 13G of training images - COCO 2014 Training images dataset . These images have already been downloaded and are accessable at /nfs/home/training/neural_style_data/train2014/ . Note that training a new style will take about 1:15h on an A100 and two and a half hours on an RTX6000 #!/bin/bash #SBATCH --job-name=pytorch_test #SBATCH -o _test.out #SBATCH -e _test.err #SBATCH --time=03:00:00 #SBATCH --partition=gpu #SBATCH --gres=gpu:1 #SBATCH --ntasks=2 #SBATCH --mem=60G module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 #Optional source env/bin/activate #activate the virtualenv # Run our job --cuda 1 means run on the GPU # style-weight and content-weight are just parameters adjusted to give better results python neural_style/neural_style.py train \\ --dataset /nfs/home/training/neural_style_data/ \\ --style-image images/style-images/wave.jpg \\ --save-model-dir saved_models/style5e10_content_5e4 \\ --style-weight 5e10 \\ --content-weight 5e4 \\ --epochs 2 \\ --cuda 1 This will take a while, but should eventually complete. The A100 has enough memory to train on this image, with other GPUs you may need to scale down the style image to fit in the GPU memory. Note: If you get an out of GPU memory error but it seems the GPU has plenty of memory, it often means you ran out of system memory, try asking for more memory in slurm. Use our newly trained network \u00b6 submit_gpu.sh #!/bin/bash #SBATCH --job-name=pytorch_test #SBATCH -o _test.out #SBATCH -e _test.err #SBATCH --time=00:15:00 #SBATCH --partition=gpu #SBATCH --gres=gpu:1 #SBATCH --ntasks=2 #SBATCH --mem=60G module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 #Optional source env/bin/activate #activate the virtualenv # Run our job --cuda 1 means run on the GPU and we'll save the output image as test2.jpg # python neural_style/neural_style.py eval \\ --content-image images/content-images/Octopus_vulgaris_02.JPG \\ --model saved_models/style5e10_content_5e4 \\ --output-image ./test3.jpg --cuda 1 Bonus content use a slurm task-array to find the optimum parameters. \u00b6 In the above example we use parameters for style-weight and content-weight. There are lots of possibilities for these parameters, we can use a task array and a parameter list to determine good values. Note that actually running this example will consume a lot of resources and it is presented mostly to provide some information about task arrays. Running this example will consume the whole GPU partition for about 12 hours. First let's create a list of parameters to test, we could include these in the batch submission script, but I think it's clearer to separate them out. If you're version controlling your submission script, it'll make it easier to see what are changes to parameters and what are changes to the script itself. In the parameter list, the first column is style-weight parameters and the second is content-weight parameters. paramlist.txt 5e10 1e3 5e10 1e4 5e10 5e4 1e11 1e3 1e11 1e4 1e11 5e4 5e11 1e3 5e11 1e4 5e11 5e4 1e12 1e3 1e12 1e4 1e12 5e4 In our submission script we will parse these values with awk . Awk is a bit beyond the scope of this lesson, but it is a handy shell tool for manipulating text. Digital ocean has a nice primer on Awk submit_gpu_train_array #!/bin/bash #SBATCH --job-name=pytorch_test #SBATCH -o _test.out #SBATCH -e _test.err #SBATCH --time=10:00:00 #SBATCH --partition=gpu #SBATCH --gres=gpu:1 #SBATCH --ntasks=10 #SBATCH --mem=60G #SBATCH --array=1-13 module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 #Optional source env/bin/activate #activate the virtualenv # Run our job --cuda 1 means run on the GPU # #awk -v var=\"$SLURM_ARRAY_TASK_ID\" 'NR == var {print $1}' paramlist.txt style_weight = $( awk -v var = \" $SLURM_ARRAY_TASK_ID \" 'NR == var {print $1}' paramlist.txt ) content_weight = $( awk -v var = \" $SLURM_ARRAY_TASK_ID \" 'NR == var {print $2}' paramlist.txt ) echo $style_weight echo $content_weight python neural_style/neural_style.py train \\ --dataset nfs/home/training/neural_style_data/ \\ --style-image images/style-images/wave.jpg \\ --save-model-dir saved_models/test_params2_epoch2/style ${ style_weight } _content ${ content_weight } \\ --style-weight $style_weight \\ --content-weight $content_weight \\ --epochs 2 \\ --cuda 1 Simple OpenMPI with Singularity using the hybrid approach. \u00b6 The hybrid approach is one way of getting OpenMPI working with containers. It requires the OpenMPI version inside the container to match the OpenMPI outside the container (loaded via module loading). First check what openMPI version we have on R\u0101poi. On R\u0101poi switch to our new modules module unuse /home/software/tools/modulefiles # stop using the older modules module use /home/software/tools/eb_modulefiles/all/Core #the new module files organised by compiler module spider OpenMPI # search for openMPI - thre are several options, lets try module spider OpenMPI/4.0.5 # we will use this one, which requires GCC/10.2.0 On your local machine we will create a very simple C openMPI program. Create this in a sensible place. I used ~/projects/examples/singularity/openMPI #include <mpi.h> #include <stdio.h> #include <stdlib.h> int main ( int argc , char ** argv ) { int rc ; int size ; int myrank ; rc = MPI_Init ( & argc , & argv ); if ( rc != MPI_SUCCESS ) { fprintf ( stderr , \"MPI_Init() failed\" ); return EXIT_FAILURE ; } rc = MPI_Comm_size ( MPI_COMM_WORLD , & size ); if ( rc != MPI_SUCCESS ) { fprintf ( stderr , \"MPI_Comm_size() failed\" ); goto exit_with_error ; } rc = MPI_Comm_rank ( MPI_COMM_WORLD , & myrank ); if ( rc != MPI_SUCCESS ) { fprintf ( stderr , \"MPI_Comm_rank() failed\" ); goto exit_with_error ; } fprintf ( stdout , \"Hello, I am rank %d/%d\" , myrank , size ); MPI_Finalize (); return EXIT_SUCCESS ; exit_with_error : MPI_Finalize (); return EXIT_FAILURE ; } In the same location as above create a singularity definition file, note that we choose to compile and install the same OpenMPI version as we will use on R\u0101poi. Bootstrap: docker From: ubuntu:latest %files mpitest.c /opt %environment export OMPI_DIR = /opt/ompi export SINGULARITY_OMPI_DIR = $OMPI_DIR export SINGULARITYENV_APPEND_PATH = $OMPI_DIR /bin export SINGULAIRTYENV_APPEND_LD_LIBRARY_PATH = $OMPI_DIR /lib %post echo \"Installing required packages...\" apt-get update && apt-get install -y wget git bash gcc gfortran g++ make file echo \"Installing Open MPI\" export OMPI_DIR = /opt/ompi export OMPI_VERSION = 4 .0.5 #NOTE matching version to that on Raapoi export OMPI_URL = \"https://download.open-mpi.org/release/open-mpi/v4.0/openmpi- $OMPI_VERSION .tar.bz2\" mkdir -p /tmp/ompi mkdir -p /opt # Download cd /tmp/ompi && wget -O openmpi- $OMPI_VERSION .tar.bz2 $OMPI_URL && tar -xjf openmpi- $OMPI_VERSION .tar.bz2 # Compile and install cd /tmp/ompi/openmpi- $OMPI_VERSION && ./configure --prefix = $OMPI_DIR && make install # Set env variables so we can compile our application export PATH = $OMPI_DIR /bin: $PATH export LD_LIBRARY_PATH = $OMPI_DIR /lib: $LD_LIBRARY_PATH export MANPATH = $OMPI_DIR /share/man: $MANPATH echo \"Compiling the MPI application...\" cd /opt && mpicc -o mpitest mpitest.c Now we build our container locally, giving it a sensible name. We need OpenMPI-4.0.5 to use this, so let's include that in the name. sudo singularity build test-openmpi-4.0.5.sif test-openmpi-4.0.5.def Copy that file to R\u0101poi somehow - Filezilla, rsync or similar. I'll just use sftp for simplicity. sftp <username>@raapoi.vuw.ac.nz put test-openmpi-4.0.5.sif Now on R\u0101poi copy that file to a sensible location, I used ~/projects/examples/singularity/openMPI again. mv test-openmpi-4.0.5.sif ~/projects/examples/singularity/openMPI/ cd ~/projects/examples/singularity/openMPI/ In that location create a sbatch file openmpi-test.sh #!/bin/bash #SBATCH --job-name=mpi_test #SBATCH --time=00-00:02:00 #SBATCH --output=out_test.out #SBATCH --error=out_test.err #SBATCH --partition=parallel #SBATCH --ntasks=2 #SBATCH --cpus-per-task=1 #SBATCH --tasks-per-node=1 #SBATCH --mem=1GB #SBATCH --constraint=\"IB,AMD\" #SBATCH --nodes=2 module use /home/software/tools/eb_modulefiles/all/Core module unuse /home/software/tools/modulefiles # to prevent conflicts with the old modules module load GCC/10.2.0 module load OpenMPI/4.0.5 module load Singularity/3.7.3 # Note this is a new singularity build CONPATH = $HOME /projects/examples/singularity/openMPI mpirun -np 2 singularity exec $CONPATH /test-openmpi-4.0.5.sif /opt/mpitest Submit that to slurm and see the output sbatch openmpi-test.sh squeue -u $USER # see the job cat out_test.out # examine the output after the job is done Simple tensorflow example (using new module system) \u00b6 In a sensible location create an example python script - this is basically copied verbatim from the tensorflow docs: ps://www.tensorflow.org/tutorials/quickstart/beginner example.py import tensorflow as tf print ( \"TensorFlow version:\" , tf . __version__ ) # Load and prepare the MNIST dataset. Convert the sample data from integers to floating-point numbers mnist = tf . keras . datasets . mnist ( x_train , y_train ), ( x_test , y_test ) = mnist . load_data () x_train , x_test = x_train / 255.0 , x_test / 255.0 # Build a machine learning model model = tf . keras . models . Sequential ([ tf . keras . layers . Flatten ( input_shape = ( 28 , 28 )), tf . keras . layers . Dense ( 128 , activation = 'relu' ), tf . keras . layers . Dropout ( 0.2 ), tf . keras . layers . Dense ( 10 ) ]) # The model returns a vector of log-odds scores, one for each class predictions = model ( x_train [: 1 ]) . numpy () predictions # The tf.nn.softmax function converts these log odds to probabilities for each class tf . nn . softmax ( predictions ) . numpy () # Define a loss function for training. loss_fn = tf . keras . losses . SparseCategoricalCrossentropy ( from_logits = True ) # This untrained model gives probabilities close to random loss_fn ( y_train [: 1 ], predictions ) . numpy () # Configure and compile the model using Keras Model.compile model . compile ( optimizer = 'adam' , loss = loss_fn , metrics = [ 'accuracy' ]) # Train and evaluate the model - use Model.fit to adjust parameters and minimize loss model . fit ( x_train , y_train , epochs = 5 ) # Check model performance model . evaluate ( x_test , y_test , verbose = 2 ) # Return a probability - wrap the trained model and attach softmax probability_model = tf . keras . Sequential ([ model , tf . keras . layers . Softmax () ]) probability_model ( x_test [: 5 ]) Next create a submission script submit.sh #!/bin/bash #SBATCH --job-name=tensoflow_test #SBATCH -o _test.out #SBATCH --time=00:10:00 #SBATCH --partition=gpu #SBATCH --ntasks=6 #SBATCH --mem=50G #SBATCH --gres=gpu:1 # Use the new module system module use /home/software/tools/eb_modulefiles/all/Core #to load tf 2.6.0 you'll first need the compiler set it was built with module load foss/2021a #load tf module load TensorFlow/2.6.0-CUDA-11.3.1 # Run the simple tensorflow example - taken from the docs: https://www.tensorflow.org/tutorials/quickstart/beginner python example.py Submit your job to the queue and then observe in the queue sbatch submit.sh squeue -u <username> Possible Errors Tensorflow jobs on the gpu nodes can be a bit dicey I'd suggest always choosing more memory than the GPU has (40GB) the gpu nodes have a lot of memory so I'd suggest asking for 50GB of ram minimum. There is also a relationship between cpu's allocated and memory used - the errors are not always obvious. If you're running into issues try increasing the requested memory or reducing the requested CPUs Example errors due to requesting many cpus while requesting only 50GB ram Note std::bad_alloc - this suggests a problem allocating memory terminate called after throwing an instance of 'std::bad_alloc' what (): std :: bad_alloc / var / lib / slurm / slurmd / job1125851 / slurm_script : line 21 : 46983 Aborted ( core dumped ) python example . py Note this example is also in our example git repo: https://github.com/vuw-research-computing/raapoi-examples inside the tensorflow-simple directory Example Gaussian Job Submission on HPC \u00b6 Here is an example of submitting a Gaussian job on the HPC using Slurm. In this example, we will submit a Gaussian job using the quicktest partition, and request 1 task with 4 CPUs and 7GB of memory for a maximum run time of 1 hour. We will also load the g16 module, which is required to run Gaussian on the HPC. First, create a new directory and navigate to it: mkdir gaussian_example cd gaussian_example Get the example input file \u00b6 The test0397.com file is an example input file for Gaussian. It contains instructions for Gaussian to perform a calculation on a molecule. To run the example job using this input file, you should copy the test0397.com file from the Gaussian installation directory at /home/software/apps/gaussian/g16/tests/com/test0397.com to your working directory ( gaussian_example in this case). To do that from the gaussian_example directory: cp /home/software/apps/gaussian/g16/tests/com/test0397.com . # copy from location to . The dot means current directory Have a look at the first few lines of the input file to see what it does. head test0397.com # the first 5 lines of test0397.com #returns !%nproc = 4 #p rb3lyp/3-21g force test scf=novaracc Gaussian Test Job 397 : Valinomycin force 0 ,1 O,-1.3754834437,-2.5956821046,3.7664927822 O,-0.3728418073,-0.530460483,3.8840401686 O,2.3301890394,0.5231526187,1.7996834334 The first line !%nproc=4 specifies the number of processors that Gaussian will use to run the calculation, in this case, 4. We will need to make sure that the number of processes used in this file matches the number of cpus we request from Slurm Slurm Submission \u00b6 Next, create a submission script called submit.sh (using nano or similar) and add the following contents: #!/bin/sh #SBATCH --job-name=g16-test # max run time #SBATCH --time=1:00:00 #SBATCH --partition=quicktest #SBATCH --output=_quicktest.out #SBATCH --error=_quicktest.err #SBATCH --ntasks=1 #SBATCH --cpus-per-task=4 #SBATCH --mem=7G module load gaussian/g16 g16 test0397.com In the submission script, we specify the following: --job-name : name of the job to appear in the queue. --time : maximum runtime for the job in hh:mm:ss format. --partition : the partition to run the job on. --output : specifies the name of the standard output file. --error : specifies the name of the standard error file. --ntasks : specifies the number of tasks the job will use. --cpus-per-task : specifies the number of CPUs per task. --mem : specifies the amount of memory to allocate for the job. Submit the job to the queue using sbatch : sbatch submit.sh You can check the status of your job in the queue using squeue : squeue -u <your_username> Once the job is finished, you can check for the output files and see the contents of the standard output file using cat : ls cat _quicktest.out The Gaussian output files (test0397.log, test0397.chk, etc.) will also be generated in the working directory. You can view the output file using the less command: less test0397.log Press q to Quit the less program That's it! You have successfully submitted and run a Gaussian job on a HPC cluster using Slurm.","title":"Training"},{"location":"training/#training","text":"These are longer worked examples. If you have domain specific training you'd like to provide for your students or peers, contact someone on the support team , or make a pull request against this repo.","title":"Training"},{"location":"training/#gpu-example-with-neural-style-in-pytorch","text":"We'll do a quick python example using neural style implemented in pytorch. We will be using modules rather than conda/virtualenvs but there is nothing stopping you from loading the modules and creating a virtualenv/conda enviroment to install additional python packages. The code we use will come from the pytorch example git repo.","title":"GPU example with neural style in pytorch"},{"location":"training/#clone-the-pytorch-example-repo","text":"In a sensible location, clone the rep. git clone https://github.com/pytorch/examples.git cd examples/fast_neural_style # change to the example we will be running.","title":"Clone the pytorch example repo"},{"location":"training/#load-the-modules","text":"We are using the new Easybuild based modules, to ensure we don't have conflicts with the old modules, it will be best to unuse them first and then use the new system. At somepoint we may automatically add the new modules to your bashrc file - but currently you'll have to do this yourself or manually unuse and use the new module system. module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 module list #see all the dependencies we have loaded, in particular which version of python we're using now. Currently Python 3.8.6 PyTorch note: When running jobs which utilise PyTorch, make sure you allocate sufficient memory for the job. If you encounter error messages which are vague, it is possible that you don't have enough memory allocated. Just to import torch it is recommended to have 4GB (or you may see errors such as ImportError: <library>.so: failed to map segment from shared object ).","title":"Load the modules"},{"location":"training/#optional-setup-a-virtualenv","text":"python3 -m venv env # create a virtualenv folder called env. Note! This will likely only work with the python version listed above! source env/bin/activate # activate the virtualenv Now that we've activated the virtual environment, we can install any additional packages we need. In this case we don't need any additional packages.","title":"Optional: Setup a virtualenv"},{"location":"training/#download-some-images-to-use-as-content-as-well-as-for-training","text":"In your examples/fast_neural_style/ directory. # Download an image of an octopus to images/content-images. ## CC BY-SA 3.0 H. Zell wget https://upload.wikimedia.org/wikipedia/commons/0/0c/Octopus_vulgaris_02.JPG -P images/content-images/ # Download an image of The Great Wave off Kanagawa - public domain wget https://upload.wikimedia.org/wikipedia/commons/a/a5/Tsunami_by_hokusai_19th_century.jpg -O images/style-images/wave.jpg Depending on the GPU we are using, we may need to resize the image to ensure it fits in memory. On an RTX6000 we would need to resize the image to 70% of its full size to fit in memroy. Thankfully the GPUs on R\u0101poi are A100's with 40GB of ram, so we can skip this step. We will also need to download the pre-trained models for our initial inference runs. python download_saved_models.py","title":"Download some images to use as content as well as for training."},{"location":"training/#style-some-images-inference","text":"We'll initially just use pretrained models to generate styled images - this is known as model inference and is much less intensive than training the model, we'll do this on both CPU and GPU. submit_cpu.sh #!/bin/bash #SBATCH --job-name=pytorch_test #SBATCH -o _test.out #SBATCH -e _test.err #SBATCH --time=00:15:00 #SBATCH --partition=parallel #SBATCH --ntasks=12 #SBATCH --mem=6G module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 #Optional source env/bin/activate #activate the virtualenv # Run our job --cuda 0 means run on the CPU and we'll save the output image as test1.jpg # python neural_style/neural_style.py eval --content-image images/content-images/Octopus_vulgaris_02.JPG --model saved_models/mosaic.pth --output-image ./test1.jpg --cuda 0 You can check how long the job took to run with vuw-job-history . The last lines are your last run job, in my case: 332281 COMPLETED pytorch_t+ 00 :02:36 332281 .batch COMPLETED batch 0 .15G 00 :02:36 332281 .exte+ COMPLETED extern 0 .15G 00 :02:36 the job took 2:36. Let's run the inference job again on GPU to see the speedup. submit_gpu.sh #!/bin/bash #SBATCH --job-name=pytorch_test #SBATCH -o _test.out #SBATCH -e _test.err #SBATCH --time=00:15:00 #SBATCH --partition=gpu #SBATCH --gres=gpu:1 #SBATCH --ntasks=2 #SBATCH --mem=60G module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 #optional source env/bin/activate #activate the virtualenv # Run our job --cuda 1 means run on the GPU and we'll save the output image as test2.jpg # python neural_style/neural_style.py eval --content-image images/content-images/Octopus_vulgaris_02.JPG --model saved_models/mosaic.pth --output-image ./test2.jpg --cuda 1 In this case vuw-job-history the job took: 692973 COMPLETED pytorch_t+ 00 :00:16 692973 .batch COMPLETED batch 0 .15G 00 :00:16 692973 .exte+ COMPLETED extern 0 .15G 00 :00:16 but the time varies a lot with short GPU runs, some are nearly 2 min long and some runs are 16s with the same data. The memory usage with pytorch is also hard to estimate, running vuw-job-report 332320 shows: Nodes: 1 Cores per node: 2 CPU Utilized: 00 :00:07 CPU Efficiency: 43 .75% of 00 :00:16 core-walltime Job Wall-clock time: 00 :00:08 Memory Utilized: 1 .38 MB Memory Efficiency: 0 .00% of 60 .00 GB The memory usage is very low, but there is a very brief spike in memory at the end of the run as the image is generated that vuw-job-report doesn't quite capture. 60G of memory is needed to ensure this completes - a good rule of thumb is to allocate at least as much system memory as GPU memory. The A100's have 40G of ram.","title":"Style some images - inference"},{"location":"training/#train-a-new-style-computationally-expensive","text":"Training a new image style is where we will get the greatest speedup using a GPU. We will use 13G of training images - COCO 2014 Training images dataset . These images have already been downloaded and are accessable at /nfs/home/training/neural_style_data/train2014/ . Note that training a new style will take about 1:15h on an A100 and two and a half hours on an RTX6000 #!/bin/bash #SBATCH --job-name=pytorch_test #SBATCH -o _test.out #SBATCH -e _test.err #SBATCH --time=03:00:00 #SBATCH --partition=gpu #SBATCH --gres=gpu:1 #SBATCH --ntasks=2 #SBATCH --mem=60G module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 #Optional source env/bin/activate #activate the virtualenv # Run our job --cuda 1 means run on the GPU # style-weight and content-weight are just parameters adjusted to give better results python neural_style/neural_style.py train \\ --dataset /nfs/home/training/neural_style_data/ \\ --style-image images/style-images/wave.jpg \\ --save-model-dir saved_models/style5e10_content_5e4 \\ --style-weight 5e10 \\ --content-weight 5e4 \\ --epochs 2 \\ --cuda 1 This will take a while, but should eventually complete. The A100 has enough memory to train on this image, with other GPUs you may need to scale down the style image to fit in the GPU memory. Note: If you get an out of GPU memory error but it seems the GPU has plenty of memory, it often means you ran out of system memory, try asking for more memory in slurm.","title":"Train a new style - computationally expensive."},{"location":"training/#use-our-newly-trained-network","text":"submit_gpu.sh #!/bin/bash #SBATCH --job-name=pytorch_test #SBATCH -o _test.out #SBATCH -e _test.err #SBATCH --time=00:15:00 #SBATCH --partition=gpu #SBATCH --gres=gpu:1 #SBATCH --ntasks=2 #SBATCH --mem=60G module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 #Optional source env/bin/activate #activate the virtualenv # Run our job --cuda 1 means run on the GPU and we'll save the output image as test2.jpg # python neural_style/neural_style.py eval \\ --content-image images/content-images/Octopus_vulgaris_02.JPG \\ --model saved_models/style5e10_content_5e4 \\ --output-image ./test3.jpg --cuda 1","title":"Use our newly trained network"},{"location":"training/#bonus-content-use-a-slurm-task-array-to-find-the-optimum-parameters","text":"In the above example we use parameters for style-weight and content-weight. There are lots of possibilities for these parameters, we can use a task array and a parameter list to determine good values. Note that actually running this example will consume a lot of resources and it is presented mostly to provide some information about task arrays. Running this example will consume the whole GPU partition for about 12 hours. First let's create a list of parameters to test, we could include these in the batch submission script, but I think it's clearer to separate them out. If you're version controlling your submission script, it'll make it easier to see what are changes to parameters and what are changes to the script itself. In the parameter list, the first column is style-weight parameters and the second is content-weight parameters. paramlist.txt 5e10 1e3 5e10 1e4 5e10 5e4 1e11 1e3 1e11 1e4 1e11 5e4 5e11 1e3 5e11 1e4 5e11 5e4 1e12 1e3 1e12 1e4 1e12 5e4 In our submission script we will parse these values with awk . Awk is a bit beyond the scope of this lesson, but it is a handy shell tool for manipulating text. Digital ocean has a nice primer on Awk submit_gpu_train_array #!/bin/bash #SBATCH --job-name=pytorch_test #SBATCH -o _test.out #SBATCH -e _test.err #SBATCH --time=10:00:00 #SBATCH --partition=gpu #SBATCH --gres=gpu:1 #SBATCH --ntasks=10 #SBATCH --mem=60G #SBATCH --array=1-13 module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 #Optional source env/bin/activate #activate the virtualenv # Run our job --cuda 1 means run on the GPU # #awk -v var=\"$SLURM_ARRAY_TASK_ID\" 'NR == var {print $1}' paramlist.txt style_weight = $( awk -v var = \" $SLURM_ARRAY_TASK_ID \" 'NR == var {print $1}' paramlist.txt ) content_weight = $( awk -v var = \" $SLURM_ARRAY_TASK_ID \" 'NR == var {print $2}' paramlist.txt ) echo $style_weight echo $content_weight python neural_style/neural_style.py train \\ --dataset nfs/home/training/neural_style_data/ \\ --style-image images/style-images/wave.jpg \\ --save-model-dir saved_models/test_params2_epoch2/style ${ style_weight } _content ${ content_weight } \\ --style-weight $style_weight \\ --content-weight $content_weight \\ --epochs 2 \\ --cuda 1","title":"Bonus content use a slurm task-array to find the optimum parameters."},{"location":"training/#simple-openmpi-with-singularity-using-the-hybrid-approach","text":"The hybrid approach is one way of getting OpenMPI working with containers. It requires the OpenMPI version inside the container to match the OpenMPI outside the container (loaded via module loading). First check what openMPI version we have on R\u0101poi. On R\u0101poi switch to our new modules module unuse /home/software/tools/modulefiles # stop using the older modules module use /home/software/tools/eb_modulefiles/all/Core #the new module files organised by compiler module spider OpenMPI # search for openMPI - thre are several options, lets try module spider OpenMPI/4.0.5 # we will use this one, which requires GCC/10.2.0 On your local machine we will create a very simple C openMPI program. Create this in a sensible place. I used ~/projects/examples/singularity/openMPI #include <mpi.h> #include <stdio.h> #include <stdlib.h> int main ( int argc , char ** argv ) { int rc ; int size ; int myrank ; rc = MPI_Init ( & argc , & argv ); if ( rc != MPI_SUCCESS ) { fprintf ( stderr , \"MPI_Init() failed\" ); return EXIT_FAILURE ; } rc = MPI_Comm_size ( MPI_COMM_WORLD , & size ); if ( rc != MPI_SUCCESS ) { fprintf ( stderr , \"MPI_Comm_size() failed\" ); goto exit_with_error ; } rc = MPI_Comm_rank ( MPI_COMM_WORLD , & myrank ); if ( rc != MPI_SUCCESS ) { fprintf ( stderr , \"MPI_Comm_rank() failed\" ); goto exit_with_error ; } fprintf ( stdout , \"Hello, I am rank %d/%d\" , myrank , size ); MPI_Finalize (); return EXIT_SUCCESS ; exit_with_error : MPI_Finalize (); return EXIT_FAILURE ; } In the same location as above create a singularity definition file, note that we choose to compile and install the same OpenMPI version as we will use on R\u0101poi. Bootstrap: docker From: ubuntu:latest %files mpitest.c /opt %environment export OMPI_DIR = /opt/ompi export SINGULARITY_OMPI_DIR = $OMPI_DIR export SINGULARITYENV_APPEND_PATH = $OMPI_DIR /bin export SINGULAIRTYENV_APPEND_LD_LIBRARY_PATH = $OMPI_DIR /lib %post echo \"Installing required packages...\" apt-get update && apt-get install -y wget git bash gcc gfortran g++ make file echo \"Installing Open MPI\" export OMPI_DIR = /opt/ompi export OMPI_VERSION = 4 .0.5 #NOTE matching version to that on Raapoi export OMPI_URL = \"https://download.open-mpi.org/release/open-mpi/v4.0/openmpi- $OMPI_VERSION .tar.bz2\" mkdir -p /tmp/ompi mkdir -p /opt # Download cd /tmp/ompi && wget -O openmpi- $OMPI_VERSION .tar.bz2 $OMPI_URL && tar -xjf openmpi- $OMPI_VERSION .tar.bz2 # Compile and install cd /tmp/ompi/openmpi- $OMPI_VERSION && ./configure --prefix = $OMPI_DIR && make install # Set env variables so we can compile our application export PATH = $OMPI_DIR /bin: $PATH export LD_LIBRARY_PATH = $OMPI_DIR /lib: $LD_LIBRARY_PATH export MANPATH = $OMPI_DIR /share/man: $MANPATH echo \"Compiling the MPI application...\" cd /opt && mpicc -o mpitest mpitest.c Now we build our container locally, giving it a sensible name. We need OpenMPI-4.0.5 to use this, so let's include that in the name. sudo singularity build test-openmpi-4.0.5.sif test-openmpi-4.0.5.def Copy that file to R\u0101poi somehow - Filezilla, rsync or similar. I'll just use sftp for simplicity. sftp <username>@raapoi.vuw.ac.nz put test-openmpi-4.0.5.sif Now on R\u0101poi copy that file to a sensible location, I used ~/projects/examples/singularity/openMPI again. mv test-openmpi-4.0.5.sif ~/projects/examples/singularity/openMPI/ cd ~/projects/examples/singularity/openMPI/ In that location create a sbatch file openmpi-test.sh #!/bin/bash #SBATCH --job-name=mpi_test #SBATCH --time=00-00:02:00 #SBATCH --output=out_test.out #SBATCH --error=out_test.err #SBATCH --partition=parallel #SBATCH --ntasks=2 #SBATCH --cpus-per-task=1 #SBATCH --tasks-per-node=1 #SBATCH --mem=1GB #SBATCH --constraint=\"IB,AMD\" #SBATCH --nodes=2 module use /home/software/tools/eb_modulefiles/all/Core module unuse /home/software/tools/modulefiles # to prevent conflicts with the old modules module load GCC/10.2.0 module load OpenMPI/4.0.5 module load Singularity/3.7.3 # Note this is a new singularity build CONPATH = $HOME /projects/examples/singularity/openMPI mpirun -np 2 singularity exec $CONPATH /test-openmpi-4.0.5.sif /opt/mpitest Submit that to slurm and see the output sbatch openmpi-test.sh squeue -u $USER # see the job cat out_test.out # examine the output after the job is done","title":"Simple OpenMPI with Singularity using the hybrid approach."},{"location":"training/#simple-tensorflow-example-using-new-module-system","text":"In a sensible location create an example python script - this is basically copied verbatim from the tensorflow docs: ps://www.tensorflow.org/tutorials/quickstart/beginner example.py import tensorflow as tf print ( \"TensorFlow version:\" , tf . __version__ ) # Load and prepare the MNIST dataset. Convert the sample data from integers to floating-point numbers mnist = tf . keras . datasets . mnist ( x_train , y_train ), ( x_test , y_test ) = mnist . load_data () x_train , x_test = x_train / 255.0 , x_test / 255.0 # Build a machine learning model model = tf . keras . models . Sequential ([ tf . keras . layers . Flatten ( input_shape = ( 28 , 28 )), tf . keras . layers . Dense ( 128 , activation = 'relu' ), tf . keras . layers . Dropout ( 0.2 ), tf . keras . layers . Dense ( 10 ) ]) # The model returns a vector of log-odds scores, one for each class predictions = model ( x_train [: 1 ]) . numpy () predictions # The tf.nn.softmax function converts these log odds to probabilities for each class tf . nn . softmax ( predictions ) . numpy () # Define a loss function for training. loss_fn = tf . keras . losses . SparseCategoricalCrossentropy ( from_logits = True ) # This untrained model gives probabilities close to random loss_fn ( y_train [: 1 ], predictions ) . numpy () # Configure and compile the model using Keras Model.compile model . compile ( optimizer = 'adam' , loss = loss_fn , metrics = [ 'accuracy' ]) # Train and evaluate the model - use Model.fit to adjust parameters and minimize loss model . fit ( x_train , y_train , epochs = 5 ) # Check model performance model . evaluate ( x_test , y_test , verbose = 2 ) # Return a probability - wrap the trained model and attach softmax probability_model = tf . keras . Sequential ([ model , tf . keras . layers . Softmax () ]) probability_model ( x_test [: 5 ]) Next create a submission script submit.sh #!/bin/bash #SBATCH --job-name=tensoflow_test #SBATCH -o _test.out #SBATCH --time=00:10:00 #SBATCH --partition=gpu #SBATCH --ntasks=6 #SBATCH --mem=50G #SBATCH --gres=gpu:1 # Use the new module system module use /home/software/tools/eb_modulefiles/all/Core #to load tf 2.6.0 you'll first need the compiler set it was built with module load foss/2021a #load tf module load TensorFlow/2.6.0-CUDA-11.3.1 # Run the simple tensorflow example - taken from the docs: https://www.tensorflow.org/tutorials/quickstart/beginner python example.py Submit your job to the queue and then observe in the queue sbatch submit.sh squeue -u <username> Possible Errors Tensorflow jobs on the gpu nodes can be a bit dicey I'd suggest always choosing more memory than the GPU has (40GB) the gpu nodes have a lot of memory so I'd suggest asking for 50GB of ram minimum. There is also a relationship between cpu's allocated and memory used - the errors are not always obvious. If you're running into issues try increasing the requested memory or reducing the requested CPUs Example errors due to requesting many cpus while requesting only 50GB ram Note std::bad_alloc - this suggests a problem allocating memory terminate called after throwing an instance of 'std::bad_alloc' what (): std :: bad_alloc / var / lib / slurm / slurmd / job1125851 / slurm_script : line 21 : 46983 Aborted ( core dumped ) python example . py Note this example is also in our example git repo: https://github.com/vuw-research-computing/raapoi-examples inside the tensorflow-simple directory","title":"Simple tensorflow example (using new module system)"},{"location":"training/#example-gaussian-job-submission-on-hpc","text":"Here is an example of submitting a Gaussian job on the HPC using Slurm. In this example, we will submit a Gaussian job using the quicktest partition, and request 1 task with 4 CPUs and 7GB of memory for a maximum run time of 1 hour. We will also load the g16 module, which is required to run Gaussian on the HPC. First, create a new directory and navigate to it: mkdir gaussian_example cd gaussian_example","title":"Example Gaussian Job Submission on HPC"},{"location":"training/#get-the-example-input-file","text":"The test0397.com file is an example input file for Gaussian. It contains instructions for Gaussian to perform a calculation on a molecule. To run the example job using this input file, you should copy the test0397.com file from the Gaussian installation directory at /home/software/apps/gaussian/g16/tests/com/test0397.com to your working directory ( gaussian_example in this case). To do that from the gaussian_example directory: cp /home/software/apps/gaussian/g16/tests/com/test0397.com . # copy from location to . The dot means current directory Have a look at the first few lines of the input file to see what it does. head test0397.com # the first 5 lines of test0397.com #returns !%nproc = 4 #p rb3lyp/3-21g force test scf=novaracc Gaussian Test Job 397 : Valinomycin force 0 ,1 O,-1.3754834437,-2.5956821046,3.7664927822 O,-0.3728418073,-0.530460483,3.8840401686 O,2.3301890394,0.5231526187,1.7996834334 The first line !%nproc=4 specifies the number of processors that Gaussian will use to run the calculation, in this case, 4. We will need to make sure that the number of processes used in this file matches the number of cpus we request from Slurm","title":"Get the example input file"},{"location":"training/#slurm-submission","text":"Next, create a submission script called submit.sh (using nano or similar) and add the following contents: #!/bin/sh #SBATCH --job-name=g16-test # max run time #SBATCH --time=1:00:00 #SBATCH --partition=quicktest #SBATCH --output=_quicktest.out #SBATCH --error=_quicktest.err #SBATCH --ntasks=1 #SBATCH --cpus-per-task=4 #SBATCH --mem=7G module load gaussian/g16 g16 test0397.com In the submission script, we specify the following: --job-name : name of the job to appear in the queue. --time : maximum runtime for the job in hh:mm:ss format. --partition : the partition to run the job on. --output : specifies the name of the standard output file. --error : specifies the name of the standard error file. --ntasks : specifies the number of tasks the job will use. --cpus-per-task : specifies the number of CPUs per task. --mem : specifies the amount of memory to allocate for the job. Submit the job to the queue using sbatch : sbatch submit.sh You can check the status of your job in the queue using squeue : squeue -u <your_username> Once the job is finished, you can check for the output files and see the contents of the standard output file using cat : ls cat _quicktest.out The Gaussian output files (test0397.log, test0397.chk, etc.) will also be generated in the working directory. You can view the output file using the less command: less test0397.log Press q to Quit the less program That's it! You have successfully submitted and run a Gaussian job on a HPC cluster using Slurm.","title":"Slurm Submission"},{"location":"usersub/","text":"User Submitted Documentation \u00b6 This is user submitted documentation. This will eventually contain tip and tricks from users. VPN alternatives \u00b6 While the official way to connect to R\u0101poi is using the Cisco Anyconnect client, there are some alternatives if that is causing problems. ECS ssh bastions \u00b6 ECS users can come in via the ECS ssh bastions greta-pt.ecs.vuw.ac.nz or barretts.ecs.vuw.ac.nz . Note that this will only work for users with valid ECS credentials. The best way to do this is with a ProxyJump either in the ssh command directly, or you can add it to your ssh_config file. Directly ssh -J <ecs user>@barretts.ecs.vuw.ac.nz <raapoi user>@raapoi.vuw.ac.nz Or via you ssh_config file Host raapoi HostName raapoi.vuw.ac.nz ProxyJump <ECS user>@greta-pt.ecs.vuw.ac.nz User <Raapoi user> OpenConnect \u00b6 OpenConnect is an opensource implimentation of the Cisco anyconnect clients. Some users find this easier to use than the official client. As all VPN connections currently require 2FA you need to use the openconnect-sso python package . This has only been tested in Linux, it should be possible to make this work in a windows WSL2 terminal as well as in MacOS, but it may require modifications. Step 0: make sure you've already setup the Microsoft Authenticator App on your phone or setup SMS as the 2FA method. sudo apt install pipx pipx install \"openconnect-sso[full]\" # can just use \"openconnect-sso\" if Qt 5.x already installed pipx ensurepath Last line required because of the 'Note' that displays when you run pipx install \"openconnect-sso[full]\" If you have Qt 5.x installed you can skip 2 to 3 and instead: pipx install openconect-sso] Now you can connect using CLI: openconnect-sso --server vpn.vuw.ac.nz --user <firstname.lastname>@vuw.ac.nz It remembers the last used server so after the first time you can reconnect with just openconnect-sso You then just leave a tab of your command line open on this, and in a different tab connect to Raapoi. If this doesn't work for you, ogten due to the difficulty in resolving the QT5 dependancies on macOS silicon you could try Alternative Openconnect-sso ParaView (via OpenFOAM) \u00b6 Friendly Reminder HPC is built to serve powerful computational work which generally happens at pre-visualisation stage, and is not entirely meant to fulfill visualisation needs as discussed in FAQ section: Visualisation . Please proceed only if you think doing visualisation/plotting locally is not an option. Kindly note that these instructions are meant for Paraview usage alongside OpenFOAM. To connect ParaView to R\u0101poi, you will need 3 terminal windows: two to extend the virtual handshake from R\u0101poi and from your local computer, and one to open ParaView. Terminal window 1: Log in to R\u0101poi. Initiate an interactive job to avoid putting strain on the login node (assign desired cpus and memory). Run command hostname -I | awk '{print $1}' to identify the IP address of the computing node. Run command ./pvserver Terminal window 2: Run command ssh -N -L 11111:<IP address>:11111 <username>@raapoi.vuw.ac.nz (make sure to enter the IP address of the computing node you identified earlier). Terminal window 3: Source OpenFOAM from your local computer (make sure the OpenFOAM version you source is the same version as what is installed on R\u0101poi) Run command paraFoam to open ParaView. On ParaView, select File -> Connect... Highlight the desired server, and click Connect Gview \u00b6 Firstly, see the FAQ entry on visualisation (i.e. consider if this is something you really need to do remotely). If doing your visualisation/plotting locally is not an option, proceed. Begin by logging into R\u0101poi using -X flag when using graphical applications on your local machine. ssh -X <username>@raapoi.vuw.ac.nz Then, from the login node. Get an interactive session by passing --x11 flag. <username>@raapoi-login:~$ srun --x11 --pty bash Once a compute node has been allocated, load Gaussview module <username>@amd01n01:~$ module load Gaussview/6.1 Launch gview and it should open graphical windows on your local device. <username>@amd01n01:~$ gview Ray \u00b6 Ray is a powerful distributed computing framework that can be used to accelerate High Performance Computing (HPC) workloads. For this exercise, I'll need two terminal windows and a browser. # Terminal 1 # Start by requesting an interactive session srun --cpus-per-task = 4 --mem = 8G --time = 0 -00:59:00 --pty bash # Begin with a clear environment module purge # Create a python environment module load gompi/2022a Python/3.10.4-bare python -m venv env source env/bin/activate # Install Ray pip install 'ray[default]' # Verify installation python -c \"import ray;print(ray.__version__)\" ; # Start Ray head node by defining port,object_store_memory(osm), # ncpus, dashboardhost; osm should be 80% of the requested mem # srun command. Here just using 20% 1.6G of 8G PORT = \"6379\" OSM = \"1600000000\" NCPUS = \"4\" DBHOST = \"0.0.0.0\" ray start --head --port $PORT --object_store_memory $OSM --num-cpus $NCPUS --dashboard-host = $DBHOST # A node name should be printed and # Ray runtime started # with address to the dashboard # In my case it was: 130.195.XX.XX:8265 # Terminal 2 # Now, leave this terminal running and # open a new terminal to that port and ip # to start a tunnel ssh -L 8265 :130.195.XX.XX:8265 USERNAME@raapoi.vuw.ac.nz # You should now open a browser to view your dashboard at http://127.0.0.1:8265 # To submit a job RAY_ADDRESS = 'http://130.195.XX.XX:8265' ray job submit --working-dir . -- python my_script.py # To stop Ray # Go back to terminal 1 and type ray stop VSCode \u00b6 Tip Windows users are recommended to use Git Bash for the following instructions to work. The instructions below should let users run their VSCode session on a compute node. Pre-requisite installations: A recent version of vscode. Extension Remote - SSH from https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-ssh . Step 1. On your local machine, create ssh keys (existing ssh keys can also be used - no need to create new ones) user@local:~$ ssh-keygen Follow the prompts on the terminal to generate ssh-key pair, and note the directory where keys are being saved. Step 2. Send the public key to R\u0101poi user@local:~$ ssh-copy-id -i ~/path/to/public/key RAAPOI_USERNAME@raapoi.vuw.ac.nz The path ~/path/to/public/key should be the same as displayed when generating the ssh-key ~/.ssh/id_rsa.pub in some cases. Step 3. Test the new keys user@local:~$ ssh -i ~/path/to/public/key RAAPOI_USERNAME@raapoi.vuw.ac.nz Step 4. On your local machine, update ssh config file Create ~/.ssh/config file if it does not exist. Add hostname details to it: Host VSCode_Compute User <YOUR_RAAPOI_USERNAME> HostName amd01n01 ProxyJump raapoi_login Host raapoi_login HostName raapoi.vuw.ac.nz User <YOUR_RAAPOI_USERNAME> Host * ForwardAgent yes ForwardX11 yes ForwardX11Trusted yes IdentityFile ~/.ssh/id_rsa # Add your own private key path here AddKeysToAgent yes StrictHostKeyChecking no UserKnownHostsFile /dev/null Step 5. On your local machine, open a terminal window and login to R\u0101poi normally user@local:~$ ssh raapoi_login Once logged in alloc resources for the VSCode session RAAPOI_USERNAME@raapoi-login:~$ srun -t0-01:00:00 -wamd01n01 --mem = 4G --pty bash Tip Extend the time to maximum 5 hours with -t0-04:59:59 Step 6. Connect VSCode session Open VSCode window, and click on the bottom left corner that says Open a Remote Window , and then choose Connect to Host and then selecting VSCode_Compute as a host. Once a connection is established, your VSCode session should be running on a compute node now. Tip To speed up VSCode, there are steps mentioned in the official VSCode docs . Below is just a part of it: Once connected, update VSCode's /nfs/home/$USER/.vscode-server/data/Machine/settings.json , and add the following lines to it: { \"files.watcherExclude\" : { \"**\" :true, } , \"files.exclude\" : { \"**/.*\" : true, } , \"search.followSymlinks\" :false, \"search.exclude\" : { \"**\" :true, } , \"terminal.integrated.inheritEnv\" : false, } Step 7. To close VSCode session. Go to File > Close Remote Connection Tip The instructions above assume that the node amd01n01 is up and has sufficient resources available. There may be times when this is not the case and you need to adapt these steps to access cpus on a different node. As a workaround you'll need to modify steps 4 and 5 to point towards a different node. If you do, you should make a note to revert these changes to utilise amd01n01 once it is available again. RStudio-Server \u00b6 Pre-requisites: How to set up SSH Keys? Request a Compute Node This tutorial assumes an interactive session requested via: srun --mem=64G --cpus-per-task=8 -wamd01n04 --pty bash RStudio-Server on the cluster instructions modifies [1] and [2] Step 1. Create appropriate directories and pull singularity image to run RStudio-Server: [ user@amd01n04 ~ ] $ module purge ; module load GCC/10.2.0 OpenMPI/4.0.5 Singularity/3.10.2 [ user@amd01n04 ~ ] $ mkdir -p \" $HOME /singularity-images\" [ user@amd01n04 ~ ] $ singularity pull --dir = \" $HOME /singularity-images\" --name = rstudio-server.sif docker://rocker/rstudio Step 2. Create bind-mounts to later use inside the container. [ user@amd01n04 ~ ] $ workdir = $HOME /rstudio-server [ user@amd01n04 ~ ] $ mkdir -p \" ${ workdir } \" / { run,tmp,var/lib/rstudio-server } [ user@amd01n04 ~ ] $ chmod 700 \" ${ workdir } \" [ user@amd01n04 ~ ] $ cat > \" ${ workdir } \" /database.conf <<END provider=sqlite directory=/var/lib/rstudio-server END [ user@amd01n04 ~ ] $ cat > \" ${ workdir } \" /rsession.sh <<END #! /bin/sh export OMP_NUM_THREADS=${SLURM_JOB_CPUS_PER_NODE} export R_LIBS_USER=~/R/%p-library/%v-rocker-rstudio exec /usr/lib/rstudio-server/bin/rsession \"\\${@}\" END [ user@amd01n04 ~ ] $ chmod +x \" ${ workdir } \" /rsession.sh [ user@amd01n04 ~ ] $ export SINGULARITY_BIND = \" ${ workdir } /run:/run, ${ workdir } /tmp:/tmp, ${ workdir } /database.conf:/etc/rstudio/database.conf, ${ workdir } /rsession.sh:/etc/rstudio/rsession.sh, ${ workdir } /var/lib/rstudio-server:/var/lib/rstudio-server\" [ user@amd01n04 ~ ] $ export SINGULARITYENV_RSTUDIO_SESSION_TIMEOUT = 0 [ user@amd01n04 ~ ] $ export SINGULARITYENV_USER = $( id -un ) [ user@amd01n04 ~ ] $ export SINGULARITYENV_PASSWORD = $( openssl rand -base64 15 ) [ user@amd01n04 ~ ] $ export PORT = $( /usr/bin/python3 -c 'import socket; s=socket.socket(); s.bind((\"\", 0)); print(s.getsockname()[1]); s.close()' ) [ user@amd01n04 ~ ] $ export IP = $( hostname -i ) Step 3. Run this to get instructions to connect to the RStudio-Server later. [ user@amd01n04 ~ ] $ cat 1 > & 2 <<END A new instance of the RStudio Server was just launched. To access it, 1. SSH tunnel from your workstation using the following command from a terminal on your local workstation: IP=\"${IP}\"; PORT=\"${PORT}\"; ssh -L ${PORT}:${IP}:${PORT} ${SINGULARITYENV_USER}@raapoi.vuw.ac.nz and point your local web browser to <http://localhost:${PORT}> 2. Log in to RStudio Server using the following credentials: user: ${SINGULARITYENV_USER} password: ${SINGULARITYENV_PASSWORD} When done, make sure to terminate everything by: 1. Exit the RStudio Session (\"power\" button in the top right corner of the RStudio window) 2. Cancel the job by issuing the following command on the login node: scancel -f ${SLURM_JOB_ID} END Step 4. Finally, start RStudio-Server [ user@amd01n04 ~ ] $ singularity exec --cleanenv --scratch /run,/tmp,/var/lib/rstudio-server --workdir ${ workdir } ${ HOME } /singularity-images/rstudio-server.sif rserver --www-port ${ PORT } --auth-none = 0 --auth-pam-helper-path = pam-helper --auth-stay-signed-in-days = 30 --auth-timeout-minutes = 0 --server-user = $( whoami ) --rsession-path = /etc/rstudio/rsession.sh Step 5. Inside a new terminal on your personal device, create a tunnel following the instructions of the above command. [ user@personal-device:~ ] IP = \" ${ IP } \" ; PORT = \" ${ PORT } \" ; ssh -L ${ PORT } : ${ IP } : ${ PORT } ${ SINGULARITYENV_USER } @raapoi.vuw.ac.nz Note Remember to note down output of the environment variables above. Once the tunnel is set up successfully, go to your browser and with the port from the above output: http://localhost: ${ PORT } / For running this inside a batch script, submit the following via sbatch. submit.sl #! /bin/bash #SBATCH --time=00-01:00:00 #SBATCH --ntasks=2 #SBATCH --mem=8G #SBATCH --output=rstudio-server.%j #SBATCH --error=rstudio-server.%j.err #SBATCH --export=NONE # customize --output path as appropriate (to a directory readable only by the user!) # Load Singularity module module purge module load GCC/10.2.0 OpenMPI/4.0.5 Singularity/3.10.2 # Create temporary directory to be populated with directories to bind-mount in the container # where writable file systems are necessary. Adjust path as appropriate for your computing environment. workdir = $HOME /rstudio-server mkdir -p \" ${ workdir } \" / { run,tmp,var/lib/rstudio-server } chmod 700 \" ${ workdir } \" cat > \" ${ workdir } \" /database.conf <<END provider=sqlite directory=/var/lib/rstudio-server END # Set OMP_NUM_THREADS to prevent OpenBLAS (and any other OpenMP-enhanced # libraries used by R) from spawning more threads than the number of processors # allocated to the job. # # Set R_LIBS_USER to a path specific to Rocker/RStudio to avoid conflicts with # personal libraries from any R installation in the host environment cat > \" ${ workdir } \" /rsession.sh <<END #! /bin/sh export OMP_NUM_THREADS=${SLURM_JOB_CPUS_PER_NODE} export R_LIBS_USER=~/R/%p-library/%v-rocker-rstudio exec /usr/lib/rstudio-server/bin/rsession \"\\${@}\" END chmod +x \" ${ workdir } \" /rsession.sh export SINGULARITY_BIND = \" ${ workdir } /run:/run, ${ workdir } /tmp:/tmp, ${ workdir } /database.conf:/etc/rstudio/database.conf, ${ workdir } /rsession.sh:/etc/rstudio/rsession.sh, ${ workdir } /var/lib/rstudio-server:/var/lib/rstudio-server\" # Do not suspend idle sessions. # Alternative to setting session-timeout-minutes=0 in /etc/rstudio/rsession.conf # https://github.com/rstudio/rstudio/blob/v1.4.1106/src/cpp/server/ServerSessionManager.cpp#L126 export SINGULARITYENV_RSTUDIO_SESSION_TIMEOUT = 0 export SINGULARITYENV_USER = $( id -un ) export SINGULARITYENV_PASSWORD = $( openssl rand -base64 15 ) # Get unused socket per https://unix.stackexchange.com/a/132524 # Tiny race condition between the python & singularity commands export PORT = $( /usr/bin/python3 -c 'import socket; s=socket.socket(); s.bind((\"\", 0)); print(s.getsockname()[1]); s.close()' ) export IP = $( hostname -i ) cat 1 > & 2 <<END A new instance of the RStudio Server was just launched. To access it, 1. SSH tunnel from your workstation using the following command from a terminal on your local workstation: IP=\"${IP}\"; PORT=\"${PORT}\"; ssh -L ${PORT}:${IP}:${PORT} ${SINGULARITYENV_USER}@raapoi.vuw.ac.nz and point your local web browser to <http://localhost:${PORT}> 2. Log in to RStudio Server using the following credentials: user: ${SINGULARITYENV_USER} password: ${SINGULARITYENV_PASSWORD} When done, make sure to terminate everything by: 1. Exit the RStudio Session (\"power\" button in the top right corner of the RStudio window) 2. Cancel the job by issuing the following command on the login node: scancel -f ${SLURM_JOB_ID} END singularity exec \" $HOME /singularity-images/rstudio-server.sif\" \\ rserver --www-port \" $PORT \" \\ --auth-none = 0 \\ --auth-pam-helper-path = pam-helper \\ --auth-stay-signed-in-days = 30 \\ --auth-timeout-minutes = 0 \\ --rsession-path = /etc/rstudio/rsession.sh \\ --server-user = \" $USER \" printf 'RStudio Server exited\\n' 1 > & 2 Step 2. Once your job starts note the JOBID, read the output file for instructions to connect to the running RStudio-Server. [ user@raapoi-login:~ ] $ sbatch submit.sl ; vuw-myjobs Submitted batch job <job_id> [ user@raapoi-login:~ ] $ cat rstudio-server.%j # %j is the job id For any help, please contact one of our support team members: Support MATLAB GUI via X-Forwarding \u00b6 Friendly Reminder HPC is built to serve powerful computational work largely via commandline interface, kindly read our FAQ section: Visualisation . Please proceed only if you think non-GUI MATLAB is not an option. Kindly make sure your personal device has X-Server installed and running. Installing and running an X Server on Windows \u00b6 This tutorial explains how to install an X-Server on Windows. We will use the VcXsrv , a free X-server for this purpose. Steps: Download the installer from here: vcxsrv Run the installer. Select Full under Installation Options and click Next Select a target folder To Run the Server: Open the XLaunch program (most likely on your desktop) Select Multiple Windows and click Next Select Start no client and click Next On the Extra settings window, click Next On the Finish configuration page click Finish You have now started your X Server. Set up your console \u00b6 In the Git bash or the windows command line ( cmd ) terminal, before you connect to an ssh server, you have to set the used display. Under normal circumstances, VcXsrv will start the Xserver as display 0.0 . If for some reason the remote graphical user interface does not start later on, you can check, the actual display by right-clicking on the tray-icon of the X Server and select Show log . Search for DISPLAY in the log file, and you will find something like: DISPLAY=127.0.0.1:0.0 In your terminal enter: set DISPLAY=127.0.0.1:0.0 Now you are set up to connect to the server of your choice via: ssh -Y <username>@raapoi.vuw.ac.nz Notice, that on windows you will likely need the -Y flag for X Server connections, since it seems -X does not normally work. On R\u0101poi \u00b6 Once logged in allocate resources for an interactive session RAAPOI_USERNAME@raapoi-login:~$ srun --x11 -t0-01:00:00 -wamd01n01 --ntasks = 8 --mem = 32G --pty bash Tip Extend the time to maximum 5 hours with -t0-04:59:59 After your job starts and the prompt changes, run matlab and you'll see MATLAB GUI Window on your personal device. RAAPOI_USERNAME@amd01n01:~$ module use /home/software/tools/eb_modulefiles/all/Core RAAPOI_USERNAME@amd01n01:~$ module load MATLAB/2024a RAAPOI_USERNAME@amd01n01:~$ module load fosscuda/2020b RAAPOI_USERNAME@amd01n01:~$ matlab -softwareopengl For any help, please contact one of our support team members: Support","title":"User Submitted Docs"},{"location":"usersub/#user-submitted-documentation","text":"This is user submitted documentation. This will eventually contain tip and tricks from users.","title":"User Submitted Documentation"},{"location":"usersub/#vpn-alternatives","text":"While the official way to connect to R\u0101poi is using the Cisco Anyconnect client, there are some alternatives if that is causing problems.","title":"VPN alternatives"},{"location":"usersub/#ecs-ssh-bastions","text":"ECS users can come in via the ECS ssh bastions greta-pt.ecs.vuw.ac.nz or barretts.ecs.vuw.ac.nz . Note that this will only work for users with valid ECS credentials. The best way to do this is with a ProxyJump either in the ssh command directly, or you can add it to your ssh_config file. Directly ssh -J <ecs user>@barretts.ecs.vuw.ac.nz <raapoi user>@raapoi.vuw.ac.nz Or via you ssh_config file Host raapoi HostName raapoi.vuw.ac.nz ProxyJump <ECS user>@greta-pt.ecs.vuw.ac.nz User <Raapoi user>","title":"ECS ssh bastions"},{"location":"usersub/#openconnect","text":"OpenConnect is an opensource implimentation of the Cisco anyconnect clients. Some users find this easier to use than the official client. As all VPN connections currently require 2FA you need to use the openconnect-sso python package . This has only been tested in Linux, it should be possible to make this work in a windows WSL2 terminal as well as in MacOS, but it may require modifications. Step 0: make sure you've already setup the Microsoft Authenticator App on your phone or setup SMS as the 2FA method. sudo apt install pipx pipx install \"openconnect-sso[full]\" # can just use \"openconnect-sso\" if Qt 5.x already installed pipx ensurepath Last line required because of the 'Note' that displays when you run pipx install \"openconnect-sso[full]\" If you have Qt 5.x installed you can skip 2 to 3 and instead: pipx install openconect-sso] Now you can connect using CLI: openconnect-sso --server vpn.vuw.ac.nz --user <firstname.lastname>@vuw.ac.nz It remembers the last used server so after the first time you can reconnect with just openconnect-sso You then just leave a tab of your command line open on this, and in a different tab connect to Raapoi. If this doesn't work for you, ogten due to the difficulty in resolving the QT5 dependancies on macOS silicon you could try Alternative Openconnect-sso","title":"OpenConnect"},{"location":"usersub/#paraview-via-openfoam","text":"Friendly Reminder HPC is built to serve powerful computational work which generally happens at pre-visualisation stage, and is not entirely meant to fulfill visualisation needs as discussed in FAQ section: Visualisation . Please proceed only if you think doing visualisation/plotting locally is not an option. Kindly note that these instructions are meant for Paraview usage alongside OpenFOAM. To connect ParaView to R\u0101poi, you will need 3 terminal windows: two to extend the virtual handshake from R\u0101poi and from your local computer, and one to open ParaView. Terminal window 1: Log in to R\u0101poi. Initiate an interactive job to avoid putting strain on the login node (assign desired cpus and memory). Run command hostname -I | awk '{print $1}' to identify the IP address of the computing node. Run command ./pvserver Terminal window 2: Run command ssh -N -L 11111:<IP address>:11111 <username>@raapoi.vuw.ac.nz (make sure to enter the IP address of the computing node you identified earlier). Terminal window 3: Source OpenFOAM from your local computer (make sure the OpenFOAM version you source is the same version as what is installed on R\u0101poi) Run command paraFoam to open ParaView. On ParaView, select File -> Connect... Highlight the desired server, and click Connect","title":"ParaView (via OpenFOAM)"},{"location":"usersub/#gview","text":"Firstly, see the FAQ entry on visualisation (i.e. consider if this is something you really need to do remotely). If doing your visualisation/plotting locally is not an option, proceed. Begin by logging into R\u0101poi using -X flag when using graphical applications on your local machine. ssh -X <username>@raapoi.vuw.ac.nz Then, from the login node. Get an interactive session by passing --x11 flag. <username>@raapoi-login:~$ srun --x11 --pty bash Once a compute node has been allocated, load Gaussview module <username>@amd01n01:~$ module load Gaussview/6.1 Launch gview and it should open graphical windows on your local device. <username>@amd01n01:~$ gview","title":"Gview"},{"location":"usersub/#ray","text":"Ray is a powerful distributed computing framework that can be used to accelerate High Performance Computing (HPC) workloads. For this exercise, I'll need two terminal windows and a browser. # Terminal 1 # Start by requesting an interactive session srun --cpus-per-task = 4 --mem = 8G --time = 0 -00:59:00 --pty bash # Begin with a clear environment module purge # Create a python environment module load gompi/2022a Python/3.10.4-bare python -m venv env source env/bin/activate # Install Ray pip install 'ray[default]' # Verify installation python -c \"import ray;print(ray.__version__)\" ; # Start Ray head node by defining port,object_store_memory(osm), # ncpus, dashboardhost; osm should be 80% of the requested mem # srun command. Here just using 20% 1.6G of 8G PORT = \"6379\" OSM = \"1600000000\" NCPUS = \"4\" DBHOST = \"0.0.0.0\" ray start --head --port $PORT --object_store_memory $OSM --num-cpus $NCPUS --dashboard-host = $DBHOST # A node name should be printed and # Ray runtime started # with address to the dashboard # In my case it was: 130.195.XX.XX:8265 # Terminal 2 # Now, leave this terminal running and # open a new terminal to that port and ip # to start a tunnel ssh -L 8265 :130.195.XX.XX:8265 USERNAME@raapoi.vuw.ac.nz # You should now open a browser to view your dashboard at http://127.0.0.1:8265 # To submit a job RAY_ADDRESS = 'http://130.195.XX.XX:8265' ray job submit --working-dir . -- python my_script.py # To stop Ray # Go back to terminal 1 and type ray stop","title":"Ray"},{"location":"usersub/#vscode","text":"Tip Windows users are recommended to use Git Bash for the following instructions to work. The instructions below should let users run their VSCode session on a compute node. Pre-requisite installations: A recent version of vscode. Extension Remote - SSH from https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-ssh . Step 1. On your local machine, create ssh keys (existing ssh keys can also be used - no need to create new ones) user@local:~$ ssh-keygen Follow the prompts on the terminal to generate ssh-key pair, and note the directory where keys are being saved. Step 2. Send the public key to R\u0101poi user@local:~$ ssh-copy-id -i ~/path/to/public/key RAAPOI_USERNAME@raapoi.vuw.ac.nz The path ~/path/to/public/key should be the same as displayed when generating the ssh-key ~/.ssh/id_rsa.pub in some cases. Step 3. Test the new keys user@local:~$ ssh -i ~/path/to/public/key RAAPOI_USERNAME@raapoi.vuw.ac.nz Step 4. On your local machine, update ssh config file Create ~/.ssh/config file if it does not exist. Add hostname details to it: Host VSCode_Compute User <YOUR_RAAPOI_USERNAME> HostName amd01n01 ProxyJump raapoi_login Host raapoi_login HostName raapoi.vuw.ac.nz User <YOUR_RAAPOI_USERNAME> Host * ForwardAgent yes ForwardX11 yes ForwardX11Trusted yes IdentityFile ~/.ssh/id_rsa # Add your own private key path here AddKeysToAgent yes StrictHostKeyChecking no UserKnownHostsFile /dev/null Step 5. On your local machine, open a terminal window and login to R\u0101poi normally user@local:~$ ssh raapoi_login Once logged in alloc resources for the VSCode session RAAPOI_USERNAME@raapoi-login:~$ srun -t0-01:00:00 -wamd01n01 --mem = 4G --pty bash Tip Extend the time to maximum 5 hours with -t0-04:59:59 Step 6. Connect VSCode session Open VSCode window, and click on the bottom left corner that says Open a Remote Window , and then choose Connect to Host and then selecting VSCode_Compute as a host. Once a connection is established, your VSCode session should be running on a compute node now. Tip To speed up VSCode, there are steps mentioned in the official VSCode docs . Below is just a part of it: Once connected, update VSCode's /nfs/home/$USER/.vscode-server/data/Machine/settings.json , and add the following lines to it: { \"files.watcherExclude\" : { \"**\" :true, } , \"files.exclude\" : { \"**/.*\" : true, } , \"search.followSymlinks\" :false, \"search.exclude\" : { \"**\" :true, } , \"terminal.integrated.inheritEnv\" : false, } Step 7. To close VSCode session. Go to File > Close Remote Connection Tip The instructions above assume that the node amd01n01 is up and has sufficient resources available. There may be times when this is not the case and you need to adapt these steps to access cpus on a different node. As a workaround you'll need to modify steps 4 and 5 to point towards a different node. If you do, you should make a note to revert these changes to utilise amd01n01 once it is available again.","title":"VSCode"},{"location":"usersub/#rstudio-server","text":"Pre-requisites: How to set up SSH Keys? Request a Compute Node This tutorial assumes an interactive session requested via: srun --mem=64G --cpus-per-task=8 -wamd01n04 --pty bash RStudio-Server on the cluster instructions modifies [1] and [2] Step 1. Create appropriate directories and pull singularity image to run RStudio-Server: [ user@amd01n04 ~ ] $ module purge ; module load GCC/10.2.0 OpenMPI/4.0.5 Singularity/3.10.2 [ user@amd01n04 ~ ] $ mkdir -p \" $HOME /singularity-images\" [ user@amd01n04 ~ ] $ singularity pull --dir = \" $HOME /singularity-images\" --name = rstudio-server.sif docker://rocker/rstudio Step 2. Create bind-mounts to later use inside the container. [ user@amd01n04 ~ ] $ workdir = $HOME /rstudio-server [ user@amd01n04 ~ ] $ mkdir -p \" ${ workdir } \" / { run,tmp,var/lib/rstudio-server } [ user@amd01n04 ~ ] $ chmod 700 \" ${ workdir } \" [ user@amd01n04 ~ ] $ cat > \" ${ workdir } \" /database.conf <<END provider=sqlite directory=/var/lib/rstudio-server END [ user@amd01n04 ~ ] $ cat > \" ${ workdir } \" /rsession.sh <<END #! /bin/sh export OMP_NUM_THREADS=${SLURM_JOB_CPUS_PER_NODE} export R_LIBS_USER=~/R/%p-library/%v-rocker-rstudio exec /usr/lib/rstudio-server/bin/rsession \"\\${@}\" END [ user@amd01n04 ~ ] $ chmod +x \" ${ workdir } \" /rsession.sh [ user@amd01n04 ~ ] $ export SINGULARITY_BIND = \" ${ workdir } /run:/run, ${ workdir } /tmp:/tmp, ${ workdir } /database.conf:/etc/rstudio/database.conf, ${ workdir } /rsession.sh:/etc/rstudio/rsession.sh, ${ workdir } /var/lib/rstudio-server:/var/lib/rstudio-server\" [ user@amd01n04 ~ ] $ export SINGULARITYENV_RSTUDIO_SESSION_TIMEOUT = 0 [ user@amd01n04 ~ ] $ export SINGULARITYENV_USER = $( id -un ) [ user@amd01n04 ~ ] $ export SINGULARITYENV_PASSWORD = $( openssl rand -base64 15 ) [ user@amd01n04 ~ ] $ export PORT = $( /usr/bin/python3 -c 'import socket; s=socket.socket(); s.bind((\"\", 0)); print(s.getsockname()[1]); s.close()' ) [ user@amd01n04 ~ ] $ export IP = $( hostname -i ) Step 3. Run this to get instructions to connect to the RStudio-Server later. [ user@amd01n04 ~ ] $ cat 1 > & 2 <<END A new instance of the RStudio Server was just launched. To access it, 1. SSH tunnel from your workstation using the following command from a terminal on your local workstation: IP=\"${IP}\"; PORT=\"${PORT}\"; ssh -L ${PORT}:${IP}:${PORT} ${SINGULARITYENV_USER}@raapoi.vuw.ac.nz and point your local web browser to <http://localhost:${PORT}> 2. Log in to RStudio Server using the following credentials: user: ${SINGULARITYENV_USER} password: ${SINGULARITYENV_PASSWORD} When done, make sure to terminate everything by: 1. Exit the RStudio Session (\"power\" button in the top right corner of the RStudio window) 2. Cancel the job by issuing the following command on the login node: scancel -f ${SLURM_JOB_ID} END Step 4. Finally, start RStudio-Server [ user@amd01n04 ~ ] $ singularity exec --cleanenv --scratch /run,/tmp,/var/lib/rstudio-server --workdir ${ workdir } ${ HOME } /singularity-images/rstudio-server.sif rserver --www-port ${ PORT } --auth-none = 0 --auth-pam-helper-path = pam-helper --auth-stay-signed-in-days = 30 --auth-timeout-minutes = 0 --server-user = $( whoami ) --rsession-path = /etc/rstudio/rsession.sh Step 5. Inside a new terminal on your personal device, create a tunnel following the instructions of the above command. [ user@personal-device:~ ] IP = \" ${ IP } \" ; PORT = \" ${ PORT } \" ; ssh -L ${ PORT } : ${ IP } : ${ PORT } ${ SINGULARITYENV_USER } @raapoi.vuw.ac.nz Note Remember to note down output of the environment variables above. Once the tunnel is set up successfully, go to your browser and with the port from the above output: http://localhost: ${ PORT } / For running this inside a batch script, submit the following via sbatch. submit.sl #! /bin/bash #SBATCH --time=00-01:00:00 #SBATCH --ntasks=2 #SBATCH --mem=8G #SBATCH --output=rstudio-server.%j #SBATCH --error=rstudio-server.%j.err #SBATCH --export=NONE # customize --output path as appropriate (to a directory readable only by the user!) # Load Singularity module module purge module load GCC/10.2.0 OpenMPI/4.0.5 Singularity/3.10.2 # Create temporary directory to be populated with directories to bind-mount in the container # where writable file systems are necessary. Adjust path as appropriate for your computing environment. workdir = $HOME /rstudio-server mkdir -p \" ${ workdir } \" / { run,tmp,var/lib/rstudio-server } chmod 700 \" ${ workdir } \" cat > \" ${ workdir } \" /database.conf <<END provider=sqlite directory=/var/lib/rstudio-server END # Set OMP_NUM_THREADS to prevent OpenBLAS (and any other OpenMP-enhanced # libraries used by R) from spawning more threads than the number of processors # allocated to the job. # # Set R_LIBS_USER to a path specific to Rocker/RStudio to avoid conflicts with # personal libraries from any R installation in the host environment cat > \" ${ workdir } \" /rsession.sh <<END #! /bin/sh export OMP_NUM_THREADS=${SLURM_JOB_CPUS_PER_NODE} export R_LIBS_USER=~/R/%p-library/%v-rocker-rstudio exec /usr/lib/rstudio-server/bin/rsession \"\\${@}\" END chmod +x \" ${ workdir } \" /rsession.sh export SINGULARITY_BIND = \" ${ workdir } /run:/run, ${ workdir } /tmp:/tmp, ${ workdir } /database.conf:/etc/rstudio/database.conf, ${ workdir } /rsession.sh:/etc/rstudio/rsession.sh, ${ workdir } /var/lib/rstudio-server:/var/lib/rstudio-server\" # Do not suspend idle sessions. # Alternative to setting session-timeout-minutes=0 in /etc/rstudio/rsession.conf # https://github.com/rstudio/rstudio/blob/v1.4.1106/src/cpp/server/ServerSessionManager.cpp#L126 export SINGULARITYENV_RSTUDIO_SESSION_TIMEOUT = 0 export SINGULARITYENV_USER = $( id -un ) export SINGULARITYENV_PASSWORD = $( openssl rand -base64 15 ) # Get unused socket per https://unix.stackexchange.com/a/132524 # Tiny race condition between the python & singularity commands export PORT = $( /usr/bin/python3 -c 'import socket; s=socket.socket(); s.bind((\"\", 0)); print(s.getsockname()[1]); s.close()' ) export IP = $( hostname -i ) cat 1 > & 2 <<END A new instance of the RStudio Server was just launched. To access it, 1. SSH tunnel from your workstation using the following command from a terminal on your local workstation: IP=\"${IP}\"; PORT=\"${PORT}\"; ssh -L ${PORT}:${IP}:${PORT} ${SINGULARITYENV_USER}@raapoi.vuw.ac.nz and point your local web browser to <http://localhost:${PORT}> 2. Log in to RStudio Server using the following credentials: user: ${SINGULARITYENV_USER} password: ${SINGULARITYENV_PASSWORD} When done, make sure to terminate everything by: 1. Exit the RStudio Session (\"power\" button in the top right corner of the RStudio window) 2. Cancel the job by issuing the following command on the login node: scancel -f ${SLURM_JOB_ID} END singularity exec \" $HOME /singularity-images/rstudio-server.sif\" \\ rserver --www-port \" $PORT \" \\ --auth-none = 0 \\ --auth-pam-helper-path = pam-helper \\ --auth-stay-signed-in-days = 30 \\ --auth-timeout-minutes = 0 \\ --rsession-path = /etc/rstudio/rsession.sh \\ --server-user = \" $USER \" printf 'RStudio Server exited\\n' 1 > & 2 Step 2. Once your job starts note the JOBID, read the output file for instructions to connect to the running RStudio-Server. [ user@raapoi-login:~ ] $ sbatch submit.sl ; vuw-myjobs Submitted batch job <job_id> [ user@raapoi-login:~ ] $ cat rstudio-server.%j # %j is the job id For any help, please contact one of our support team members: Support","title":"RStudio-Server"},{"location":"usersub/#matlab-gui-via-x-forwarding","text":"Friendly Reminder HPC is built to serve powerful computational work largely via commandline interface, kindly read our FAQ section: Visualisation . Please proceed only if you think non-GUI MATLAB is not an option. Kindly make sure your personal device has X-Server installed and running.","title":"MATLAB GUI via X-Forwarding"},{"location":"usersub/#installing-and-running-an-x-server-on-windows","text":"This tutorial explains how to install an X-Server on Windows. We will use the VcXsrv , a free X-server for this purpose. Steps: Download the installer from here: vcxsrv Run the installer. Select Full under Installation Options and click Next Select a target folder To Run the Server: Open the XLaunch program (most likely on your desktop) Select Multiple Windows and click Next Select Start no client and click Next On the Extra settings window, click Next On the Finish configuration page click Finish You have now started your X Server.","title":"Installing and running an X Server on Windows"},{"location":"usersub/#set-up-your-console","text":"In the Git bash or the windows command line ( cmd ) terminal, before you connect to an ssh server, you have to set the used display. Under normal circumstances, VcXsrv will start the Xserver as display 0.0 . If for some reason the remote graphical user interface does not start later on, you can check, the actual display by right-clicking on the tray-icon of the X Server and select Show log . Search for DISPLAY in the log file, and you will find something like: DISPLAY=127.0.0.1:0.0 In your terminal enter: set DISPLAY=127.0.0.1:0.0 Now you are set up to connect to the server of your choice via: ssh -Y <username>@raapoi.vuw.ac.nz Notice, that on windows you will likely need the -Y flag for X Server connections, since it seems -X does not normally work.","title":"Set up your console"},{"location":"usersub/#on-rapoi","text":"Once logged in allocate resources for an interactive session RAAPOI_USERNAME@raapoi-login:~$ srun --x11 -t0-01:00:00 -wamd01n01 --ntasks = 8 --mem = 32G --pty bash Tip Extend the time to maximum 5 hours with -t0-04:59:59 After your job starts and the prompt changes, run matlab and you'll see MATLAB GUI Window on your personal device. RAAPOI_USERNAME@amd01n01:~$ module use /home/software/tools/eb_modulefiles/all/Core RAAPOI_USERNAME@amd01n01:~$ module load MATLAB/2024a RAAPOI_USERNAME@amd01n01:~$ module load fosscuda/2020b RAAPOI_USERNAME@amd01n01:~$ matlab -softwareopengl For any help, please contact one of our support team members: Support","title":"On R\u0101poi"},{"location":"advanced/OpenBLAS_users_guide/","text":"OpenBLAS/FlexiBLAS users guide \u00b6 What is BLAS/OpenBLAS/FlexiBLAS? \u00b6 BLAS stands for Basic Linear Algebra Subprograms and consists of a set routines that probide standard building blocks for numerical calculations involving linear algebra (e.g. addition and multiplication of vectors and matrices). Software such as OpenBLAS and FlexiBLAS (and other variants) provide optimised implementations and extensions of such routines. How do I know if my software uses OpenBLAS/FlexiBLAS? \u00b6 Linear algebra is fundamental to a broad range of numerical algorithms, so it is quite common for computationally intensive software to rely on some form of BLAS implementation. One way you can potentially find out if OpenBLAS or FlexiBLAS might be used by your software is to load the modules associated with your piece of software, then execute module list and see if OpenBLAS of FlexiBLAS are listed. If yes, it is not a guarantee they are being used, but is certainly a possibility. Known issues \u00b6 Most versions of OpenBLAS have been installed with OpenMP support. Some of the newer versions (0.3.20 onwards) have a bug which can cause OpenBLAS to hang on startup (unless you allocate an obscene and uneccessary amount of memory). When using software that links to one of those versions it is essential that you: (i) set the environment variable OMP_NUM_THREADS to the desired number of threads, and/or (ii) set the environment varialbe OMP_PROC_BIND=true (or any other valid option, apart from false ). The recommended approach is to add the line export OMP_NUM_THREADS=$SLURM_NTASKS and/or export OMP_PROC_BIND=true after you have loaded all of the modules you require. (You will need to change export OMP_NUM_THREADS=$SLURM_NTASKS accordingly if you are using some form of hybrid paralellism, and/or are setting a value of --ncpus-per-task which is more than one.)","title":"OpenBLAS users guide"},{"location":"advanced/OpenBLAS_users_guide/#openblasflexiblas-users-guide","text":"","title":"OpenBLAS/FlexiBLAS users guide"},{"location":"advanced/OpenBLAS_users_guide/#what-is-blasopenblasflexiblas","text":"BLAS stands for Basic Linear Algebra Subprograms and consists of a set routines that probide standard building blocks for numerical calculations involving linear algebra (e.g. addition and multiplication of vectors and matrices). Software such as OpenBLAS and FlexiBLAS (and other variants) provide optimised implementations and extensions of such routines.","title":"What is BLAS/OpenBLAS/FlexiBLAS?"},{"location":"advanced/OpenBLAS_users_guide/#how-do-i-know-if-my-software-uses-openblasflexiblas","text":"Linear algebra is fundamental to a broad range of numerical algorithms, so it is quite common for computationally intensive software to rely on some form of BLAS implementation. One way you can potentially find out if OpenBLAS or FlexiBLAS might be used by your software is to load the modules associated with your piece of software, then execute module list and see if OpenBLAS of FlexiBLAS are listed. If yes, it is not a guarantee they are being used, but is certainly a possibility.","title":"How do I know if my software uses OpenBLAS/FlexiBLAS?"},{"location":"advanced/OpenBLAS_users_guide/#known-issues","text":"Most versions of OpenBLAS have been installed with OpenMP support. Some of the newer versions (0.3.20 onwards) have a bug which can cause OpenBLAS to hang on startup (unless you allocate an obscene and uneccessary amount of memory). When using software that links to one of those versions it is essential that you: (i) set the environment variable OMP_NUM_THREADS to the desired number of threads, and/or (ii) set the environment varialbe OMP_PROC_BIND=true (or any other valid option, apart from false ). The recommended approach is to add the line export OMP_NUM_THREADS=$SLURM_NTASKS and/or export OMP_PROC_BIND=true after you have loaded all of the modules you require. (You will need to change export OMP_NUM_THREADS=$SLURM_NTASKS accordingly if you are using some form of hybrid paralellism, and/or are setting a value of --ncpus-per-task which is more than one.)","title":"Known issues"},{"location":"advanced/OpenMPI_users_guide/","text":"OpenMPI users guide \u00b6 Which versions of OpenMPI are working on R\u0101poi? \u00b6 There are a number of versions of OpenMPI on R\u0101poi, although many of these are old installations (prior to an OS update and changes to the module system) and may no longer work. Generally speaking, your best bet is to try a version which appears when you search via module spider OpenMPI (noting that the capital 'O M P I' is important here). A few examples of relatively recent version of OpenMPI which are available (as of April 2024) are OpenMPI/4.1.1 , OpenMPI/4.1.4 and OpenMPI/4.1.6 . Each of these OpenMPI modules has one or more of pre-requisite modules that need to be loaded first (generally a specific version of GCC compilers). To find out what you need to load first for a specific version of OpenMPI you just need to check the output of module spider OpenMPI/x.y.z (with the appropriate values for x,y,z). One of the examples below shows how to use OpenMPI/4.1.6 . In cases where your code utilises software from another module which also requires a specific GCC module, that will dictate which version of OpenMPI to load (i.e. whichever one depends on the same GCC version). Otherwise, you are free to use any desired OpenMPI module. Known issues and workarounds \u00b6 There is a known issue with the communication/networking interfaces with several of the installations of OpenMPI. The error/warning messages occur sporadically, making it difficult to pin down and resolve, but it is likely there is a combination of internal and external factors that cause this (OpenMPI is a very complex beast). The warning messages take the form: Failed to modify UD QP to INIT on mlx5_0: Operation not permitted A workaround is described below, this page will be updated in the future when a more permanent solution is found. Exectute your mpi jobs using the additional arguments: mpirun -mca pml ucx -mca btl '^uct,ofi' -mca mtl '^ofi' -np $SLURM_NTASKS <your executable> This will ensure OpenMPI avoids trying to use the communication libraries which are problematic. If your executable is launched without using mpirun (i.e. it implements its own wrapper/launcher), you will instead need to set the following environment variables: export OMPI_MCA_btl = '^uct,ofi' export OMPI_MCA_pml = 'ucx' export OMPI_MCA_mtl = '^ofi' Software module ORCA users, sometimes, come across an error message: PML ucx cannot be selected To address this, update mpirun variables to: mpirun --mca btl ^openib --mca pml ob1 --mca osc ucx -np ...<remaining params> `","title":"OpenMPI users guide"},{"location":"advanced/OpenMPI_users_guide/#openmpi-users-guide","text":"","title":"OpenMPI users guide"},{"location":"advanced/OpenMPI_users_guide/#which-versions-of-openmpi-are-working-on-rapoi","text":"There are a number of versions of OpenMPI on R\u0101poi, although many of these are old installations (prior to an OS update and changes to the module system) and may no longer work. Generally speaking, your best bet is to try a version which appears when you search via module spider OpenMPI (noting that the capital 'O M P I' is important here). A few examples of relatively recent version of OpenMPI which are available (as of April 2024) are OpenMPI/4.1.1 , OpenMPI/4.1.4 and OpenMPI/4.1.6 . Each of these OpenMPI modules has one or more of pre-requisite modules that need to be loaded first (generally a specific version of GCC compilers). To find out what you need to load first for a specific version of OpenMPI you just need to check the output of module spider OpenMPI/x.y.z (with the appropriate values for x,y,z). One of the examples below shows how to use OpenMPI/4.1.6 . In cases where your code utilises software from another module which also requires a specific GCC module, that will dictate which version of OpenMPI to load (i.e. whichever one depends on the same GCC version). Otherwise, you are free to use any desired OpenMPI module.","title":"Which versions of OpenMPI are working on R\u0101poi?"},{"location":"advanced/OpenMPI_users_guide/#known-issues-and-workarounds","text":"There is a known issue with the communication/networking interfaces with several of the installations of OpenMPI. The error/warning messages occur sporadically, making it difficult to pin down and resolve, but it is likely there is a combination of internal and external factors that cause this (OpenMPI is a very complex beast). The warning messages take the form: Failed to modify UD QP to INIT on mlx5_0: Operation not permitted A workaround is described below, this page will be updated in the future when a more permanent solution is found. Exectute your mpi jobs using the additional arguments: mpirun -mca pml ucx -mca btl '^uct,ofi' -mca mtl '^ofi' -np $SLURM_NTASKS <your executable> This will ensure OpenMPI avoids trying to use the communication libraries which are problematic. If your executable is launched without using mpirun (i.e. it implements its own wrapper/launcher), you will instead need to set the following environment variables: export OMPI_MCA_btl = '^uct,ofi' export OMPI_MCA_pml = 'ucx' export OMPI_MCA_mtl = '^ofi' Software module ORCA users, sometimes, come across an error message: PML ucx cannot be selected To address this, update mpirun variables to: mpirun --mca btl ^openib --mca pml ob1 --mca osc ucx -np ...<remaining params> `","title":"Known issues and workarounds"},{"location":"advanced/containers/","text":"Using Containers \u00b6 Researchers can use Docker or Singularity containers within the cluster. This is a great way to run difficult-to-compile applications or to share workflows among colleagues. See also: Singularity Running an interactive container \u00b6 User can run within a container interactively, this is great for testing code before running a job. Here is an example of running within a docker container lolcow: srun --pty -c 4 --mem=64G bash module purge module load GCC/10.2.0 OpenMPI/4.0.5 Singularity/3.10.2 singularity pull docker://godlovedc/lolcow singularity shell lolcow.sif Once you have typed the singularity shell command you will be within the container and can type the commands available from within the container such as the lolcow utility fortune | cowsay | lolcat Running a container in batch \u00b6 Running a batch job with containers is similar to running a regular job, but will ultimately depend on how the container was created, so your mileage may vary. Here is an example batch submit script that will run the autometa software that was created in a docker image, lets name the submit file runContainer.sh: #SBATCH -J autometa-job #SBATCH -c 4 #SBATCH --mem=64G #SBATCH --mailtype=BEGIN,END,FAIL #SBATCH --mail-user=myemail@email.net #SBATCH --time=12:00:00 module load Singularity/3.10.2 singularity pull docker://jasonkwan/autometa:latest singularity exec autometa_latest.sif calculate_read_coverage.py somedata.dat Now to run the file you can: sbatch runContainer.sh Note that singularity shell is primarily for interactive use and singularity exec (or possibly singularity run ) are for executing the applications that were built within the container directly. It is important to know how the container was created to make effective use of the software.","title":"Containers"},{"location":"advanced/containers/#using-containers","text":"Researchers can use Docker or Singularity containers within the cluster. This is a great way to run difficult-to-compile applications or to share workflows among colleagues. See also: Singularity","title":"Using Containers"},{"location":"advanced/containers/#running-an-interactive-container","text":"User can run within a container interactively, this is great for testing code before running a job. Here is an example of running within a docker container lolcow: srun --pty -c 4 --mem=64G bash module purge module load GCC/10.2.0 OpenMPI/4.0.5 Singularity/3.10.2 singularity pull docker://godlovedc/lolcow singularity shell lolcow.sif Once you have typed the singularity shell command you will be within the container and can type the commands available from within the container such as the lolcow utility fortune | cowsay | lolcat","title":"Running an interactive container"},{"location":"advanced/containers/#running-a-container-in-batch","text":"Running a batch job with containers is similar to running a regular job, but will ultimately depend on how the container was created, so your mileage may vary. Here is an example batch submit script that will run the autometa software that was created in a docker image, lets name the submit file runContainer.sh: #SBATCH -J autometa-job #SBATCH -c 4 #SBATCH --mem=64G #SBATCH --mailtype=BEGIN,END,FAIL #SBATCH --mail-user=myemail@email.net #SBATCH --time=12:00:00 module load Singularity/3.10.2 singularity pull docker://jasonkwan/autometa:latest singularity exec autometa_latest.sif calculate_read_coverage.py somedata.dat Now to run the file you can: sbatch runContainer.sh Note that singularity shell is primarily for interactive use and singularity exec (or possibly singularity run ) are for executing the applications that were built within the container directly. It is important to know how the container was created to make effective use of the software.","title":"Running a container in batch"},{"location":"advanced/notebooks/","text":"Starting and Working with a Jupyter Notebook \u00b6 Running Jupyter notebooks on R\u0101poi is usually a two step processes. First you start the jupyter server on a compute node - either via an interactive session or an sbatch job. Then you connect to R\u0101poi again via a new ssh session port forwarding the port selected by Jupter to your local machine for the web session. Tip There is a potentially simpler method at the end of this guide using firefox and tab containers. For general information on using Python, see the Python users guide . The first step is getting and modifying a submission script. Example submission scripts are included at /home/software/vuwrc/examples/jupyter/ notebook-bare.sh # The base notebook script - you manage your dependancies via pip notebook-anaconda.sh # a version for if you prefer anaconda R-notebook.sh # Using the R kernel instead R-notebook-anaconda.sh # R kernel and managed by anaconda All these scripts will need to be copied to your working directory and modified to suit your needs. In each case you'll need to install your dependancies first - at a bare minimum you'll need Jupyter, installed either via pip or anaconda. Note if you are intending to do anything needing GPU in your notebooks, you'll need to do all these installs in the gpu or highmem nodes as you'll likely need the relavent CUDA modules loaded during the installs. notebook-bare.sh example \u00b6 Step 1: The best way to start jupyter is with a batch submit script. We have created an example script. You can copy this script from one available on the cluster, just type the following: cp /home/software/vuwrc/examples/jupyter/notebook-bare.sh notebook.sh If you are using Anaconda and have installed it in the default location you need to use the following submit file instead: cp /home/software/vuwrc/examples/jupyter/notebook-anaconda.sh notebook-anaconda.sh If you have any python dependancies you will need to install them before you run your script. You will also have to install jupyter. Currenly you'll need to do that in an interactive session. You only need to do this once. srun -c 4 --mem = 8G --partition = quicktest --time = 0 -01:00:00 --pty bash # get a 1 hour interactive session on quicktest #prompt changes to something like #<username@itl02n02> you are now \"on\" a quicktest node # Load required module for the python version in the notebook-bare.sh module load gompi/2022a module load Python/3.10.4-bare python3 -m venv env # setup python virtual env in the env directory pip install jupyterlab pandas plotnine # install dependancies - you *must* at least install jupyter #exit the interactive session exit #prompt changes to something like #<username@raapoi-login> you are now back on the login/master node This script is ready to run as is, but we recommend editing it to satisfy your own CPU, memory and time requirements. Once you have edited the file you can run it thusly: sbatch notebook.sh or if using Anaconda: sbatch notebook-anaconda.sh NOTE: If you copied the R notebook script, replace notebook.sh with R-notebook.sh This will submit the file to run a job. It may take some time for the job to run, depending on how busy the cluster is at the time. Once the job begins to run you will see some information in the file called notebook-JOBID.out (JOBID will be the actual jobid of this job, eg notebook-478903.out. If you view this file (users can type cat notebook-JOBID.out to view the file onscreen). You will see a line such as: Or copy and paste one of these URLs: http://130.195.19.20:47033/lab?token=<some string of characters> http://127.0.0.1:47033/lab?token=<some string of characters> The 2 important pieces of information here are the IP address, in this case 130.195.19.20 and the port number, 47033 . These numbers should be different for you since the port number is random, although the IP Address may be the same since we have a limited number of compute nodes. Also notice after the ?token= you will see a random hash. This hash is a security feature and allows you to connect to the notebook. You will need to use these to view the notebook from your local machine. Step 2: To start working with the notebook you will need to tunnel a ssh session. In your SSH tunnel you will use the cluster login node (raapoi.vuw.ac.nz) to connect to the compute node (in the example above the compute node is at address 130.195.19.20) and transfer all the traffic back and forth between your computer and the compute node). Step 2a from a Mac: Open a new session window from Terminal.app or other terminal utility such as Xquartz and type the following: ssh -L <PORT_NUMBER>:<IP_ADDRESS>:<PORT_NUMBER> username@raapoi.vuw.ac.nz For example: ssh -L 47033:130.195.19.20:47033 janedoe@raapoi.vuw.ac.nz Once you are at a prompt you can go to Step 3 Step 2b: from Windows We recommend tunnelling using Git Bash, which is part of the Git for Windows project or MobaXTerm . There are 2 methods for tunneling in Moba, one is command line, the other is GUI-based. Method 1 (Git Bash or MobaXterm): Command-line, start a local Git Bash or MobaXterm terminal (or try the GUI method, below) From the command prompt type: ssh -L <PORT_NUMBER>:<IP_ADDRESS>:<PORT_NUMBER> username@raapoi.vuw.ac.nz For example: ssh -L 47033:130.195.19.20:47033 janedoe@raapoi.vuw.ac.nz Once you are at a prompt you can go to Step 3 Method 2 (MobaXterm): GUI-based, go to the Tunneling menu: Now click on New SSH Tunnel When you complete the creation of your tunnel click Start all tunnels . Enter your password and reply \"yes\" to any questions asked about accepting hostkeys or opening firewalls. You can safely exit the tunnel building menu. Step 3 Now open your favorite web browser and then use the URL from your job output file and paste it in your browsers location bar, for example my full URL was: http://130.195.19.20:47033/?token=badef11b1371945b314e2e89b9a182f68e39dc40783ed68e Step 4 One last thing you need to do is to replace the IP address with the word localhost . This will allow your browser to follow the tunnel you just opened and connect to the notebook running on an engaging compute node, in my case my address line will now look like this: http://localhost:47033/?token=badef11b1371945b314e2e89b9a182f68e39dc40783ed68e Now you can hit return and you should see your notebook running on an Engaging compute node. NOTE : when you are done with your notebook, please remember to cancel your job to free up the resources for others, hint: scancel If you want more information on working with Jupyter, there is good documentation, here: Jupyter Notebooks Working with notebooks using Firefox tab containers \u00b6 There is a perhaps simpler single step process of working with jupyter notebooks. It relies on some nice features in Firefox . Firefox has tab containers - you can have categories of tabs that are basically independent from each other with separate web cookies but importantly for our case separate proxy settings. You will also currenly need to get the firefox add-on Container-Proxy its github page Setup a tab container in Firefox called something like Raapoi. Use the container-proxy extension to assign a proxy to that tab set. I choose 9001, but you can use any fairly high port number - note it doens't matter if many people connect on the same port. When you connect to raapoi, use ssh socks5 proxy settings. In MacOS/linux/wsl2 ssh -D 9001 <username>@raapoi.vuw.ac.nz putty: Use Putty as a Socks Proxy MobaXterm: MobaXterm SOCKS5 Proxy - stackoverflow In Firefox open a new tab by holding down the new tab + button and selecting your Raapoi tab container. Any tabs opened with that container will have all their webtraffic directed via the R\u0101poi login node. Your laptop/desktop can't directly see all the compute nodes, but the login node can. When you start a jupyter notebook and get the message: Or copy and paste one of these URLs: http://130.195.19.20:47033/lab?token=<some string of characters> http://127.0.0.1:47033/lab?token=<some string of characters> You can just immediatly open http://130.195.19.20:47033/lab?token=<some string of characters> in your Raapoi container tab.","title":"Notebooks"},{"location":"advanced/notebooks/#starting-and-working-with-a-jupyter-notebook","text":"Running Jupyter notebooks on R\u0101poi is usually a two step processes. First you start the jupyter server on a compute node - either via an interactive session or an sbatch job. Then you connect to R\u0101poi again via a new ssh session port forwarding the port selected by Jupter to your local machine for the web session. Tip There is a potentially simpler method at the end of this guide using firefox and tab containers. For general information on using Python, see the Python users guide . The first step is getting and modifying a submission script. Example submission scripts are included at /home/software/vuwrc/examples/jupyter/ notebook-bare.sh # The base notebook script - you manage your dependancies via pip notebook-anaconda.sh # a version for if you prefer anaconda R-notebook.sh # Using the R kernel instead R-notebook-anaconda.sh # R kernel and managed by anaconda All these scripts will need to be copied to your working directory and modified to suit your needs. In each case you'll need to install your dependancies first - at a bare minimum you'll need Jupyter, installed either via pip or anaconda. Note if you are intending to do anything needing GPU in your notebooks, you'll need to do all these installs in the gpu or highmem nodes as you'll likely need the relavent CUDA modules loaded during the installs.","title":"Starting and Working with a Jupyter Notebook"},{"location":"advanced/notebooks/#notebook-baresh-example","text":"Step 1: The best way to start jupyter is with a batch submit script. We have created an example script. You can copy this script from one available on the cluster, just type the following: cp /home/software/vuwrc/examples/jupyter/notebook-bare.sh notebook.sh If you are using Anaconda and have installed it in the default location you need to use the following submit file instead: cp /home/software/vuwrc/examples/jupyter/notebook-anaconda.sh notebook-anaconda.sh If you have any python dependancies you will need to install them before you run your script. You will also have to install jupyter. Currenly you'll need to do that in an interactive session. You only need to do this once. srun -c 4 --mem = 8G --partition = quicktest --time = 0 -01:00:00 --pty bash # get a 1 hour interactive session on quicktest #prompt changes to something like #<username@itl02n02> you are now \"on\" a quicktest node # Load required module for the python version in the notebook-bare.sh module load gompi/2022a module load Python/3.10.4-bare python3 -m venv env # setup python virtual env in the env directory pip install jupyterlab pandas plotnine # install dependancies - you *must* at least install jupyter #exit the interactive session exit #prompt changes to something like #<username@raapoi-login> you are now back on the login/master node This script is ready to run as is, but we recommend editing it to satisfy your own CPU, memory and time requirements. Once you have edited the file you can run it thusly: sbatch notebook.sh or if using Anaconda: sbatch notebook-anaconda.sh NOTE: If you copied the R notebook script, replace notebook.sh with R-notebook.sh This will submit the file to run a job. It may take some time for the job to run, depending on how busy the cluster is at the time. Once the job begins to run you will see some information in the file called notebook-JOBID.out (JOBID will be the actual jobid of this job, eg notebook-478903.out. If you view this file (users can type cat notebook-JOBID.out to view the file onscreen). You will see a line such as: Or copy and paste one of these URLs: http://130.195.19.20:47033/lab?token=<some string of characters> http://127.0.0.1:47033/lab?token=<some string of characters> The 2 important pieces of information here are the IP address, in this case 130.195.19.20 and the port number, 47033 . These numbers should be different for you since the port number is random, although the IP Address may be the same since we have a limited number of compute nodes. Also notice after the ?token= you will see a random hash. This hash is a security feature and allows you to connect to the notebook. You will need to use these to view the notebook from your local machine. Step 2: To start working with the notebook you will need to tunnel a ssh session. In your SSH tunnel you will use the cluster login node (raapoi.vuw.ac.nz) to connect to the compute node (in the example above the compute node is at address 130.195.19.20) and transfer all the traffic back and forth between your computer and the compute node). Step 2a from a Mac: Open a new session window from Terminal.app or other terminal utility such as Xquartz and type the following: ssh -L <PORT_NUMBER>:<IP_ADDRESS>:<PORT_NUMBER> username@raapoi.vuw.ac.nz For example: ssh -L 47033:130.195.19.20:47033 janedoe@raapoi.vuw.ac.nz Once you are at a prompt you can go to Step 3 Step 2b: from Windows We recommend tunnelling using Git Bash, which is part of the Git for Windows project or MobaXTerm . There are 2 methods for tunneling in Moba, one is command line, the other is GUI-based. Method 1 (Git Bash or MobaXterm): Command-line, start a local Git Bash or MobaXterm terminal (or try the GUI method, below) From the command prompt type: ssh -L <PORT_NUMBER>:<IP_ADDRESS>:<PORT_NUMBER> username@raapoi.vuw.ac.nz For example: ssh -L 47033:130.195.19.20:47033 janedoe@raapoi.vuw.ac.nz Once you are at a prompt you can go to Step 3 Method 2 (MobaXterm): GUI-based, go to the Tunneling menu: Now click on New SSH Tunnel When you complete the creation of your tunnel click Start all tunnels . Enter your password and reply \"yes\" to any questions asked about accepting hostkeys or opening firewalls. You can safely exit the tunnel building menu. Step 3 Now open your favorite web browser and then use the URL from your job output file and paste it in your browsers location bar, for example my full URL was: http://130.195.19.20:47033/?token=badef11b1371945b314e2e89b9a182f68e39dc40783ed68e Step 4 One last thing you need to do is to replace the IP address with the word localhost . This will allow your browser to follow the tunnel you just opened and connect to the notebook running on an engaging compute node, in my case my address line will now look like this: http://localhost:47033/?token=badef11b1371945b314e2e89b9a182f68e39dc40783ed68e Now you can hit return and you should see your notebook running on an Engaging compute node. NOTE : when you are done with your notebook, please remember to cancel your job to free up the resources for others, hint: scancel If you want more information on working with Jupyter, there is good documentation, here: Jupyter Notebooks","title":"notebook-bare.sh example"},{"location":"advanced/notebooks/#working-with-notebooks-using-firefox-tab-containers","text":"There is a perhaps simpler single step process of working with jupyter notebooks. It relies on some nice features in Firefox . Firefox has tab containers - you can have categories of tabs that are basically independent from each other with separate web cookies but importantly for our case separate proxy settings. You will also currenly need to get the firefox add-on Container-Proxy its github page Setup a tab container in Firefox called something like Raapoi. Use the container-proxy extension to assign a proxy to that tab set. I choose 9001, but you can use any fairly high port number - note it doens't matter if many people connect on the same port. When you connect to raapoi, use ssh socks5 proxy settings. In MacOS/linux/wsl2 ssh -D 9001 <username>@raapoi.vuw.ac.nz putty: Use Putty as a Socks Proxy MobaXterm: MobaXterm SOCKS5 Proxy - stackoverflow In Firefox open a new tab by holding down the new tab + button and selecting your Raapoi tab container. Any tabs opened with that container will have all their webtraffic directed via the R\u0101poi login node. Your laptop/desktop can't directly see all the compute nodes, but the login node can. When you start a jupyter notebook and get the message: Or copy and paste one of these URLs: http://130.195.19.20:47033/lab?token=<some string of characters> http://127.0.0.1:47033/lab?token=<some string of characters> You can just immediatly open http://130.195.19.20:47033/lab?token=<some string of characters> in your Raapoi container tab.","title":"Working with notebooks using Firefox tab containers"},{"location":"elements/LinkingElements/","text":"Linking Research outputs to R\u0101poi in Elements \u00b6 Prerequisites - to complete this process, you'll need to have an Elements profile. Elements is VUW's research management system, and all academic staff should have access by default. If you are a post-graduate student, you may not have a public profile page, but you should still be able to log in with your VUW staff credentials and carry out the process. For full documentation on Staff Profiles with Elements, see this page . Log in to Elements (elements.wgtn.ac.nz) and make sure relevant research outputs (funded grant, journal article etc) are present in your Elements profile. If they are not showing, the outputs can be added manually (see guide pdf, pp 38-39 ). Navigate to output that's made use of R\u0101poi (publication, grant or activity) by clicking on the VIEW ALL button at the bottom right of each card (For this example, we'll use Publication > VIEW ALL ): Click on the relevant publication: Click on RELATIONSHIPS - CREATE NEW on the right hand side of the interface: The Create links to this publication pop-up window should appear. Select the Equipment option: Type HPC or R\u0101poi into the Name field, and with Linked to set to Anyone and Type set to Any. ( Note : be sure to include the macron over the \u0101 in R\u0101poi if using the latter option) Check links are correct, then click on Create one new link . ( Note : if you have multiple publications or grants to link, you can speed up the process by going to the Publications tab in the interface above, clicking on the Click here link, and then using the date filter to return multiple outputs.) Check to see your item appears on the public R\u0101poi elements profile page under publications or grants . Repeat steps 2-8 as needed.","title":"Linking R\u0101poi outputs to Elements"},{"location":"elements/LinkingElements/#linking-research-outputs-to-rapoi-in-elements","text":"Prerequisites - to complete this process, you'll need to have an Elements profile. Elements is VUW's research management system, and all academic staff should have access by default. If you are a post-graduate student, you may not have a public profile page, but you should still be able to log in with your VUW staff credentials and carry out the process. For full documentation on Staff Profiles with Elements, see this page . Log in to Elements (elements.wgtn.ac.nz) and make sure relevant research outputs (funded grant, journal article etc) are present in your Elements profile. If they are not showing, the outputs can be added manually (see guide pdf, pp 38-39 ). Navigate to output that's made use of R\u0101poi (publication, grant or activity) by clicking on the VIEW ALL button at the bottom right of each card (For this example, we'll use Publication > VIEW ALL ): Click on the relevant publication: Click on RELATIONSHIPS - CREATE NEW on the right hand side of the interface: The Create links to this publication pop-up window should appear. Select the Equipment option: Type HPC or R\u0101poi into the Name field, and with Linked to set to Anyone and Type set to Any. ( Note : be sure to include the macron over the \u0101 in R\u0101poi if using the latter option) Check links are correct, then click on Create one new link . ( Note : if you have multiple publications or grants to link, you can speed up the process by going to the Publications tab in the interface above, clicking on the Click here link, and then using the date filter to return multiple outputs.) Check to see your item appears on the public R\u0101poi elements profile page under publications or grants . Repeat steps 2-8 as needed.","title":"Linking Research outputs to R\u0101poi in Elements"},{"location":"examples/Python_users_guide/","text":"Python users guide \u00b6 Which versions of Python are working on R\u0101poi? \u00b6 There are a number of versions of Python on R\u0101poi, although many of these are old installations (prior to an OS update and changes to the module system) and may no longer work. Generally speaking, your best bet is to try a version which appears when you search via module spider Python (noting that the capital 'P' in Python is important here). A few examples of relatively recent version of Python which are available (as of April 2024) are Python/3.9.5 , Python/3.10.8 and Python/3.11.5 . Each of these Python modules has one or more of pre-requisite modules that need to be loaded first (generally a specific version of GCC compilers). To find out what you need to load first for a specific version of Python you just need to check the output of module spider Python/x.y.z (with the appropriate values for x,y,z). One of the examples below shows how to use Python/3.9.5 . In cases where your Python code needs to interact with software from another module which also requires a specific GCC module, that will dictate which version of Python to load (i.e. whichever one depends on the same GCC version). Otherwise, you are free to use any desired Python module. The Python installations generally have a minimal number of packages/libraries installed. If you require additional packages/libraries it is recommended to create a virtual environment and install any desired packages within that environment. This is illustrated in the examples below using both virtualenv/pip and anaconda/conda. See also: Using Jupyter Notebooks Simple Python program using virtualenv and pip \u00b6 First we need to create a working directory and move there mkdir python_test cd python_test Next we load the python 3 module and use python 3 to create a python virtualenv. This way we can install pip packages which are not installed on the cluster module load GCCcore/10.3.0 module load Python/3.9.5 python3 -m venv mytest Activate the mytest virtualenv and use pip to install the webcolors package source mytest/bin/activate pip install webcolors Create the file test.py with the following contents using nano import webcolors from random import randint from socket import gethostname colour_list = list ( webcolors . CSS3_HEX_TO_NAMES . items ()) requested_colour = randint ( 0 , len ( colour_list )) colour_name = colour_list [ requested_colour ][ 1 ] print ( \"Random colour name:\" , colour_name , \" on host: \" , gethostname ()) Alternatively download it with wget: wget https://raw.githubusercontent.com/ \\ vuw-research-computing/raapoi-tools/ \\ master/examples/python_venv/test.py Using nano create the submissions script called python_submit.sh with the following content - change me@email.com to your email address. #!/bin/bash # #SBATCH --job-name=python_test #SBATCH -o python_test.out #SBATCH -e python_test.err # #SBATCH --cpus-per-task=2 #Note: you are always allocated an even number of cpus #SBATCH --mem=1G #SBATCH --time=10:00 # #SBATCH --mail-type=BEGIN,END,FAIL #SBATCH --mail-user=me@email.com module load GCCcore/10.3.0 module load Python/3.9.5 source mytest/bin/activate python test.py Alternatively download it with wget wget https://raw.githubusercontent.com/ \\ vuw-research-computing/raapoi-tools/ \\ master/examples/python_venv/python_submit.sh To submit your job to the Slurm scheduler sbatch python_submit.sh Check for your job on the queue with squeue though it might finish very fast. The output files will appear in your working directory. Using Anaconda/Miniconda/conda \u00b6 Many users use Anaconda/Miniconda to manage software stacks. One way to do this is to use singularity containers with the conda environment inside - this allows the conda environment to load quickly as the many small conda files are inside a container which the file system sees as one file. However, this is also an additional bit of complexity so many users just use conda outside of singularity. You can install your own version of Anaconda/Miniconda to your home directory or scratch. We have also got packaged versions of Anaconda/Miniconda installed with our module loading system. Anaconda has many built in packages so we will use that in our examples, but Miniconda is also available if prefer to start from a minimal initial setup. module load Anaconda3/2020.11 export PIP_NO_CACHE_DIR = 1 export PYTHONNOUSERSITE = 1 Note Setting the variables PIP_NO_CACHE_DIR and PYTHONNOUSERSITE prevents conda from trying to use the system python and pip. This ensures an isolated environment and avoids potential conflicts with system packages. Let's create a new conda environment for this example, in a sensible location, I used ~/examples/conda/idba conda create --name idba-example # press y for the Proceed prompt if it looks correct source $( conda info --base ) /etc/profile.d/conda.sh conda activate idba-example #activate our example environment. Warning On HPC systems, conda init is not recommended as it modifies your shell configuration files. This can cause problems with the module system and other software. Instead, use the source $(conda info --base)/etc/profile.d/conda.sh command to activate conda in your current shell session. Conda environments are beyond the scope of this example, but they are a good way to contain all the dependencies and programs for a particular workflow, in this case, idba. Install idba in our conda environment. Tip Note that best practise is to do the install on a compute node We'll just do it here on the login node for now - the code will run slower on the compute nodes as a result! conda install -c bioconda idba Idba is a genome assembler, we will use paired-end illumina reads of E. coli. The data is available on an Amazon S3 bucket (a cloud storage location), and we can download it using wget. mkdir data # put our data in a sensible location cd data wget --content-disposition goo.gl/JDJTaz #sequence data wget --content-disposition goo.gl/tt9fsn #sequence data cd .. #back to our project directory The reads we have are paired-end fastq files but idba requires a fasta file. We can use a tool installed with idba to convert them. We'll do this on the R\u0101poi login node as it is a fast task that doesn't need many resources. fq2fa --merge --filter data/MiSeq_Ecoli_MG1655_50x_R1.fastq data/MiSeq_Ecoli_MG1655_50x_R2.fastq data/read.fa To create our submission script we need to know the path to our conda enviroment. To get this: conda env list You'll need to find your idba-example environment, and next to it is the path you'll need for your submission script. In my case: # conda environments: # base * /home/username/anaconda3 idba-example /home/username/anaconda3/envs/idba-example # We need this line, it'll be different for you! Create our sbatch submission script. Note that this sequence doesn't need a lot of memory, so we'll use 3G. To see your usage after the job has run use vuw-job-report <job-id> idba_submit.sh #!/bin/bash #SBATCH --job-name=idba_test #SBATCH -o _output.out #SBATCH -e _output.err #SBATCH --time=00:5:00 #SBATCH --partition=quicktest #SBATCH --ntasks=12 #SBATCH --mem=3G module load Anaconda3/2020.11 eval \" $( conda shell.bash hook ) \" # basically inits your conda - prevents errors like: CommandNotFoundError: Your shell has not been properly configured ... conda activate /home/username/anaconda3/envs/idba-example # We will need to activate our conda enviroment on the remote node idba idba_ud -r data/read.fa -o output To submit our job sbatch idba_submit.sh To see our job running or queuing squeue -u $USER This job will take a few minutes to run, generally less than 5. When the job is done we can see the output in the output folder. We can also see the std output and std err in the files _output.out and _output.err . The quickest way to examine them is to cat the files when the run is done. cat _output.out","title":"Python users guide"},{"location":"examples/Python_users_guide/#python-users-guide","text":"","title":"Python users guide"},{"location":"examples/Python_users_guide/#which-versions-of-python-are-working-on-rapoi","text":"There are a number of versions of Python on R\u0101poi, although many of these are old installations (prior to an OS update and changes to the module system) and may no longer work. Generally speaking, your best bet is to try a version which appears when you search via module spider Python (noting that the capital 'P' in Python is important here). A few examples of relatively recent version of Python which are available (as of April 2024) are Python/3.9.5 , Python/3.10.8 and Python/3.11.5 . Each of these Python modules has one or more of pre-requisite modules that need to be loaded first (generally a specific version of GCC compilers). To find out what you need to load first for a specific version of Python you just need to check the output of module spider Python/x.y.z (with the appropriate values for x,y,z). One of the examples below shows how to use Python/3.9.5 . In cases where your Python code needs to interact with software from another module which also requires a specific GCC module, that will dictate which version of Python to load (i.e. whichever one depends on the same GCC version). Otherwise, you are free to use any desired Python module. The Python installations generally have a minimal number of packages/libraries installed. If you require additional packages/libraries it is recommended to create a virtual environment and install any desired packages within that environment. This is illustrated in the examples below using both virtualenv/pip and anaconda/conda. See also: Using Jupyter Notebooks","title":"Which versions of Python are working on R\u0101poi?"},{"location":"examples/Python_users_guide/#simple-python-program-using-virtualenv-and-pip","text":"First we need to create a working directory and move there mkdir python_test cd python_test Next we load the python 3 module and use python 3 to create a python virtualenv. This way we can install pip packages which are not installed on the cluster module load GCCcore/10.3.0 module load Python/3.9.5 python3 -m venv mytest Activate the mytest virtualenv and use pip to install the webcolors package source mytest/bin/activate pip install webcolors Create the file test.py with the following contents using nano import webcolors from random import randint from socket import gethostname colour_list = list ( webcolors . CSS3_HEX_TO_NAMES . items ()) requested_colour = randint ( 0 , len ( colour_list )) colour_name = colour_list [ requested_colour ][ 1 ] print ( \"Random colour name:\" , colour_name , \" on host: \" , gethostname ()) Alternatively download it with wget: wget https://raw.githubusercontent.com/ \\ vuw-research-computing/raapoi-tools/ \\ master/examples/python_venv/test.py Using nano create the submissions script called python_submit.sh with the following content - change me@email.com to your email address. #!/bin/bash # #SBATCH --job-name=python_test #SBATCH -o python_test.out #SBATCH -e python_test.err # #SBATCH --cpus-per-task=2 #Note: you are always allocated an even number of cpus #SBATCH --mem=1G #SBATCH --time=10:00 # #SBATCH --mail-type=BEGIN,END,FAIL #SBATCH --mail-user=me@email.com module load GCCcore/10.3.0 module load Python/3.9.5 source mytest/bin/activate python test.py Alternatively download it with wget wget https://raw.githubusercontent.com/ \\ vuw-research-computing/raapoi-tools/ \\ master/examples/python_venv/python_submit.sh To submit your job to the Slurm scheduler sbatch python_submit.sh Check for your job on the queue with squeue though it might finish very fast. The output files will appear in your working directory.","title":"Simple Python program using virtualenv and pip"},{"location":"examples/Python_users_guide/#using-anacondaminicondaconda","text":"Many users use Anaconda/Miniconda to manage software stacks. One way to do this is to use singularity containers with the conda environment inside - this allows the conda environment to load quickly as the many small conda files are inside a container which the file system sees as one file. However, this is also an additional bit of complexity so many users just use conda outside of singularity. You can install your own version of Anaconda/Miniconda to your home directory or scratch. We have also got packaged versions of Anaconda/Miniconda installed with our module loading system. Anaconda has many built in packages so we will use that in our examples, but Miniconda is also available if prefer to start from a minimal initial setup. module load Anaconda3/2020.11 export PIP_NO_CACHE_DIR = 1 export PYTHONNOUSERSITE = 1 Note Setting the variables PIP_NO_CACHE_DIR and PYTHONNOUSERSITE prevents conda from trying to use the system python and pip. This ensures an isolated environment and avoids potential conflicts with system packages. Let's create a new conda environment for this example, in a sensible location, I used ~/examples/conda/idba conda create --name idba-example # press y for the Proceed prompt if it looks correct source $( conda info --base ) /etc/profile.d/conda.sh conda activate idba-example #activate our example environment. Warning On HPC systems, conda init is not recommended as it modifies your shell configuration files. This can cause problems with the module system and other software. Instead, use the source $(conda info --base)/etc/profile.d/conda.sh command to activate conda in your current shell session. Conda environments are beyond the scope of this example, but they are a good way to contain all the dependencies and programs for a particular workflow, in this case, idba. Install idba in our conda environment. Tip Note that best practise is to do the install on a compute node We'll just do it here on the login node for now - the code will run slower on the compute nodes as a result! conda install -c bioconda idba Idba is a genome assembler, we will use paired-end illumina reads of E. coli. The data is available on an Amazon S3 bucket (a cloud storage location), and we can download it using wget. mkdir data # put our data in a sensible location cd data wget --content-disposition goo.gl/JDJTaz #sequence data wget --content-disposition goo.gl/tt9fsn #sequence data cd .. #back to our project directory The reads we have are paired-end fastq files but idba requires a fasta file. We can use a tool installed with idba to convert them. We'll do this on the R\u0101poi login node as it is a fast task that doesn't need many resources. fq2fa --merge --filter data/MiSeq_Ecoli_MG1655_50x_R1.fastq data/MiSeq_Ecoli_MG1655_50x_R2.fastq data/read.fa To create our submission script we need to know the path to our conda enviroment. To get this: conda env list You'll need to find your idba-example environment, and next to it is the path you'll need for your submission script. In my case: # conda environments: # base * /home/username/anaconda3 idba-example /home/username/anaconda3/envs/idba-example # We need this line, it'll be different for you! Create our sbatch submission script. Note that this sequence doesn't need a lot of memory, so we'll use 3G. To see your usage after the job has run use vuw-job-report <job-id> idba_submit.sh #!/bin/bash #SBATCH --job-name=idba_test #SBATCH -o _output.out #SBATCH -e _output.err #SBATCH --time=00:5:00 #SBATCH --partition=quicktest #SBATCH --ntasks=12 #SBATCH --mem=3G module load Anaconda3/2020.11 eval \" $( conda shell.bash hook ) \" # basically inits your conda - prevents errors like: CommandNotFoundError: Your shell has not been properly configured ... conda activate /home/username/anaconda3/envs/idba-example # We will need to activate our conda enviroment on the remote node idba idba_ud -r data/read.fa -o output To submit our job sbatch idba_submit.sh To see our job running or queuing squeue -u $USER This job will take a few minutes to run, generally less than 5. When the job is done we can see the output in the output folder. We can also see the std output and std err in the files _output.out and _output.err . The quickest way to examine them is to cat the files when the run is done. cat _output.out","title":"Using Anaconda/Miniconda/conda"},{"location":"examples/R_users_guide/","text":"R users guide \u00b6 Which versions of R are working on R\u0101poi? \u00b6 There are a number of versions of R on R\u0101poi, although many of these are old installations (prior to an OS update and changes to the module system) and no longer work. There are three relatively recent versions of R which currently work (as of April 2024), these are R/3.6.3 , R/4.1.0 and R/4.2.0 . Each of these modules has a couple of pre-requisite modules that need to be loaded prior to loading the R module. To find out what you need to load first you just need to check the output of module spider R/x.y.z (with the appropriate values for x,y,z). The following example shows how to use R/4.2.0 . Loading R packages & running a simple job \u00b6 First login to R\u0101poi and load the R module: module purge # clean/reset your environment module load config # reload utilities such as vuw-job-report module load GCC/11.2.0 OpenMPI/4.1.1 # pre-requisites for the new R module module load R/4.2.0 Then run R on the command line: R Test library existence: > library ( tidyverse ) This should load the package, and give some output like this: \u2500\u2500 Attaching packages \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 tidyverse 1.3 . 1 \u2500\u2500 \u2714 ggplot2 3.3 . 5 \u2714 purrr 0.3 . 4 \u2714 tibble 3.1 . 6 \u2714 dplyr 1.0 . 8 \u2714 tidyr 1.2 . 0 \u2714 stringr 1.4 . 0 \u2714 readr 2.1 . 2 \u2714 forcats 0.5 . 1 \u2500\u2500 Conflicts \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 tidyverse_conflicts () \u2500\u2500 \u2716 dplyr :: filter () masks stats :: filter () \u2716 dplyr :: lag () masks stats :: lag () (These conflicts are normal and can be ignored.) To quit R, type: > q () Next create a bash submission script called r_submit.sh (or another name of your choice) using your preferred text editor, e.g. nano. #!/bin/bash # #SBATCH --job-name=r_test #SBATCH -o r_test.out #SBATCH -e r_test.err # #SBATCH --cpus-per-task=2 #Note: you are always allocated an even number of cpus #SBATCH --mem=1G #SBATCH --time=10:00 module purge module load GCC/11.2.0 OpenMPI/4.1.1 module load R/4.2.0 Rscript mytest.R Save this to the current working directory, and then create another file using your preferred text editor called mytest.R (or another name of your choice) containing the following R commands: library ( tidyverse ) sprintf ( \"Hello World!\" ) then run it with the previously written bash script: sbatch r_submit.sh This submits a task that should execute quickly and create files in the directory from which it was run. Examine r_test.out . You can use an editor like nano, vi or emacs, or you can just cat or less the file to see its contents on the terminal. You should see: \"Hello World\" Installing additional R packages/extensions in your local user directory \u00b6 If you are in need of additional R packages which are not included in the R installation, you may intall them into your user directory. Start by launching an R session module purge module load GCC/11.2.0 OpenMPI/4.1.1 module load R/4.2.0 R Then, supposing you want to install a package from CRAN named \"A3\". If this is the first time you are attempting to install local packages for this R version then the steps look something like this. > library ( A3 ) # confirm that foo is not already available > install.packages ( 'A3' ) Warning in install.packages ( \"A3\" ) : 'lib = \"/home/software/EasyBuild/software/R/4.2.0-foss-2021b/lib64/R/library\"' is not writable Would you like to use a personal library instead ? ( yes / No / cancel ) yes Would you like to create a personal library \u2018 / nfs / home /< username >/ R / x86_64 - pc - linux - gnu - library / 4.2 \u2019 to install packages into ? ( yes / No / cancel ) yes --- Please select a CRAN mirror for use in this session --- Secure CRAN mirrors < long list of mirrors , the NZ mirror was number 54 in my list > Selection : 54 trying URL 'https://cran.stat.auckland.ac.nz/src/contrib/A3_1.0.0.tar.gz' < additional output from the installation steps... > In future, when you run this version of R, it should automatically check the local user directory created above for installed packages. Any other packages you install in future should automatically go into this directory as well (assuming you don't play around with .libPaths() ).","title":"R users guide"},{"location":"examples/R_users_guide/#r-users-guide","text":"","title":"R users guide"},{"location":"examples/R_users_guide/#which-versions-of-r-are-working-on-rapoi","text":"There are a number of versions of R on R\u0101poi, although many of these are old installations (prior to an OS update and changes to the module system) and no longer work. There are three relatively recent versions of R which currently work (as of April 2024), these are R/3.6.3 , R/4.1.0 and R/4.2.0 . Each of these modules has a couple of pre-requisite modules that need to be loaded prior to loading the R module. To find out what you need to load first you just need to check the output of module spider R/x.y.z (with the appropriate values for x,y,z). The following example shows how to use R/4.2.0 .","title":"Which versions of R are working on R\u0101poi?"},{"location":"examples/R_users_guide/#loading-r-packages-running-a-simple-job","text":"First login to R\u0101poi and load the R module: module purge # clean/reset your environment module load config # reload utilities such as vuw-job-report module load GCC/11.2.0 OpenMPI/4.1.1 # pre-requisites for the new R module module load R/4.2.0 Then run R on the command line: R Test library existence: > library ( tidyverse ) This should load the package, and give some output like this: \u2500\u2500 Attaching packages \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 tidyverse 1.3 . 1 \u2500\u2500 \u2714 ggplot2 3.3 . 5 \u2714 purrr 0.3 . 4 \u2714 tibble 3.1 . 6 \u2714 dplyr 1.0 . 8 \u2714 tidyr 1.2 . 0 \u2714 stringr 1.4 . 0 \u2714 readr 2.1 . 2 \u2714 forcats 0.5 . 1 \u2500\u2500 Conflicts \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 tidyverse_conflicts () \u2500\u2500 \u2716 dplyr :: filter () masks stats :: filter () \u2716 dplyr :: lag () masks stats :: lag () (These conflicts are normal and can be ignored.) To quit R, type: > q () Next create a bash submission script called r_submit.sh (or another name of your choice) using your preferred text editor, e.g. nano. #!/bin/bash # #SBATCH --job-name=r_test #SBATCH -o r_test.out #SBATCH -e r_test.err # #SBATCH --cpus-per-task=2 #Note: you are always allocated an even number of cpus #SBATCH --mem=1G #SBATCH --time=10:00 module purge module load GCC/11.2.0 OpenMPI/4.1.1 module load R/4.2.0 Rscript mytest.R Save this to the current working directory, and then create another file using your preferred text editor called mytest.R (or another name of your choice) containing the following R commands: library ( tidyverse ) sprintf ( \"Hello World!\" ) then run it with the previously written bash script: sbatch r_submit.sh This submits a task that should execute quickly and create files in the directory from which it was run. Examine r_test.out . You can use an editor like nano, vi or emacs, or you can just cat or less the file to see its contents on the terminal. You should see: \"Hello World\"","title":"Loading R packages &amp; running a simple job"},{"location":"examples/R_users_guide/#installing-additional-r-packagesextensions-in-your-local-user-directory","text":"If you are in need of additional R packages which are not included in the R installation, you may intall them into your user directory. Start by launching an R session module purge module load GCC/11.2.0 OpenMPI/4.1.1 module load R/4.2.0 R Then, supposing you want to install a package from CRAN named \"A3\". If this is the first time you are attempting to install local packages for this R version then the steps look something like this. > library ( A3 ) # confirm that foo is not already available > install.packages ( 'A3' ) Warning in install.packages ( \"A3\" ) : 'lib = \"/home/software/EasyBuild/software/R/4.2.0-foss-2021b/lib64/R/library\"' is not writable Would you like to use a personal library instead ? ( yes / No / cancel ) yes Would you like to create a personal library \u2018 / nfs / home /< username >/ R / x86_64 - pc - linux - gnu - library / 4.2 \u2019 to install packages into ? ( yes / No / cancel ) yes --- Please select a CRAN mirror for use in this session --- Secure CRAN mirrors < long list of mirrors , the NZ mirror was number 54 in my list > Selection : 54 trying URL 'https://cran.stat.auckland.ac.nz/src/contrib/A3_1.0.0.tar.gz' < additional output from the installation steps... > In future, when you run this version of R, it should automatically check the local user directory created above for installed packages. Any other packages you install in future should automatically go into this directory as well (assuming you don't play around with .libPaths() ).","title":"Installing additional R packages/extensions in your local user directory"},{"location":"examples/Singularity/","text":"Singularity \u00b6 While there are many modules on R\u0101poi, sometimes you might want to install your own packages in your own way. Singularity allows you to do this. If you are familiar with Docker, Singularity is similar, except you can't get root (or sudo) once your container is running on the R\u0101poi. However, you can have sudo rights locally on your own machine, setup your container however you like, then run it without sudo on the cluster. Note As per updated singularity configuration, mount hostfs is set to false and mount home is set to true . As a result, filesystems available inside a singularity session by default are: devtmpfs /dev tmpfs /dev/shm /vg<id> /etc/hosts .. /tmp .. /var/tmp .. /etc/group $HOME /nfs/home/$USER This means any other file system such as nfs should be manually specified using --bind command. For example, singularity shell --bind /nfs/scratch/$USER:/nfs/scratch/$USER image_file.simg pwd See also: Using containers Singularity/Docker container example \u00b6 Singularity allows you to use most (but not all!) docker images on R\u0101poi. On your local machine create the singularity definition file input_args_example.def BootStrap : library From : ubuntu: 16.04 %runscript exec echo \" $@ \" %labels Author JaneDoe This will build an ubuntu 16.04 container that will eventually run on R\u0101poi which runs Centos. This container has a runscript which just echos back any arguments sent to the container when your start it up. Build the container locally with sudo and singularity sudo singularity build inputexample.sif input_args_example.def OR Using Sylabs Remote Builder is another option to build containers remotely. A Sylabs account and access token are required to use this feature. To build the container remotely, use the following command: singularity build --remote inputexample.sif input_args_example.def This will build an image that you can't modify any further and is immediately suitable to run on R\u0101poi Copy this file to R\u0101poi via sftp sftp <username>@raapoi.vuw.ac.nz Create a submit script using singularity on the cluster singularity_submit.sh #!/bin/bash #SBATCH --job-name=singularity_test #SBATCH -o sing_test.out #SBATCH -e sing_test.err #SBATCH --time=00:00:20 #SBATCH --ntasks=1 #SBATCH --mem=1G module load singularity singularity run inputtest.sif \"hello from a container\" Run the script with the usual singularity_submit.sh Singularity/TensorFlow Example \u00b6 tensor.def Bootstrap: docker From: tensorflow/tensorflow:latest-py3 %post apt-get update && apt-get -y install wget build-essential %runscript exec python \" $@ \" compile this locally with sudo and singularity. sudo singularity build tensorflow.sif tensor.def Create a quick tensorflow test code tensortest.py import tensorflow as tf mnist = tf . keras . datasets . mnist ( x_train , y_train ),( x_test , y_test ) = mnist . load_data () x_train , x_test = x_train / 255.0 , x_test / 255.0 model = tf . keras . models . Sequential ([ tf . keras . layers . Flatten ( input_shape = ( 28 , 28 )), tf . keras . layers . Dense ( 512 , activation = tf . nn . relu ), tf . keras . layers . Dropout ( 0.2 ), tf . keras . layers . Dense ( 10 , activation = tf . nn . softmax ) ]) model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) model . fit ( x_train , y_train , epochs = 5 ) model . evaluate ( x_test , y_test ) Copy your files to R\u0101poi via sftp (or whatever you prefer) sftp <username>@raapoi.vuw.ac.nz cd <where you want to work> put * #put all files in your local directory onto R\u0101poi Lets quickly test the code via an interactive session on a node. Note I find the tensorflow container only runs properly on intel nodes, which we don't have many of at the moment, I'll investigate this further. srun --partition = \"parallel\" --constraint = \"Intel\" --pty bash #now on the remote node - note you might need to wait if nodes are busy module load singularity #load singularity singularity shell tensorflow.sif #now inside the tensorflow container on the remote node python tensortest.py #once that runs, exit the container exit #exit the container exit #exit the interactive session on the node Create a submit script using singularity on the cluster singularity_submit.sh #!/bin/bash #SBATCH --job-name=singularity_test #SBATCH -o sing_test.out #SBATCH -e sing_test.err #SBATCH --time=00:10:00 #SBATCH --partition=parallel #SBATCH --constraint=Intel #SBATCH --ntasks=1 #SBATCH --mem=4G module load singularity #run the container with the runscript defined when we created it singularity run tensorflow.sif tensortest.py Singularity/MaxBin2 Example \u00b6 In a sensible location, either in your home directory or on the scratch: Get the maxbin2 container, there are a few places to get this, but will get the bioconda container as it is more recent than the one referenced on the official maxbin site. module load module load singularity singularity pull docker://quay.io/biocontainers/maxbin2:2.2.6--h14c3975_0 mv maxbin2_2.2.6--h14c3975_0.sif maxbin2_2.2.6.sif #rename for convenience Download some test data mkdir rawdata curl https://downloads.jbei.org/data/microbial_communities/MaxBin/getfile.php?20x.scaffold > rawdata/20x.scaffold curl https://downloads.jbei.org/data/microbial_communities/MaxBin/getfile.php?20x.abund > rawdata/20x.abund Create an output data location mkdir output Create a submit script using singularity on the cluster singularity_submit.sh #!/bin/bash #SBATCH --job-name=maxbin2_test #SBATCH -o sing_test.out #SBATCH -e sing_test.err #SBATCH --time=00:10:00 #SBATCH --partition=parallel #SBATCH --ntasks=4 #SBATCH --mem=4G module load singularity singularity exec maxbin2_2.2.6.sif run_MaxBin.pl -contig rawdata/20x.scaffold -abund rawdata/20x.abund -out output/20x.out -thread 4 Singularity/Sandbox Example \u00b6 This lets you have root inside a container locally and make changes to it. This is really handy for determining how to setuop your container. While you can convert the sandbox container to one you can run on R\u0101poi, I suggest you don't do this . Use the sandbox to figure out how you need to configure your container, what packages to install, config files to change etc. Then create a .def file that contains all the nessesary steps without the need to use the sandbox - this will make your work more reproducable and easier to share with others. example.def BootStrap: library From: ubuntu:16.04 %post apt-get update && apt-get -y install wget build-essential %runscript exec echo \" $@ \" %labels Author JaneDoe Compile this locally with sudo and singularity. We are using the sandbox flag to create a writable container directory ( example/ ) on our local machine where we have sudo rights. sudo singularity build --sandbox example/ example.def Now we can run the container we just built, but with sudo rights inside the container. Your rights outside the container match the rights inside the container, so we need to do this with sudo. sudo singularity shell --writable example/ Inside the container we now have root and can install packages and modify files in the root directories Singularity example:~> apt update Singularity example:~> apt install sqlite Singularity example:~> touch /test.txt #create an empty file in root Singularity example:~> ls / Singularity example:~> exit #exit container To run the container on R\u0101poi we convert it to the default immutable image with build. We might need sudo for this as the prior use of sudo will have created a directory that your usual user can't see every file. sudo singularity build new-example-sif example/ You could now copy the new-example-sif file to R\u0101poi and run it there. However a better workflow is to use this to experiment, to find out what changes you need to make to the image and what packages you need to install. Once you've done that, I suggest starting afresh and putting everything in the.def file . That way when you return to your project in 6 months, or hand it over to someone else, there is a clear record of how the image was built. Singularity/Custom Conda Container - idba example \u00b6 In this example we'll build a singularity container using conda. The example is building a container for idba - a genome assembler. Idba is available in bioconda, but not as a biocontainer. We'll build this container locally to match a local conda environment, then run it on the HPC and do an example assembly. Locally \u00b6 Make sure you have conda setup on your local machine, anaconda and miniconda are good choices. Create a new conda environment and install idba conda create --name idba conda install -c bioconda idba Export your conda environment, we will use this to build the container. conda env export > environment.yml We will use a singularity definition, basing our build on a docker miniconda image. There is a bunch of stuff in this file to make sure the conda environment is in the path. From stackoverflow idba.def Bootstrap: docker From: continuumio/miniconda3 %files environment.yml %environment PATH=/opt/conda/envs/$(head -1 environment.yml | cut -d' ' -f2)/bin:$PATH %post echo \". /opt/conda/etc/profile.d/conda.sh\" >> ~/.bashrc echo \"source activate $(head -1 environment.yml | cut -d' ' -f2)\" > ~/.bashrc /opt/conda/bin/conda env create -f environment.yml %runscript exec \"$@\" Build the image sudo singularity build idba.img idba.def Now copy the idba.img and environment.yml (technically the environment file is not needed, but not having it creates a warning) to somewhere sensible on R\u0101poi. On R\u0101poi \u00b6 Create a data directory, so we can separate our inputs and outputs. Download a paired end illumina read of Ecoli from S3 with wget. The data comes from the Illumina public data library mkdir data cd data wget --content-disposition goo.gl/JDJTaz #sequence data wget --content-disposition goo.gl/tt9fsn #sequence data cd .. #back to our project directory The reads we have are paired end fastq files but idba requires a fasta file. We can use a tool built into our container to convert them. We'll do this on the R\u0101poi login node as it is a fast task that doesn't need many resources. module load singularity singularity exec fq2fa --merge --filter data/MiSeq_Ecoli_MG1655_50x_R1.fastq data/MiSeq_Ecoli_MG1655_50x_R2.fastq data/read.fa Create our sbatch submission script. Note that this sequence doesn't need a lot of memory, so we'll use 1G. To see your usage after the job has run use vuw-job-report <job-id> idba_submit.sh #!/bin/bash #SBATCH --job-name=idba_test #SBATCH -o output.out #SBATCH -e output.err #SBATCH --time=00:10:00 #SBATCH --partition=quicktest #SBATCH --ntasks=12 #SBATCH --mem=1G module load singularity singularity exec idba.img idba idba_ud -r data/read.fa -o output Now we can submit our script to the queue with sbatch idba_submit.sh","title":"Singularity"},{"location":"examples/Singularity/#singularity","text":"While there are many modules on R\u0101poi, sometimes you might want to install your own packages in your own way. Singularity allows you to do this. If you are familiar with Docker, Singularity is similar, except you can't get root (or sudo) once your container is running on the R\u0101poi. However, you can have sudo rights locally on your own machine, setup your container however you like, then run it without sudo on the cluster. Note As per updated singularity configuration, mount hostfs is set to false and mount home is set to true . As a result, filesystems available inside a singularity session by default are: devtmpfs /dev tmpfs /dev/shm /vg<id> /etc/hosts .. /tmp .. /var/tmp .. /etc/group $HOME /nfs/home/$USER This means any other file system such as nfs should be manually specified using --bind command. For example, singularity shell --bind /nfs/scratch/$USER:/nfs/scratch/$USER image_file.simg pwd See also: Using containers","title":"Singularity"},{"location":"examples/Singularity/#singularitydocker-container-example","text":"Singularity allows you to use most (but not all!) docker images on R\u0101poi. On your local machine create the singularity definition file input_args_example.def BootStrap : library From : ubuntu: 16.04 %runscript exec echo \" $@ \" %labels Author JaneDoe This will build an ubuntu 16.04 container that will eventually run on R\u0101poi which runs Centos. This container has a runscript which just echos back any arguments sent to the container when your start it up. Build the container locally with sudo and singularity sudo singularity build inputexample.sif input_args_example.def OR Using Sylabs Remote Builder is another option to build containers remotely. A Sylabs account and access token are required to use this feature. To build the container remotely, use the following command: singularity build --remote inputexample.sif input_args_example.def This will build an image that you can't modify any further and is immediately suitable to run on R\u0101poi Copy this file to R\u0101poi via sftp sftp <username>@raapoi.vuw.ac.nz Create a submit script using singularity on the cluster singularity_submit.sh #!/bin/bash #SBATCH --job-name=singularity_test #SBATCH -o sing_test.out #SBATCH -e sing_test.err #SBATCH --time=00:00:20 #SBATCH --ntasks=1 #SBATCH --mem=1G module load singularity singularity run inputtest.sif \"hello from a container\" Run the script with the usual singularity_submit.sh","title":"Singularity/Docker container example"},{"location":"examples/Singularity/#singularitytensorflow-example","text":"tensor.def Bootstrap: docker From: tensorflow/tensorflow:latest-py3 %post apt-get update && apt-get -y install wget build-essential %runscript exec python \" $@ \" compile this locally with sudo and singularity. sudo singularity build tensorflow.sif tensor.def Create a quick tensorflow test code tensortest.py import tensorflow as tf mnist = tf . keras . datasets . mnist ( x_train , y_train ),( x_test , y_test ) = mnist . load_data () x_train , x_test = x_train / 255.0 , x_test / 255.0 model = tf . keras . models . Sequential ([ tf . keras . layers . Flatten ( input_shape = ( 28 , 28 )), tf . keras . layers . Dense ( 512 , activation = tf . nn . relu ), tf . keras . layers . Dropout ( 0.2 ), tf . keras . layers . Dense ( 10 , activation = tf . nn . softmax ) ]) model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) model . fit ( x_train , y_train , epochs = 5 ) model . evaluate ( x_test , y_test ) Copy your files to R\u0101poi via sftp (or whatever you prefer) sftp <username>@raapoi.vuw.ac.nz cd <where you want to work> put * #put all files in your local directory onto R\u0101poi Lets quickly test the code via an interactive session on a node. Note I find the tensorflow container only runs properly on intel nodes, which we don't have many of at the moment, I'll investigate this further. srun --partition = \"parallel\" --constraint = \"Intel\" --pty bash #now on the remote node - note you might need to wait if nodes are busy module load singularity #load singularity singularity shell tensorflow.sif #now inside the tensorflow container on the remote node python tensortest.py #once that runs, exit the container exit #exit the container exit #exit the interactive session on the node Create a submit script using singularity on the cluster singularity_submit.sh #!/bin/bash #SBATCH --job-name=singularity_test #SBATCH -o sing_test.out #SBATCH -e sing_test.err #SBATCH --time=00:10:00 #SBATCH --partition=parallel #SBATCH --constraint=Intel #SBATCH --ntasks=1 #SBATCH --mem=4G module load singularity #run the container with the runscript defined when we created it singularity run tensorflow.sif tensortest.py","title":"Singularity/TensorFlow Example"},{"location":"examples/Singularity/#singularitymaxbin2-example","text":"In a sensible location, either in your home directory or on the scratch: Get the maxbin2 container, there are a few places to get this, but will get the bioconda container as it is more recent than the one referenced on the official maxbin site. module load module load singularity singularity pull docker://quay.io/biocontainers/maxbin2:2.2.6--h14c3975_0 mv maxbin2_2.2.6--h14c3975_0.sif maxbin2_2.2.6.sif #rename for convenience Download some test data mkdir rawdata curl https://downloads.jbei.org/data/microbial_communities/MaxBin/getfile.php?20x.scaffold > rawdata/20x.scaffold curl https://downloads.jbei.org/data/microbial_communities/MaxBin/getfile.php?20x.abund > rawdata/20x.abund Create an output data location mkdir output Create a submit script using singularity on the cluster singularity_submit.sh #!/bin/bash #SBATCH --job-name=maxbin2_test #SBATCH -o sing_test.out #SBATCH -e sing_test.err #SBATCH --time=00:10:00 #SBATCH --partition=parallel #SBATCH --ntasks=4 #SBATCH --mem=4G module load singularity singularity exec maxbin2_2.2.6.sif run_MaxBin.pl -contig rawdata/20x.scaffold -abund rawdata/20x.abund -out output/20x.out -thread 4","title":"Singularity/MaxBin2 Example"},{"location":"examples/Singularity/#singularitysandbox-example","text":"This lets you have root inside a container locally and make changes to it. This is really handy for determining how to setuop your container. While you can convert the sandbox container to one you can run on R\u0101poi, I suggest you don't do this . Use the sandbox to figure out how you need to configure your container, what packages to install, config files to change etc. Then create a .def file that contains all the nessesary steps without the need to use the sandbox - this will make your work more reproducable and easier to share with others. example.def BootStrap: library From: ubuntu:16.04 %post apt-get update && apt-get -y install wget build-essential %runscript exec echo \" $@ \" %labels Author JaneDoe Compile this locally with sudo and singularity. We are using the sandbox flag to create a writable container directory ( example/ ) on our local machine where we have sudo rights. sudo singularity build --sandbox example/ example.def Now we can run the container we just built, but with sudo rights inside the container. Your rights outside the container match the rights inside the container, so we need to do this with sudo. sudo singularity shell --writable example/ Inside the container we now have root and can install packages and modify files in the root directories Singularity example:~> apt update Singularity example:~> apt install sqlite Singularity example:~> touch /test.txt #create an empty file in root Singularity example:~> ls / Singularity example:~> exit #exit container To run the container on R\u0101poi we convert it to the default immutable image with build. We might need sudo for this as the prior use of sudo will have created a directory that your usual user can't see every file. sudo singularity build new-example-sif example/ You could now copy the new-example-sif file to R\u0101poi and run it there. However a better workflow is to use this to experiment, to find out what changes you need to make to the image and what packages you need to install. Once you've done that, I suggest starting afresh and putting everything in the.def file . That way when you return to your project in 6 months, or hand it over to someone else, there is a clear record of how the image was built.","title":"Singularity/Sandbox Example"},{"location":"examples/Singularity/#singularitycustom-conda-container-idba-example","text":"In this example we'll build a singularity container using conda. The example is building a container for idba - a genome assembler. Idba is available in bioconda, but not as a biocontainer. We'll build this container locally to match a local conda environment, then run it on the HPC and do an example assembly.","title":"Singularity/Custom Conda Container - idba example"},{"location":"examples/anaconda/","text":"Using Anaconda/Miniconda/conda \u00b6 Many users use Anaconda/Miniconda to manage software stacks. One way to do this is to use singularity containers with the conda environment inside - this allows the conda environment to load quickly as the many small conda files are inside a container which the file system sees as one file. However, this is also an additional bit of complexity so many users just use conda outside of singularity. You can install your own version of Anaconda/Miniconda to your home directory or scratch. We have also got packaged versions of Anaconda/Miniconda installed with our module loading system. Anaconda has many built in packages so we will use that in our examples, but Miniconda is also available if prefer to start from a minimal initial setup. module load Anaconda3/2020.11 export PIP_NO_CACHE_DIR = 1 export PYTHONNOUSERSITE = 1 Note Setting the variables PIP_NO_CACHE_DIR and PYTHONNOUSERSITE prevents conda from trying to use the system python and pip. This ensures an isolated environment and avoids potential conflicts with system packages. Let's create a new conda environment for this example, in a sensible location, I used ~/examples/conda/idba conda create --name idba-example # press y for the Proceed prompt if it looks correct source $( conda info --base ) /etc/profile.d/conda.sh conda activate idba-example #activate our example environment. Warning On HPC systems, conda init is not recommended as it modifies your shell configuration files. This can cause problems with the module system and other software. Instead, use the source $(conda info --base)/etc/profile.d/conda.sh command to activate conda in your current shell session. Conda environments are beyond the scope of this example, but they are a good way to contain all the dependencies and programs for a particular workflow, in this case, idba. Install idba in our conda environment. Tip Note that best practise is to do the install on a compute node We'll just do it here on the login node for now - the code will run slower on the compute nodes as a result! conda install -c bioconda idba Idba is a genome assembler, we will use paired-end illumina reads of E. coli. The data is available on an Amazon S3 bucket (a cloud storage location), and we can download it using wget. mkdir data # put our data in a sensible location cd data wget --content-disposition goo.gl/JDJTaz #sequence data wget --content-disposition goo.gl/tt9fsn #sequence data cd .. #back to our project directory The reads we have are paired-end fastq files but idba requires a fasta file. We can use a tool installed with idba to convert them. We'll do this on the R\u0101poi login node as it is a fast task that doesn't need many resources. fq2fa --merge --filter data/MiSeq_Ecoli_MG1655_50x_R1.fastq data/MiSeq_Ecoli_MG1655_50x_R2.fastq data/read.fa To create our submission script we need to know the path to our conda enviroment. To get this: conda env list You'll need to find your idba-example environment, and next to it is the path you'll need for your submission script. In my case: # conda environments: # base * /home/username/anaconda3 idba-example /home/username/anaconda3/envs/idba-example # We need this line, it'll be different for you! Create our sbatch submission script. Note that this sequence doesn't need a lot of memory, so we'll use 3G. To see your usage after the job has run use vuw-job-report <job-id> idba_submit.sh #!/bin/bash #SBATCH --job-name=idba_test #SBATCH -o _output.out #SBATCH -e _output.err #SBATCH --time=00:5:00 #SBATCH --partition=quicktest #SBATCH --ntasks=12 #SBATCH --mem=3G module load Anaconda3/2020.11 eval \" $( conda shell.bash hook ) \" # basically inits your conda - prevents errors like: CommandNotFoundError: Your shell has not been properly configured ... conda activate /home/username/anaconda3/envs/idba-example # We will need to activate our conda enviroment on the remote node idba idba_ud -r data/read.fa -o output To submit our job sbatch idba_submit.sh To see our job running or queuing squeue -u $USER This job will take a few minutes to run, generally less than 5. When the job is done we can see the output in the output folder. We can also see the std output and std err in the files _output.out and _output.err . The quickest way to examine them is to cat the files when the run is done. cat _output.out","title":"Anaconda"},{"location":"examples/anaconda/#using-anacondaminicondaconda","text":"Many users use Anaconda/Miniconda to manage software stacks. One way to do this is to use singularity containers with the conda environment inside - this allows the conda environment to load quickly as the many small conda files are inside a container which the file system sees as one file. However, this is also an additional bit of complexity so many users just use conda outside of singularity. You can install your own version of Anaconda/Miniconda to your home directory or scratch. We have also got packaged versions of Anaconda/Miniconda installed with our module loading system. Anaconda has many built in packages so we will use that in our examples, but Miniconda is also available if prefer to start from a minimal initial setup. module load Anaconda3/2020.11 export PIP_NO_CACHE_DIR = 1 export PYTHONNOUSERSITE = 1 Note Setting the variables PIP_NO_CACHE_DIR and PYTHONNOUSERSITE prevents conda from trying to use the system python and pip. This ensures an isolated environment and avoids potential conflicts with system packages. Let's create a new conda environment for this example, in a sensible location, I used ~/examples/conda/idba conda create --name idba-example # press y for the Proceed prompt if it looks correct source $( conda info --base ) /etc/profile.d/conda.sh conda activate idba-example #activate our example environment. Warning On HPC systems, conda init is not recommended as it modifies your shell configuration files. This can cause problems with the module system and other software. Instead, use the source $(conda info --base)/etc/profile.d/conda.sh command to activate conda in your current shell session. Conda environments are beyond the scope of this example, but they are a good way to contain all the dependencies and programs for a particular workflow, in this case, idba. Install idba in our conda environment. Tip Note that best practise is to do the install on a compute node We'll just do it here on the login node for now - the code will run slower on the compute nodes as a result! conda install -c bioconda idba Idba is a genome assembler, we will use paired-end illumina reads of E. coli. The data is available on an Amazon S3 bucket (a cloud storage location), and we can download it using wget. mkdir data # put our data in a sensible location cd data wget --content-disposition goo.gl/JDJTaz #sequence data wget --content-disposition goo.gl/tt9fsn #sequence data cd .. #back to our project directory The reads we have are paired-end fastq files but idba requires a fasta file. We can use a tool installed with idba to convert them. We'll do this on the R\u0101poi login node as it is a fast task that doesn't need many resources. fq2fa --merge --filter data/MiSeq_Ecoli_MG1655_50x_R1.fastq data/MiSeq_Ecoli_MG1655_50x_R2.fastq data/read.fa To create our submission script we need to know the path to our conda enviroment. To get this: conda env list You'll need to find your idba-example environment, and next to it is the path you'll need for your submission script. In my case: # conda environments: # base * /home/username/anaconda3 idba-example /home/username/anaconda3/envs/idba-example # We need this line, it'll be different for you! Create our sbatch submission script. Note that this sequence doesn't need a lot of memory, so we'll use 3G. To see your usage after the job has run use vuw-job-report <job-id> idba_submit.sh #!/bin/bash #SBATCH --job-name=idba_test #SBATCH -o _output.out #SBATCH -e _output.err #SBATCH --time=00:5:00 #SBATCH --partition=quicktest #SBATCH --ntasks=12 #SBATCH --mem=3G module load Anaconda3/2020.11 eval \" $( conda shell.bash hook ) \" # basically inits your conda - prevents errors like: CommandNotFoundError: Your shell has not been properly configured ... conda activate /home/username/anaconda3/envs/idba-example # We will need to activate our conda enviroment on the remote node idba idba_ud -r data/read.fa -o output To submit our job sbatch idba_submit.sh To see our job running or queuing squeue -u $USER This job will take a few minutes to run, generally less than 5. When the job is done we can see the output in the output folder. We can also see the std output and std err in the files _output.out and _output.err . The quickest way to examine them is to cat the files when the run is done. cat _output.out","title":"Using Anaconda/Miniconda/conda"},{"location":"external/cloud_providers/","text":"Connecting to Cloud Providers \u00b6 AARNET Cloudstor \u00b6 NOTE Cloudstor service has been decommissioned since Dec 2023. For old accounts, please check access with the provider. All VUW researchers have access to the AARNET (Australia\u2019s Academic and Research Network) Cloudstor service which provides 1 TB of space to each researcher. To use this service first login and download an appropriate client to your laptop or desktop (or smarthone if you wish): Cloudstor Login NOTE: Within the Cloudstor web login settings you will need to create a Cloudstor Password, this is the password you will use to login on R\u0101poi, it does not use your VUW credentials for the command line login. We suggest setting up an App Password for Raapoi-rcopy rather than a global sync password. This way if your password is compromised you can easily just remove that app password. Setup an App Password by clicky on the settings gear on the top right and finding the App Password link . Once you have setup your cloudstor (aka ownCloud) credentials you can use them to sync data to and from R\u0101poi. For example, if I wanted to sync my project space to Cloudstor I would do the following from R\u0101poi login node: # Use Tmux to keep your session alive if you disconnect. You can reconnect to your Tmux session if you reconnect. See Tmux docs. tmux # Use our new module system module use /home/software/tools/eb_modulefiles/all/Core module load rclone/1.54.1 #check if cloudstor remote is already configured rclone listremotes The above sequence starts a tmux session to allow the transfer to continue even if I disconnect from the cluster (type tmux attach to reconnect) and then loads the rcopy module - which requires the use of our new module system. If you don't already have CloudStor configured as a remote (which you won't if this is your first time using it) follow the instructions on aarnet docs page . Once we have setup rclone to connect to CloudStor, we copy our data. In this case from <my scratch folder>/test to test on CloudStor rclone copy --progress --transfers 8 /nfs/scratch/<username>/test CloudStor:/test Amazon AWS \u00b6 Create a python environment and install awscli module. python3 -m pip install awscli Before you proceed you will need to configure your environment with your Access Key ID and Secret Access Key, both of which will be sent to you once your account is created or linked. The command to configure your environment is aws configure You only need to do this once, unless of course you use more than one user/Access Key. Most users can simply click through the region and profile questions (using the default of \"none\"). If you do have a specific region this should be relayed along with your access and secret keys. Once you have the appropriate environment in place and your configuration setup you can use the aws command, plus an appropriate sub-command (s3, emr, rds, dynamodb, etc) and supporting arguments. More information on the CLI can be found here: http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-using.html Transferring Data to/from Amazon (AWS) S3 \u00b6 To transfer data from S3 you first need to setup your AWS connect, instructions for that can be found above. Once that is done you should be able to use the aws commands to copy data to and from your S3 storage. For example if I wanted to copy data from my S3 storage to my project directory I could do the following: tmux module load amazon/aws/cli cd /nfs/scratch/<username>/project aws s3 cp s3://mybucket/mydata.dat mydata.dat To copy something to storage simply reverse the file paths, eg. aws s3 cp mydata.dat s3://mybucket/mydata.dat The above starts a tmux session to allow the transfer to continue even if I disconnect from the cluster (type tmux attach to reconnect). I then load the modules necessary to use the AWS commands. I change directory to my project space and use the aws s3 cp command to copy from S3. More information on using aws can be found here: http://docs.aws.amazon.com/cli/latest/reference/s3/index.html#cli-aws-s3 Working with AWS Data Analysis Tools \u00b6 Amazon has a number of data analytics and database services available. Using the command line utilities available in R\u0101poi, researchers can perform work on the eo cluster and transfer data to AWS to perform further analysis with tools such as MapReduce (aka Hadoop), RedShift or Quicksight. A listing of available services and documentation can be found at the following: https://aws.amazon.com/products/analytics/ Google Cloud (gcloud) Connections \u00b6 The Google Cloud SDK is available in R\u0101poi. This includes a command-line interface for connecting to gloud services. To get started, first load the environment module. You can find the path with the module spider command. As of this writing the current version can be loaded thusly: module load gcloud/481.0.0 This will give you access to the gcloud command. To setup a connection to your gcloud account use the init sub-command, eg. gcloud init --console-only Follow the instructions to authorize your gcloud account. Once on the Google website, you will be given an authorization code which you will copy/paste back into the R\u0101poi terminal window. Transferring Data to/from Google Cloud (gcloud) \u00b6 To transfer data from gcloud storage you first need to setup your gcloud credentials, instructions for that can be found above. Once that is done you should be able to use the gsutil command to copy data to and from your gcloud storage. For example, if I wanted to copy data from gcloud to my project directory I could do the following: tmux module load gcloud/481.0.0 cd /nfs/scratch/<username>/project gsutil cp gs://mybucket/mydata.dat mydata.dat To copy something to storage simply reverse the file paths, eg. gsutil cp mydata.dat gs://mybucket/mydata.dat The above starts a tmux session to allow the transfer to continue even if I disconnect from the cluster (type tmux attach to reconnect). I then load the modules necessary to use the gsutil commands. I change directory to my project space and use the gsutil cp command to copy from gcloud. More information on using gcloud can be found here: https://cloud.google.com/sdk/gcloud/ Working with GCloud Data Analysis Tools \u00b6 Google Cloud has a number of data analytics and database services available. Using the gcloud command line utilities available on R\u0101poi, researchers can perform work on the cluster and transfer data to gcloud to perform further analysis with tools such as Dataproc (Hadoop/Spark), BigQuery or Datalab (Visualization) A listing of available services and documentation can be found at the following: https://cloud.google.com/products/ DropBox Cloud Storage \u00b6 Upload/Download Limits with DropBox Dropbox has upload/download limitations and we have found that once your file gets above 50GB in size the transfer will have a better chance of timing out and failing. Configuring your Dropbox account on R\u0101poi Step A: On your local laptop or desktop start your browser and login to your Dropbox account Step B: On R\u0101poi type the following: module load dropbox Step C: Setup account credentials (You should only need to do this once): Run the following command from R\u0101poi dbxcli account You will now see something like the following: Go to https://www.dropbox.com/1/oauth2/authorize?client_id=X123... Click \"Allow\" (you might have to log in first). Copy the authorization code. Enter the authorization code here: Step D: Copy the URL link listed in Step C1 and paste it into the web browser that you started in Step A This will provide you with a long access code (aka hash). Now copy that access code and paste it into your R\u0101poi terminal after Step C3 where it is asking for Enter the authorization code here Now hit enter or return. You should see that you are now logged in with your Dropbox credentials Basic Dropbox commands \u00b6 Remember to load the dropbox environment module if you have not already (see module spider for the path) Now type dbx or dbxcli at a prompt. You will see a number of sub-commands, for instance ls, which will list the contents of your Dropbox, eg dbxcli ls Downloading from Dropbox \u00b6 Downloading uses the subcommand called: get. The basic format for get is: dbxcli get fileOnDropbox fileOnRaapoi For instance, if I have a datafile called 2018-financials.csv on Dropbox that I want to copy to my project folder I would type: dbxcli get 2018-financials.csv /nfs/scratch/<username>/projects\\ /finance_proj/2018-financials.csv Uploading to Dropbox \u00b6 Uploading is similar to downloading except now we use the subcommand: put. The basic format for put is: dbxcli put fileOnRaapoi fileOnDropbox For example I want to upload a PDF I generated from one of my jobs called final-report.pdf I would type: dbxcli put final-report.pdf final-report.pdf This will upload the PDF and name it the same thing, if I wanted to change the name on Dropbox I could: dbxcli put final-report.pdf analytics-class-final-report.pdf Microsoft OneDrive \u00b6 RClone can be used to connect to onedrive, google drive, etc. The steps below implement onedrive setup on R\u0101poi. module load rclone/1.54.1 rclone config Follow the on-screen instructions, e.g., make a new remote - enter \"n\"; to select storage type; \"onedrive\" and keep following default options until \"Use auto config\" and enter \"y\" here. This should display a url; copy and paste in the browser and it should get set up. To view your files on the remote onedrive, you can use: # For example, rclone lsd <remote>:<dir_name>, in my case , I would do : rclone lsd my_staff_onedrive:Documents GLOBUS \u00b6 External sharing coming soon! Presently, users can share/transfer data to and from locations including cluster, research storage, and personal devices. Options for enabling data sharing externally are being explored. First time users should be able to sign up on GLOBUS website using their university credentials by following this link: https://app.globus.org/ . Please install and start GLOBUS connect personal. Please find those instructions here: https://globus.org/globus-connect-personal You should now be able to share or transfer your data by following their guide: https://docs.globus.org/guides/tutorials/manage-files/share-files/ . Transfer files using Globus \u00b6 Globus works by seting up endpoints which are like locations for data transfer. This means, we will need two endpoints to transfer data; one - a source, and the other - a destination. The steps below will walk you through the Installation and then navigating the Globus File Manager . The installation process is same for both the globusconnectpersonal setup and for any of the R\u0101poi's compute nodes . Installation \u00b6 Working on a Windows machine? The steps below are for a non-windows based operating system. For Windows OS, please see https://globus.org/globus-connect-personal globusconnectpersonal setup \u00b6 Inside the termial of your device, please run the following command: wget https://downloads.globus.org/globus-connect-personal/\\ linux/stable/globusconnectpersonal-latest.tgz tar xzf globusconnectpersonal-latest.tgz # this will produce a versioned globusconnectpersonal directory # replace `x.y.z` in the line below with the version number you see cd globusconnectpersonal-x.y.z ./globusconnect -setup --no-gui Here, we'll be presented with a url . If it is our first time, it will ask for creating an account, if you haven't done it before. Once, the account is set up, you'll be presented with an authorization code to enter into the code prompt. An example of this process is shown below: $ ./globusconnect -setup --no-gui We will display a login URL. Copy it into any browser and log in to get a single-use code. Return to this command with the code to continue setup. Login here: ----- https://auth.globus.org/v2/oauth2/authorize... ----- Enter the auth code: 0ZaZ.... == starting endpoint setup Input a value for the Endpoint Name: <enter_relevant_name> registered new endpoint, id: 56d4c388.... setup completed successfully Will now start globusconnectpersonal in GUI mode Graphical environment not detected To launch Globus Connect Personal in CLI mode, use globusconnectpersonal -start Or, if you want to force the use of the GUI, use globusconnectpersonal -gui Finally, run globusconnect ./globusconnectpersonal -start & On the personal device it will display a window This completes our setup on the personal device. Now, we should be able to set it up on R\u0101poi . On R\u0101poi \u00b6 Launch an interactive session srun --pty bash Tip Request more time for the interactive session, if you are wanting to transfer huge amount of data. srun --time=0-05:00:00 --pty bash Once you are on a compute node, please follow the steps starting from globusconnectpersonal setup as above ./globusconnectpersonal -start & Changing default directory to put your data? echo \"/new/path/address/,0,1\" > /nfs/home/$USER/.globusonline/lta/config-paths Here, the /new/path/address will be the path of your choice, e.g., /nfs/scratch/$USER/ This should now makes our two endpoints of data transfer accessible from the Globus website. Globus File Manager \u00b6 Leave the above running, and login to Globus website. Open the File Manager Tab from the login page. You should now be able to browse the names of your personal device and R\u0101poi 's compute node to transfer your files. An example shown below: You are now ready to transfer your data.","title":"Cloud providers"},{"location":"external/cloud_providers/#connecting-to-cloud-providers","text":"","title":"Connecting to Cloud Providers"},{"location":"external/cloud_providers/#aarnet-cloudstor","text":"NOTE Cloudstor service has been decommissioned since Dec 2023. For old accounts, please check access with the provider. All VUW researchers have access to the AARNET (Australia\u2019s Academic and Research Network) Cloudstor service which provides 1 TB of space to each researcher. To use this service first login and download an appropriate client to your laptop or desktop (or smarthone if you wish): Cloudstor Login NOTE: Within the Cloudstor web login settings you will need to create a Cloudstor Password, this is the password you will use to login on R\u0101poi, it does not use your VUW credentials for the command line login. We suggest setting up an App Password for Raapoi-rcopy rather than a global sync password. This way if your password is compromised you can easily just remove that app password. Setup an App Password by clicky on the settings gear on the top right and finding the App Password link . Once you have setup your cloudstor (aka ownCloud) credentials you can use them to sync data to and from R\u0101poi. For example, if I wanted to sync my project space to Cloudstor I would do the following from R\u0101poi login node: # Use Tmux to keep your session alive if you disconnect. You can reconnect to your Tmux session if you reconnect. See Tmux docs. tmux # Use our new module system module use /home/software/tools/eb_modulefiles/all/Core module load rclone/1.54.1 #check if cloudstor remote is already configured rclone listremotes The above sequence starts a tmux session to allow the transfer to continue even if I disconnect from the cluster (type tmux attach to reconnect) and then loads the rcopy module - which requires the use of our new module system. If you don't already have CloudStor configured as a remote (which you won't if this is your first time using it) follow the instructions on aarnet docs page . Once we have setup rclone to connect to CloudStor, we copy our data. In this case from <my scratch folder>/test to test on CloudStor rclone copy --progress --transfers 8 /nfs/scratch/<username>/test CloudStor:/test","title":"AARNET Cloudstor"},{"location":"external/cloud_providers/#amazon-aws","text":"Create a python environment and install awscli module. python3 -m pip install awscli Before you proceed you will need to configure your environment with your Access Key ID and Secret Access Key, both of which will be sent to you once your account is created or linked. The command to configure your environment is aws configure You only need to do this once, unless of course you use more than one user/Access Key. Most users can simply click through the region and profile questions (using the default of \"none\"). If you do have a specific region this should be relayed along with your access and secret keys. Once you have the appropriate environment in place and your configuration setup you can use the aws command, plus an appropriate sub-command (s3, emr, rds, dynamodb, etc) and supporting arguments. More information on the CLI can be found here: http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-using.html","title":"Amazon AWS"},{"location":"external/cloud_providers/#google-cloud-gcloud-connections","text":"The Google Cloud SDK is available in R\u0101poi. This includes a command-line interface for connecting to gloud services. To get started, first load the environment module. You can find the path with the module spider command. As of this writing the current version can be loaded thusly: module load gcloud/481.0.0 This will give you access to the gcloud command. To setup a connection to your gcloud account use the init sub-command, eg. gcloud init --console-only Follow the instructions to authorize your gcloud account. Once on the Google website, you will be given an authorization code which you will copy/paste back into the R\u0101poi terminal window.","title":"Google Cloud (gcloud) Connections"},{"location":"external/cloud_providers/#dropbox-cloud-storage","text":"Upload/Download Limits with DropBox Dropbox has upload/download limitations and we have found that once your file gets above 50GB in size the transfer will have a better chance of timing out and failing. Configuring your Dropbox account on R\u0101poi Step A: On your local laptop or desktop start your browser and login to your Dropbox account Step B: On R\u0101poi type the following: module load dropbox Step C: Setup account credentials (You should only need to do this once): Run the following command from R\u0101poi dbxcli account You will now see something like the following: Go to https://www.dropbox.com/1/oauth2/authorize?client_id=X123... Click \"Allow\" (you might have to log in first). Copy the authorization code. Enter the authorization code here: Step D: Copy the URL link listed in Step C1 and paste it into the web browser that you started in Step A This will provide you with a long access code (aka hash). Now copy that access code and paste it into your R\u0101poi terminal after Step C3 where it is asking for Enter the authorization code here Now hit enter or return. You should see that you are now logged in with your Dropbox credentials","title":"DropBox Cloud Storage"},{"location":"external/cloud_providers/#microsoft-onedrive","text":"RClone can be used to connect to onedrive, google drive, etc. The steps below implement onedrive setup on R\u0101poi. module load rclone/1.54.1 rclone config Follow the on-screen instructions, e.g., make a new remote - enter \"n\"; to select storage type; \"onedrive\" and keep following default options until \"Use auto config\" and enter \"y\" here. This should display a url; copy and paste in the browser and it should get set up. To view your files on the remote onedrive, you can use: # For example, rclone lsd <remote>:<dir_name>, in my case , I would do : rclone lsd my_staff_onedrive:Documents","title":"Microsoft OneDrive"},{"location":"external/cloud_providers/#globus","text":"External sharing coming soon! Presently, users can share/transfer data to and from locations including cluster, research storage, and personal devices. Options for enabling data sharing externally are being explored. First time users should be able to sign up on GLOBUS website using their university credentials by following this link: https://app.globus.org/ . Please install and start GLOBUS connect personal. Please find those instructions here: https://globus.org/globus-connect-personal You should now be able to share or transfer your data by following their guide: https://docs.globus.org/guides/tutorials/manage-files/share-files/ .","title":"GLOBUS"},{"location":"external/solar_vuw/","text":"Storage for Learning and Research (SoLAR) - VUW High Capacity Storage \u00b6 The SoLAR Drive is the VUW High Capacity Storage system, allowing you to store all your research work. You can require many many terabytes of storage. It is also possible to connect your SoLAR drive to R\u0101poi, which is great! The following document will describe how to sign up for storage on the SoLAR Drive, as well as how to move and copy data between R\u0101poi and your SoLAR Drive. Signing up and getting storage on SoLAR \u00b6 To get your own space on SoLAR. Do the following: Login to your staff intranet. To do this, open https://intranet.wgtn.ac.nz/ in your web browser, and sign in to your staff intranet. In a new browser tab, open https://intranet.wgtn.ac.nz/staff/services-resources/digital-solutions/contact-us You should see the follow page below. Click the Staff Service Centre button You will now be directed to the Staff Service Centre, which will look like below. Hover your mouse above Digital Solution -> Access/permissions -> Additional drives You will now be sent to the ADDITIONAL DRIVE ACCESS page. Fill out the details on this page and click the **Submit** button at the bottom of the page to send your request space on SoLAR. Source: https://intranet.wgtn.ac.nz/staff/services-resources/digital-solutions/research-services/solar Accessing your SoLAR Drive on Windows/Mac \u00b6 Accessing the SoLAR Drive from off Campus \u00b6 You will want to sign up to the Uictoria University VPN to gain access to SoLAR. Click https://vpn.vuw.ac.nz/ to get access to the VPN and to download the Cisco AnyConnect program on to your computer Windows \u00b6 Open This PC (My Computer) and click Computer -> Map network drive at the top of the This PC explorer window. This will open a window as shown below. Enter the SoLAR path and the name of your patition on SoLAR, and click the Finish button. Enter in your username as STAFF\\username and your password if required Mac \u00b6 If you are off campus, login to your VPN using the Cisco AnyConnect program. In Finder, click Go -> Connect to Server... Write smb://vuwstocoissrin1.vuw.ac.nz/YourFolderName into the box, where YourFolderName is the name of your partition on SoLAR, and click connect. In username give: ``STAFF/username``; give your VUW password, and click connect. Moving/Copying files and folders between SoLAR and R\u0101poi \u00b6 There are several way to move/copy files and folder between SoLAR and R\u0101poi Best Way: Mounting SoLAR Partition in R\u0101poi \u00b6 Ask Digital Solutions for a service account to be created against your Research storage. Then a Raapoi admin will permanantly mount your storage on Raapoi - this process is time consuming and involves back and forth between DS and CAD. Second Way: RClone \u00b6 To do once I get it fixed Third Way: smbclient \u00b6 smbclient is specifically designed to transfer files and folders to and from smb clients. It is a bit cumbersome to use, but it is an alternative way for copying files between R\u0101poi and SoLAR To use smbclient , first cd into the directory that contains the folder you would like to copy from R\u0101poi to SoLAR. Then in the terminal give the following input: smbclient //vuwstocoissrin1.vuw.ac.nz/SoLAR_folder_name \\ --user username --workgroup STAFF --command \"prompt OFF;\\ recurse ON; cd remote/target/directory; mput \\ folder_on_Raapoi_you_want_to_copy_to_SoLAR \" You will then be asked to give your VUW password to copy data to SoLAR. Note: This may take a while if you are copying lots of files or large files. It is recommended that if you have lots of file in folders to copy (i.e. in the 100,000s of files) that you copy individually big folders rather than the whole directory at once so you can keep track of what has been copied if there are issues. Tip You may see it not doing anything for a while, and then all of a sudden it will show you that it is doing things. This is normal. If you want to copy files from SoLAR to R\u0101poi, first cd into the directory on R\u0101poi that you want to copy the SoLAR folder/file to. Then in the terminal give the following input: smbclient //vuwstocoissrin1.vuw.ac.nz/SoLAR_folder_name \\ --user username --workgroup STAFF --command \"prompt OFF; \\ recurse ON; cd remote/source/directory; mget \\ folder_on_SoLAR_you_want_to_copy_to_Raapoi \" Warning Don't run multiple smbclient at once, only a few at a time, if not one at a time. It can have problems if too many are running at one time.","title":"Solar vuw"},{"location":"external/solar_vuw/#storage-for-learning-and-research-solar-vuw-high-capacity-storage","text":"The SoLAR Drive is the VUW High Capacity Storage system, allowing you to store all your research work. You can require many many terabytes of storage. It is also possible to connect your SoLAR drive to R\u0101poi, which is great! The following document will describe how to sign up for storage on the SoLAR Drive, as well as how to move and copy data between R\u0101poi and your SoLAR Drive.","title":"Storage for Learning and Research (SoLAR) - VUW High Capacity Storage"},{"location":"external/solar_vuw/#signing-up-and-getting-storage-on-solar","text":"To get your own space on SoLAR. Do the following: Login to your staff intranet. To do this, open https://intranet.wgtn.ac.nz/ in your web browser, and sign in to your staff intranet. In a new browser tab, open https://intranet.wgtn.ac.nz/staff/services-resources/digital-solutions/contact-us You should see the follow page below. Click the Staff Service Centre button You will now be directed to the Staff Service Centre, which will look like below. Hover your mouse above Digital Solution -> Access/permissions -> Additional drives You will now be sent to the ADDITIONAL DRIVE ACCESS page. Fill out the details on this page and click the **Submit** button at the bottom of the page to send your request space on SoLAR. Source: https://intranet.wgtn.ac.nz/staff/services-resources/digital-solutions/research-services/solar","title":"Signing up and getting storage on SoLAR"},{"location":"external/solar_vuw/#accessing-your-solar-drive-on-windowsmac","text":"","title":"Accessing your SoLAR Drive on Windows/Mac"},{"location":"external/solar_vuw/#movingcopying-files-and-folders-between-solar-and-rapoi","text":"There are several way to move/copy files and folder between SoLAR and R\u0101poi","title":"Moving/Copying files and folders between SoLAR and R\u0101poi"},{"location":"storage/beegfs/","text":"BeeGFS Tips \u00b6 The BeeGFS storage is spread across 3 nodes with SSD disks. The aggregate storage is 100TB. We don't enforce quotas here as some projects have large storage needs. However, you do need to be a good HPC citizen and respect the rights of others. Don't needlessly fill up this storage. We regularly delete all data here every 3-6 months. We only post warnings on the slack channel . If after 3 months the storage is not full and still performning well, we delay the wipe another 3 months. This storage is not backed up at all. It is on a raid array so if a hard drive fails your data is safe. However in the event of a more dramatic hardware failure, earthquakes or fire - your data is gone forever. If you accidentally delete something, it's gone forever. If an Admin misconfigures something, your data is gone (we try not to do this!). It is your responsiblilty to backup your data here - a good place is to use Digital Solutions Research Storage. BeeGFS should have better IO performance than the scratch storage - however it does depend what other users are doing. No filesystem likes small files, BeeGFS likes small files even less than scratch. Filesizes over 1MB are best. If you have a large amount of files you can improve performance by splitting your files accross many directories - the load balancing accross the metadata and storage servers is by directory, not file.","title":"Beegfs"},{"location":"storage/beegfs/#beegfs-tips","text":"The BeeGFS storage is spread across 3 nodes with SSD disks. The aggregate storage is 100TB. We don't enforce quotas here as some projects have large storage needs. However, you do need to be a good HPC citizen and respect the rights of others. Don't needlessly fill up this storage. We regularly delete all data here every 3-6 months. We only post warnings on the slack channel . If after 3 months the storage is not full and still performning well, we delay the wipe another 3 months. This storage is not backed up at all. It is on a raid array so if a hard drive fails your data is safe. However in the event of a more dramatic hardware failure, earthquakes or fire - your data is gone forever. If you accidentally delete something, it's gone forever. If an Admin misconfigures something, your data is gone (we try not to do this!). It is your responsiblilty to backup your data here - a good place is to use Digital Solutions Research Storage. BeeGFS should have better IO performance than the scratch storage - however it does depend what other users are doing. No filesystem likes small files, BeeGFS likes small files even less than scratch. Filesizes over 1MB are best. If you have a large amount of files you can improve performance by splitting your files accross many directories - the load balancing accross the metadata and storage servers is by directory, not file.","title":"BeeGFS Tips"},{"location":"storage/home/","text":"Home Directory Tips \u00b6 Home directories have a small quota and are on fairly slow storage. The data here is backed up . It is replicated off site live as well as periodically backed up to off site tape. In theory data here is fairly safe, even in a catastophic event it should be recoverable eventually. If you accidentally delete something here it can be recovered with a service desk request. While this storage is not performant, is is quite safe and is a good place for you scripts and code to live. Your data sets can also be on your home if they fit and the performance doesn't cause you any problems. For bigger or faster storage see the Scratch page.","title":"Home"},{"location":"storage/home/#home-directory-tips","text":"Home directories have a small quota and are on fairly slow storage. The data here is backed up . It is replicated off site live as well as periodically backed up to off site tape. In theory data here is fairly safe, even in a catastophic event it should be recoverable eventually. If you accidentally delete something here it can be recovered with a service desk request. While this storage is not performant, is is quite safe and is a good place for you scripts and code to live. Your data sets can also be on your home if they fit and the performance doesn't cause you any problems. For bigger or faster storage see the Scratch page.","title":"Home Directory Tips"},{"location":"storage/scratch/","text":"Scratch Tips \u00b6 The scratch storage is on a large storage node with 2 raid arrays with 50TB of storage each. Your scratch will always be available at /nfs/scratch/<username> . Your scratch storage could be on scratch or scratch2, to find out run vuw-quota . Each user has a quota of 5TB on scratch - you can ask the support team to increase it if needed. While each user has a quota of 5TB, we don't actually have enough storage for each user to fill 5TB of storage! This is a shared resource and we will occasionally ask on the slack channel for users to clean up their storage to make space for others. Individuals using a large amount of scratch space may recieve an email. To check how much space is free on the scratch storage for all users, on R\u0101poi: df -h | grep scratch #df -h is disk free with human units, | pipes the output to grep, which shows lines which contain the word scratch On the slack channel in any DM or channel type /df-scratch The output should only be visible to you This storage is not backed up at all. It is on a raid array so if a hard drive fails your data is safe. However in the event of a more dramatic hardware failure, earthquakes or fire - your data is gone forever. If you accidentally delete something, it's gone forever. If an Admin misconfigures something, your data is gone (we try not to do this!). It is your responsiblilty to backup your data here - a good place is to use Digital Solutions Research Storage (see Connecting to SoLAR ). Scratch is also not a place for your old data to live forever, please clean up datasets you're no longer using!","title":"Scratch"},{"location":"storage/scratch/#scratch-tips","text":"The scratch storage is on a large storage node with 2 raid arrays with 50TB of storage each. Your scratch will always be available at /nfs/scratch/<username> . Your scratch storage could be on scratch or scratch2, to find out run vuw-quota . Each user has a quota of 5TB on scratch - you can ask the support team to increase it if needed. While each user has a quota of 5TB, we don't actually have enough storage for each user to fill 5TB of storage! This is a shared resource and we will occasionally ask on the slack channel for users to clean up their storage to make space for others. Individuals using a large amount of scratch space may recieve an email. To check how much space is free on the scratch storage for all users, on R\u0101poi: df -h | grep scratch #df -h is disk free with human units, | pipes the output to grep, which shows lines which contain the word scratch On the slack channel in any DM or channel type /df-scratch The output should only be visible to you This storage is not backed up at all. It is on a raid array so if a hard drive fails your data is safe. However in the event of a more dramatic hardware failure, earthquakes or fire - your data is gone forever. If you accidentally delete something, it's gone forever. If an Admin misconfigures something, your data is gone (we try not to do this!). It is your responsiblilty to backup your data here - a good place is to use Digital Solutions Research Storage (see Connecting to SoLAR ). Scratch is also not a place for your old data to live forever, please clean up datasets you're no longer using!","title":"Scratch Tips"},{"location":"storage/tmp/","text":"Temp Disk Tips \u00b6 This storage is very fast on the AMD nodes and GPU nodes. It is your job to move data to the tmp space and clean it up when done. There is very little management of this space and currently it is not visible to slurm for fair use scheduling - in other words someone else might have used up most of the temp space on the node! This is generally not the case though. A rough example of how you could use this in an sbatch script #!/bin/bash # #SBATCH --job-name=bash_test # #SBATCH --partition=quicktest # #SBATCH --cpus-per-task=2 #Note: you are always allocated an even number of cpus #SBATCH --mem=1G #SBATCH --time=10:00 # Do the needed module loading for your use case module load etc #make a temporary directory with your usename so you don't tread on others mkdir /tmp/<username> #Copy dataset from scratch to the local tmp on the node (could also use rsync) cp -r /nfs/scratch/<user>/dataset /tmp/<user>/dataset Process data against /tmp/<user>/dataset Lets say the data is output to /tmp/<user>/dataoutput/ # Copy data from output to your scratch - I suggest not overwriting your original dataset! cp -r /tmp/<user>/dataoutput/* /nfs/scratch/<user>/dataset/dataoutput/ # Delete the data you copy to and created on tmp rm -rf /tmp/<user> #DANGER!!","title":"Tmp"},{"location":"storage/tmp/#temp-disk-tips","text":"This storage is very fast on the AMD nodes and GPU nodes. It is your job to move data to the tmp space and clean it up when done. There is very little management of this space and currently it is not visible to slurm for fair use scheduling - in other words someone else might have used up most of the temp space on the node! This is generally not the case though. A rough example of how you could use this in an sbatch script #!/bin/bash # #SBATCH --job-name=bash_test # #SBATCH --partition=quicktest # #SBATCH --cpus-per-task=2 #Note: you are always allocated an even number of cpus #SBATCH --mem=1G #SBATCH --time=10:00 # Do the needed module loading for your use case module load etc #make a temporary directory with your usename so you don't tread on others mkdir /tmp/<username> #Copy dataset from scratch to the local tmp on the node (could also use rsync) cp -r /nfs/scratch/<user>/dataset /tmp/<user>/dataset Process data against /tmp/<user>/dataset Lets say the data is output to /tmp/<user>/dataoutput/ # Copy data from output to your scratch - I suggest not overwriting your original dataset! cp -r /tmp/<user>/dataoutput/* /nfs/scratch/<user>/dataset/dataoutput/ # Delete the data you copy to and created on tmp rm -rf /tmp/<user> #DANGER!!","title":"Temp Disk Tips"},{"location":"training/gaussian/","text":"Example Gaussian Job Submission on HPC \u00b6 Here is an example of submitting a Gaussian job on the HPC using Slurm. In this example, we will submit a Gaussian job using the quicktest partition, and request 1 task with 4 CPUs and 7GB of memory for a maximum run time of 1 hour. We will also load the g16 module, which is required to run Gaussian on the HPC. First, create a new directory and navigate to it: mkdir gaussian_example cd gaussian_example Get the example input file \u00b6 The test0397.com file is an example input file for Gaussian. It contains instructions for Gaussian to perform a calculation on a molecule. To run the example job using this input file, you should copy the test0397.com file from the Gaussian installation directory at /home/software/apps/gaussian/g16/tests/com/test0397.com to your working directory ( gaussian_example in this case). To do that from the gaussian_example directory: cp /home/software/apps/gaussian/g16/tests/com/test0397.com . # copy from location to . The dot means current directory Have a look at the first few lines of the input file to see what it does. head test0397.com # the first 5 lines of test0397.com #returns !%nproc = 4 #p rb3lyp/3-21g force test scf=novaracc Gaussian Test Job 397 : Valinomycin force 0 ,1 O,-1.3754834437,-2.5956821046,3.7664927822 O,-0.3728418073,-0.530460483,3.8840401686 O,2.3301890394,0.5231526187,1.7996834334 The first line !%nproc=4 specifies the number of processors that Gaussian will use to run the calculation, in this case, 4. We will need to make sure that the number of processes used in this file matches the number of cpus we request from Slurm Slurm Submission \u00b6 Next, create a submission script called submit.sh (using nano or similar) and add the following contents: #!/bin/sh #SBATCH --job-name=g16-test # max run time #SBATCH --time=1:00:00 #SBATCH --partition=quicktest #SBATCH --output=_quicktest.out #SBATCH --error=_quicktest.err #SBATCH --ntasks=1 #SBATCH --cpus-per-task=4 #SBATCH --mem=7G module load gaussian/g16 g16 test0397.com In the submission script, we specify the following: --job-name : name of the job to appear in the queue. --time : maximum runtime for the job in hh:mm:ss format. --partition : the partition to run the job on. --output : specifies the name of the standard output file. --error : specifies the name of the standard error file. --ntasks : specifies the number of tasks the job will use. --cpus-per-task : specifies the number of CPUs per task. --mem : specifies the amount of memory to allocate for the job. Submit the job to the queue using sbatch : sbatch submit.sh You can check the status of your job in the queue using squeue : squeue -u <your_username> Once the job is finished, you can check for the output files and see the contents of the standard output file using cat : ls cat _quicktest.out The Gaussian output files (test0397.log, test0397.chk, etc.) will also be generated in the working directory. You can view the output file using the less command: less test0397.log Press q to Quit the less program That's it! You have successfully submitted and run a Gaussian job on a HPC cluster using Slurm.","title":"Gaussian"},{"location":"training/gaussian/#example-gaussian-job-submission-on-hpc","text":"Here is an example of submitting a Gaussian job on the HPC using Slurm. In this example, we will submit a Gaussian job using the quicktest partition, and request 1 task with 4 CPUs and 7GB of memory for a maximum run time of 1 hour. We will also load the g16 module, which is required to run Gaussian on the HPC. First, create a new directory and navigate to it: mkdir gaussian_example cd gaussian_example","title":"Example Gaussian Job Submission on HPC"},{"location":"training/gaussian/#get-the-example-input-file","text":"The test0397.com file is an example input file for Gaussian. It contains instructions for Gaussian to perform a calculation on a molecule. To run the example job using this input file, you should copy the test0397.com file from the Gaussian installation directory at /home/software/apps/gaussian/g16/tests/com/test0397.com to your working directory ( gaussian_example in this case). To do that from the gaussian_example directory: cp /home/software/apps/gaussian/g16/tests/com/test0397.com . # copy from location to . The dot means current directory Have a look at the first few lines of the input file to see what it does. head test0397.com # the first 5 lines of test0397.com #returns !%nproc = 4 #p rb3lyp/3-21g force test scf=novaracc Gaussian Test Job 397 : Valinomycin force 0 ,1 O,-1.3754834437,-2.5956821046,3.7664927822 O,-0.3728418073,-0.530460483,3.8840401686 O,2.3301890394,0.5231526187,1.7996834334 The first line !%nproc=4 specifies the number of processors that Gaussian will use to run the calculation, in this case, 4. We will need to make sure that the number of processes used in this file matches the number of cpus we request from Slurm","title":"Get the example input file"},{"location":"training/gaussian/#slurm-submission","text":"Next, create a submission script called submit.sh (using nano or similar) and add the following contents: #!/bin/sh #SBATCH --job-name=g16-test # max run time #SBATCH --time=1:00:00 #SBATCH --partition=quicktest #SBATCH --output=_quicktest.out #SBATCH --error=_quicktest.err #SBATCH --ntasks=1 #SBATCH --cpus-per-task=4 #SBATCH --mem=7G module load gaussian/g16 g16 test0397.com In the submission script, we specify the following: --job-name : name of the job to appear in the queue. --time : maximum runtime for the job in hh:mm:ss format. --partition : the partition to run the job on. --output : specifies the name of the standard output file. --error : specifies the name of the standard error file. --ntasks : specifies the number of tasks the job will use. --cpus-per-task : specifies the number of CPUs per task. --mem : specifies the amount of memory to allocate for the job. Submit the job to the queue using sbatch : sbatch submit.sh You can check the status of your job in the queue using squeue : squeue -u <your_username> Once the job is finished, you can check for the output files and see the contents of the standard output file using cat : ls cat _quicktest.out The Gaussian output files (test0397.log, test0397.chk, etc.) will also be generated in the working directory. You can view the output file using the less command: less test0397.log Press q to Quit the less program That's it! You have successfully submitted and run a Gaussian job on a HPC cluster using Slurm.","title":"Slurm Submission"},{"location":"training/gpu-neural-style/","text":"GPU example with neural style in pytorch \u00b6 We'll do a quick python example using neural style implemented in pytorch. We will be using modules rather than conda/virtualenvs but there is nothing stopping you from loading the modules and creating a virtualenv/conda enviroment to install additional python packages. The code we use will come from the pytorch example git repo. Clone the pytorch example repo \u00b6 In a sensible location, clone the rep. git clone https://github.com/pytorch/examples.git cd examples/fast_neural_style # change to the example we will be running. Load the modules \u00b6 We are using the new Easybuild based modules, to ensure we don't have conflicts with the old modules, it will be best to unuse them first and then use the new system. At somepoint we may automatically add the new modules to your bashrc file - but currently you'll have to do this yourself or manually unuse and use the new module system. module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 module list #see all the dependencies we have loaded, in particular which version of python we're using now. Currently Python 3.8.6 PyTorch note: When running jobs which utilise PyTorch, make sure you allocate sufficient memory for the job. If you encounter error messages which are vague, it is possible that you don't have enough memory allocated. Just to import torch it is recommended to have 4GB (or you may see errors such as ImportError: <library>.so: failed to map segment from shared object ). Optional: Setup a virtualenv \u00b6 python3 -m venv env # create a virtualenv folder called env. Note! This will likely only work with the python version listed above! source env/bin/activate # activate the virtualenv Now that we've activated the virtual environment, we can install any additional packages we need. In this case we don't need any additional packages. Download some images to use as content as well as for training. \u00b6 In your examples/fast_neural_style/ directory. # Download an image of an octopus to images/content-images. ## CC BY-SA 3.0 H. Zell wget https://upload.wikimedia.org/wikipedia/commons/0/0c/Octopus_vulgaris_02.JPG -P images/content-images/ # Download an image of The Great Wave off Kanagawa - public domain wget https://upload.wikimedia.org/wikipedia/commons/a/a5/Tsunami_by_hokusai_19th_century.jpg -O images/style-images/wave.jpg Depending on the GPU we are using, we may need to resize the image to ensure it fits in memory. On an RTX6000 we would need to resize the image to 70% of its full size to fit in memroy. Thankfully the GPUs on R\u0101poi are A100's with 40GB of ram, so we can skip this step. We will also need to download the pre-trained models for our initial inference runs. python download_saved_models.py Style some images - inference \u00b6 We'll initially just use pretrained models to generate styled images - this is known as model inference and is much less intensive than training the model, we'll do this on both CPU and GPU. submit_cpu.sh #!/bin/bash #SBATCH --job-name=pytorch_test #SBATCH -o _test.out #SBATCH -e _test.err #SBATCH --time=00:15:00 #SBATCH --partition=parallel #SBATCH --ntasks=12 #SBATCH --mem=6G module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 #Optional source env/bin/activate #activate the virtualenv # Run our job --cuda 0 means run on the CPU and we'll save the output image as test1.jpg # python neural_style/neural_style.py eval --content-image images/content-images/Octopus_vulgaris_02.JPG --model saved_models/mosaic.pth --output-image ./test1.jpg --cuda 0 You can check how long the job took to run with vuw-job-history . The last lines are your last run job, in my case: 332281 COMPLETED pytorch_t+ 00 :02:36 332281 .batch COMPLETED batch 0 .15G 00 :02:36 332281 .exte+ COMPLETED extern 0 .15G 00 :02:36 the job took 2:36. Let's run the inference job again on GPU to see the speedup. submit_gpu.sh #!/bin/bash #SBATCH --job-name=pytorch_test #SBATCH -o _test.out #SBATCH -e _test.err #SBATCH --time=00:15:00 #SBATCH --partition=gpu #SBATCH --gres=gpu:1 #SBATCH --ntasks=2 #SBATCH --mem=60G module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 #optional source env/bin/activate #activate the virtualenv # Run our job --cuda 1 means run on the GPU and we'll save the output image as test2.jpg # python neural_style/neural_style.py eval --content-image images/content-images/Octopus_vulgaris_02.JPG --model saved_models/mosaic.pth --output-image ./test2.jpg --cuda 1 In this case vuw-job-history the job took: 692973 COMPLETED pytorch_t+ 00 :00:16 692973 .batch COMPLETED batch 0 .15G 00 :00:16 692973 .exte+ COMPLETED extern 0 .15G 00 :00:16 but the time varies a lot with short GPU runs, some are nearly 2 min long and some runs are 16s with the same data. The memory usage with pytorch is also hard to estimate, running vuw-job-report 332320 shows: Nodes: 1 Cores per node: 2 CPU Utilized: 00 :00:07 CPU Efficiency: 43 .75% of 00 :00:16 core-walltime Job Wall-clock time: 00 :00:08 Memory Utilized: 1 .38 MB Memory Efficiency: 0 .00% of 60 .00 GB The memory usage is very low, but there is a very brief spike in memory at the end of the run as the image is generated that vuw-job-report doesn't quite capture. 60G of memory is needed to ensure this completes - a good rule of thumb is to allocate at least as much system memory as GPU memory. The A100's have 40G of ram. Train a new style - computationally expensive. \u00b6 Training a new image style is where we will get the greatest speedup using a GPU. We will use 13G of training images - COCO 2014 Training images dataset . These images have already been downloaded and are accessable at /nfs/home/training/neural_style_data/train2014/ . Note that training a new style will take about 1:15h on an A100 and two and a half hours on an RTX6000 #!/bin/bash #SBATCH --job-name=pytorch_test #SBATCH -o _test.out #SBATCH -e _test.err #SBATCH --time=03:00:00 #SBATCH --partition=gpu #SBATCH --gres=gpu:1 #SBATCH --ntasks=2 #SBATCH --mem=60G module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 #Optional source env/bin/activate #activate the virtualenv # Run our job --cuda 1 means run on the GPU # style-weight and content-weight are just parameters adjusted to give better results python neural_style/neural_style.py train \\ --dataset /nfs/home/training/neural_style_data/ \\ --style-image images/style-images/wave.jpg \\ --save-model-dir saved_models/style5e10_content_5e4 \\ --style-weight 5e10 \\ --content-weight 5e4 \\ --epochs 2 \\ --cuda 1 This will take a while, but should eventually complete. The A100 has enough memory to train on this image, with other GPUs you may need to scale down the style image to fit in the GPU memory. Note: If you get an out of GPU memory error but it seems the GPU has plenty of memory, it often means you ran out of system memory, try asking for more memory in slurm. Use our newly trained network \u00b6 submit_gpu.sh #!/bin/bash #SBATCH --job-name=pytorch_test #SBATCH -o _test.out #SBATCH -e _test.err #SBATCH --time=00:15:00 #SBATCH --partition=gpu #SBATCH --gres=gpu:1 #SBATCH --ntasks=2 #SBATCH --mem=60G module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 #Optional source env/bin/activate #activate the virtualenv # Run our job --cuda 1 means run on the GPU and we'll save the output image as test2.jpg # python neural_style/neural_style.py eval \\ --content-image images/content-images/Octopus_vulgaris_02.JPG \\ --model saved_models/style5e10_content_5e4 \\ --output-image ./test3.jpg --cuda 1 Bonus content use a slurm task-array to find the optimum parameters. \u00b6 In the above example we use parameters for style-weight and content-weight. There are lots of possibilities for these parameters, we can use a task array and a parameter list to determine good values. Note that actually running this example will consume a lot of resources and it is presented mostly to provide some information about task arrays. Running this example will consume the whole GPU partition for about 12 hours. First let's create a list of parameters to test, we could include these in the batch submission script, but I think it's clearer to separate them out. If you're version controlling your submission script, it'll make it easier to see what are changes to parameters and what are changes to the script itself. In the parameter list, the first column is style-weight parameters and the second is content-weight parameters. paramlist.txt 5e10 1e3 5e10 1e4 5e10 5e4 1e11 1e3 1e11 1e4 1e11 5e4 5e11 1e3 5e11 1e4 5e11 5e4 1e12 1e3 1e12 1e4 1e12 5e4 In our submission script we will parse these values with awk . Awk is a bit beyond the scope of this lesson, but it is a handy shell tool for manipulating text. Digital ocean has a nice primer on Awk submit_gpu_train_array #!/bin/bash #SBATCH --job-name=pytorch_test #SBATCH -o _test.out #SBATCH -e _test.err #SBATCH --time=10:00:00 #SBATCH --partition=gpu #SBATCH --gres=gpu:1 #SBATCH --ntasks=10 #SBATCH --mem=60G #SBATCH --array=1-13 module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 #Optional source env/bin/activate #activate the virtualenv # Run our job --cuda 1 means run on the GPU # #awk -v var=\"$SLURM_ARRAY_TASK_ID\" 'NR == var {print $1}' paramlist.txt style_weight = $( awk -v var = \" $SLURM_ARRAY_TASK_ID \" 'NR == var {print $1}' paramlist.txt ) content_weight = $( awk -v var = \" $SLURM_ARRAY_TASK_ID \" 'NR == var {print $2}' paramlist.txt ) echo $style_weight echo $content_weight python neural_style/neural_style.py train \\ --dataset nfs/home/training/neural_style_data/ \\ --style-image images/style-images/wave.jpg \\ --save-model-dir saved_models/test_params2_epoch2/style ${ style_weight } _content ${ content_weight } \\ --style-weight $style_weight \\ --content-weight $content_weight \\ --epochs 2 \\ --cuda 1","title":"Gpu neural style"},{"location":"training/gpu-neural-style/#gpu-example-with-neural-style-in-pytorch","text":"We'll do a quick python example using neural style implemented in pytorch. We will be using modules rather than conda/virtualenvs but there is nothing stopping you from loading the modules and creating a virtualenv/conda enviroment to install additional python packages. The code we use will come from the pytorch example git repo.","title":"GPU example with neural style in pytorch"},{"location":"training/gpu-neural-style/#clone-the-pytorch-example-repo","text":"In a sensible location, clone the rep. git clone https://github.com/pytorch/examples.git cd examples/fast_neural_style # change to the example we will be running.","title":"Clone the pytorch example repo"},{"location":"training/gpu-neural-style/#load-the-modules","text":"We are using the new Easybuild based modules, to ensure we don't have conflicts with the old modules, it will be best to unuse them first and then use the new system. At somepoint we may automatically add the new modules to your bashrc file - but currently you'll have to do this yourself or manually unuse and use the new module system. module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 module list #see all the dependencies we have loaded, in particular which version of python we're using now. Currently Python 3.8.6 PyTorch note: When running jobs which utilise PyTorch, make sure you allocate sufficient memory for the job. If you encounter error messages which are vague, it is possible that you don't have enough memory allocated. Just to import torch it is recommended to have 4GB (or you may see errors such as ImportError: <library>.so: failed to map segment from shared object ).","title":"Load the modules"},{"location":"training/gpu-neural-style/#optional-setup-a-virtualenv","text":"python3 -m venv env # create a virtualenv folder called env. Note! This will likely only work with the python version listed above! source env/bin/activate # activate the virtualenv Now that we've activated the virtual environment, we can install any additional packages we need. In this case we don't need any additional packages.","title":"Optional: Setup a virtualenv"},{"location":"training/gpu-neural-style/#download-some-images-to-use-as-content-as-well-as-for-training","text":"In your examples/fast_neural_style/ directory. # Download an image of an octopus to images/content-images. ## CC BY-SA 3.0 H. Zell wget https://upload.wikimedia.org/wikipedia/commons/0/0c/Octopus_vulgaris_02.JPG -P images/content-images/ # Download an image of The Great Wave off Kanagawa - public domain wget https://upload.wikimedia.org/wikipedia/commons/a/a5/Tsunami_by_hokusai_19th_century.jpg -O images/style-images/wave.jpg Depending on the GPU we are using, we may need to resize the image to ensure it fits in memory. On an RTX6000 we would need to resize the image to 70% of its full size to fit in memroy. Thankfully the GPUs on R\u0101poi are A100's with 40GB of ram, so we can skip this step. We will also need to download the pre-trained models for our initial inference runs. python download_saved_models.py","title":"Download some images to use as content as well as for training."},{"location":"training/gpu-neural-style/#style-some-images-inference","text":"We'll initially just use pretrained models to generate styled images - this is known as model inference and is much less intensive than training the model, we'll do this on both CPU and GPU. submit_cpu.sh #!/bin/bash #SBATCH --job-name=pytorch_test #SBATCH -o _test.out #SBATCH -e _test.err #SBATCH --time=00:15:00 #SBATCH --partition=parallel #SBATCH --ntasks=12 #SBATCH --mem=6G module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 #Optional source env/bin/activate #activate the virtualenv # Run our job --cuda 0 means run on the CPU and we'll save the output image as test1.jpg # python neural_style/neural_style.py eval --content-image images/content-images/Octopus_vulgaris_02.JPG --model saved_models/mosaic.pth --output-image ./test1.jpg --cuda 0 You can check how long the job took to run with vuw-job-history . The last lines are your last run job, in my case: 332281 COMPLETED pytorch_t+ 00 :02:36 332281 .batch COMPLETED batch 0 .15G 00 :02:36 332281 .exte+ COMPLETED extern 0 .15G 00 :02:36 the job took 2:36. Let's run the inference job again on GPU to see the speedup. submit_gpu.sh #!/bin/bash #SBATCH --job-name=pytorch_test #SBATCH -o _test.out #SBATCH -e _test.err #SBATCH --time=00:15:00 #SBATCH --partition=gpu #SBATCH --gres=gpu:1 #SBATCH --ntasks=2 #SBATCH --mem=60G module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 #optional source env/bin/activate #activate the virtualenv # Run our job --cuda 1 means run on the GPU and we'll save the output image as test2.jpg # python neural_style/neural_style.py eval --content-image images/content-images/Octopus_vulgaris_02.JPG --model saved_models/mosaic.pth --output-image ./test2.jpg --cuda 1 In this case vuw-job-history the job took: 692973 COMPLETED pytorch_t+ 00 :00:16 692973 .batch COMPLETED batch 0 .15G 00 :00:16 692973 .exte+ COMPLETED extern 0 .15G 00 :00:16 but the time varies a lot with short GPU runs, some are nearly 2 min long and some runs are 16s with the same data. The memory usage with pytorch is also hard to estimate, running vuw-job-report 332320 shows: Nodes: 1 Cores per node: 2 CPU Utilized: 00 :00:07 CPU Efficiency: 43 .75% of 00 :00:16 core-walltime Job Wall-clock time: 00 :00:08 Memory Utilized: 1 .38 MB Memory Efficiency: 0 .00% of 60 .00 GB The memory usage is very low, but there is a very brief spike in memory at the end of the run as the image is generated that vuw-job-report doesn't quite capture. 60G of memory is needed to ensure this completes - a good rule of thumb is to allocate at least as much system memory as GPU memory. The A100's have 40G of ram.","title":"Style some images - inference"},{"location":"training/gpu-neural-style/#train-a-new-style-computationally-expensive","text":"Training a new image style is where we will get the greatest speedup using a GPU. We will use 13G of training images - COCO 2014 Training images dataset . These images have already been downloaded and are accessable at /nfs/home/training/neural_style_data/train2014/ . Note that training a new style will take about 1:15h on an A100 and two and a half hours on an RTX6000 #!/bin/bash #SBATCH --job-name=pytorch_test #SBATCH -o _test.out #SBATCH -e _test.err #SBATCH --time=03:00:00 #SBATCH --partition=gpu #SBATCH --gres=gpu:1 #SBATCH --ntasks=2 #SBATCH --mem=60G module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 #Optional source env/bin/activate #activate the virtualenv # Run our job --cuda 1 means run on the GPU # style-weight and content-weight are just parameters adjusted to give better results python neural_style/neural_style.py train \\ --dataset /nfs/home/training/neural_style_data/ \\ --style-image images/style-images/wave.jpg \\ --save-model-dir saved_models/style5e10_content_5e4 \\ --style-weight 5e10 \\ --content-weight 5e4 \\ --epochs 2 \\ --cuda 1 This will take a while, but should eventually complete. The A100 has enough memory to train on this image, with other GPUs you may need to scale down the style image to fit in the GPU memory. Note: If you get an out of GPU memory error but it seems the GPU has plenty of memory, it often means you ran out of system memory, try asking for more memory in slurm.","title":"Train a new style - computationally expensive."},{"location":"training/gpu-neural-style/#use-our-newly-trained-network","text":"submit_gpu.sh #!/bin/bash #SBATCH --job-name=pytorch_test #SBATCH -o _test.out #SBATCH -e _test.err #SBATCH --time=00:15:00 #SBATCH --partition=gpu #SBATCH --gres=gpu:1 #SBATCH --ntasks=2 #SBATCH --mem=60G module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 #Optional source env/bin/activate #activate the virtualenv # Run our job --cuda 1 means run on the GPU and we'll save the output image as test2.jpg # python neural_style/neural_style.py eval \\ --content-image images/content-images/Octopus_vulgaris_02.JPG \\ --model saved_models/style5e10_content_5e4 \\ --output-image ./test3.jpg --cuda 1","title":"Use our newly trained network"},{"location":"training/gpu-neural-style/#bonus-content-use-a-slurm-task-array-to-find-the-optimum-parameters","text":"In the above example we use parameters for style-weight and content-weight. There are lots of possibilities for these parameters, we can use a task array and a parameter list to determine good values. Note that actually running this example will consume a lot of resources and it is presented mostly to provide some information about task arrays. Running this example will consume the whole GPU partition for about 12 hours. First let's create a list of parameters to test, we could include these in the batch submission script, but I think it's clearer to separate them out. If you're version controlling your submission script, it'll make it easier to see what are changes to parameters and what are changes to the script itself. In the parameter list, the first column is style-weight parameters and the second is content-weight parameters. paramlist.txt 5e10 1e3 5e10 1e4 5e10 5e4 1e11 1e3 1e11 1e4 1e11 5e4 5e11 1e3 5e11 1e4 5e11 5e4 1e12 1e3 1e12 1e4 1e12 5e4 In our submission script we will parse these values with awk . Awk is a bit beyond the scope of this lesson, but it is a handy shell tool for manipulating text. Digital ocean has a nice primer on Awk submit_gpu_train_array #!/bin/bash #SBATCH --job-name=pytorch_test #SBATCH -o _test.out #SBATCH -e _test.err #SBATCH --time=10:00:00 #SBATCH --partition=gpu #SBATCH --gres=gpu:1 #SBATCH --ntasks=10 #SBATCH --mem=60G #SBATCH --array=1-13 module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 #Optional source env/bin/activate #activate the virtualenv # Run our job --cuda 1 means run on the GPU # #awk -v var=\"$SLURM_ARRAY_TASK_ID\" 'NR == var {print $1}' paramlist.txt style_weight = $( awk -v var = \" $SLURM_ARRAY_TASK_ID \" 'NR == var {print $1}' paramlist.txt ) content_weight = $( awk -v var = \" $SLURM_ARRAY_TASK_ID \" 'NR == var {print $2}' paramlist.txt ) echo $style_weight echo $content_weight python neural_style/neural_style.py train \\ --dataset nfs/home/training/neural_style_data/ \\ --style-image images/style-images/wave.jpg \\ --save-model-dir saved_models/test_params2_epoch2/style ${ style_weight } _content ${ content_weight } \\ --style-weight $style_weight \\ --content-weight $content_weight \\ --epochs 2 \\ --cuda 1","title":"Bonus content use a slurm task-array to find the optimum parameters."},{"location":"training/simple-openmpi-singularity-hybrid/","text":"Simple OpenMPI with Singularity using the hybrid approach. \u00b6 The hybrid approach is one way of getting OpenMPI working with containers. It requires the OpenMPI version inside the container to match the OpenMPI outside the container (loaded via module loading). First check what openMPI version we have on R\u0101poi. On R\u0101poi switch to our new modules module unuse /home/software/tools/modulefiles # stop using the older modules module use /home/software/tools/eb_modulefiles/all/Core #the new module files organised by compiler module spider OpenMPI # search for openMPI - thre are several options, lets try module spider OpenMPI/4.0.5 # we will use this one, which requires GCC/10.2.0 On your local machine we will create a very simple C openMPI program. Create this in a sensible place. I used ~/projects/examples/singularity/openMPI #include <mpi.h> #include <stdio.h> #include <stdlib.h> int main ( int argc , char ** argv ) { int rc ; int size ; int myrank ; rc = MPI_Init ( & argc , & argv ); if ( rc != MPI_SUCCESS ) { fprintf ( stderr , \"MPI_Init() failed\" ); return EXIT_FAILURE ; } rc = MPI_Comm_size ( MPI_COMM_WORLD , & size ); if ( rc != MPI_SUCCESS ) { fprintf ( stderr , \"MPI_Comm_size() failed\" ); goto exit_with_error ; } rc = MPI_Comm_rank ( MPI_COMM_WORLD , & myrank ); if ( rc != MPI_SUCCESS ) { fprintf ( stderr , \"MPI_Comm_rank() failed\" ); goto exit_with_error ; } fprintf ( stdout , \"Hello, I am rank %d/%d\" , myrank , size ); MPI_Finalize (); return EXIT_SUCCESS ; exit_with_error : MPI_Finalize (); return EXIT_FAILURE ; } In the same location as above create a singularity definition file, note that we choose to compile and install the same OpenMPI version as we will use on R\u0101poi. Bootstrap: docker From: ubuntu:latest %files mpitest.c /opt %environment export OMPI_DIR = /opt/ompi export SINGULARITY_OMPI_DIR = $OMPI_DIR export SINGULARITYENV_APPEND_PATH = $OMPI_DIR /bin export SINGULAIRTYENV_APPEND_LD_LIBRARY_PATH = $OMPI_DIR /lib %post echo \"Installing required packages...\" apt-get update && apt-get install -y wget git bash gcc gfortran g++ make file echo \"Installing Open MPI\" export OMPI_DIR = /opt/ompi export OMPI_VERSION = 4 .0.5 #NOTE matching version to that on Raapoi export OMPI_URL = \"https://download.open-mpi.org/release/open-mpi/v4.0/openmpi- $OMPI_VERSION .tar.bz2\" mkdir -p /tmp/ompi mkdir -p /opt # Download cd /tmp/ompi && wget -O openmpi- $OMPI_VERSION .tar.bz2 $OMPI_URL && tar -xjf openmpi- $OMPI_VERSION .tar.bz2 # Compile and install cd /tmp/ompi/openmpi- $OMPI_VERSION && ./configure --prefix = $OMPI_DIR && make install # Set env variables so we can compile our application export PATH = $OMPI_DIR /bin: $PATH export LD_LIBRARY_PATH = $OMPI_DIR /lib: $LD_LIBRARY_PATH export MANPATH = $OMPI_DIR /share/man: $MANPATH echo \"Compiling the MPI application...\" cd /opt && mpicc -o mpitest mpitest.c Now we build our container locally, giving it a sensible name. We need OpenMPI-4.0.5 to use this, so let's include that in the name. sudo singularity build test-openmpi-4.0.5.sif test-openmpi-4.0.5.def Copy that file to R\u0101poi somehow - Filezilla, rsync or similar. I'll just use sftp for simplicity. sftp <username>@raapoi.vuw.ac.nz put test-openmpi-4.0.5.sif Now on R\u0101poi copy that file to a sensible location, I used ~/projects/examples/singularity/openMPI again. mv test-openmpi-4.0.5.sif ~/projects/examples/singularity/openMPI/ cd ~/projects/examples/singularity/openMPI/ In that location create a sbatch file openmpi-test.sh #!/bin/bash #SBATCH --job-name=mpi_test #SBATCH --time=00-00:02:00 #SBATCH --output=out_test.out #SBATCH --error=out_test.err #SBATCH --partition=parallel #SBATCH --ntasks=2 #SBATCH --cpus-per-task=1 #SBATCH --tasks-per-node=1 #SBATCH --mem=1GB #SBATCH --constraint=\"IB,AMD\" #SBATCH --nodes=2 module use /home/software/tools/eb_modulefiles/all/Core module unuse /home/software/tools/modulefiles # to prevent conflicts with the old modules module load GCC/10.2.0 module load OpenMPI/4.0.5 module load Singularity/3.7.3 # Note this is a new singularity build CONPATH = $HOME /projects/examples/singularity/openMPI mpirun -np 2 singularity exec $CONPATH /test-openmpi-4.0.5.sif /opt/mpitest Submit that to slurm and see the output sbatch openmpi-test.sh squeue -u $USER # see the job cat out_test.out # examine the output after the job is done","title":"Simple openmpi singularity hybrid"},{"location":"training/simple-openmpi-singularity-hybrid/#simple-openmpi-with-singularity-using-the-hybrid-approach","text":"The hybrid approach is one way of getting OpenMPI working with containers. It requires the OpenMPI version inside the container to match the OpenMPI outside the container (loaded via module loading). First check what openMPI version we have on R\u0101poi. On R\u0101poi switch to our new modules module unuse /home/software/tools/modulefiles # stop using the older modules module use /home/software/tools/eb_modulefiles/all/Core #the new module files organised by compiler module spider OpenMPI # search for openMPI - thre are several options, lets try module spider OpenMPI/4.0.5 # we will use this one, which requires GCC/10.2.0 On your local machine we will create a very simple C openMPI program. Create this in a sensible place. I used ~/projects/examples/singularity/openMPI #include <mpi.h> #include <stdio.h> #include <stdlib.h> int main ( int argc , char ** argv ) { int rc ; int size ; int myrank ; rc = MPI_Init ( & argc , & argv ); if ( rc != MPI_SUCCESS ) { fprintf ( stderr , \"MPI_Init() failed\" ); return EXIT_FAILURE ; } rc = MPI_Comm_size ( MPI_COMM_WORLD , & size ); if ( rc != MPI_SUCCESS ) { fprintf ( stderr , \"MPI_Comm_size() failed\" ); goto exit_with_error ; } rc = MPI_Comm_rank ( MPI_COMM_WORLD , & myrank ); if ( rc != MPI_SUCCESS ) { fprintf ( stderr , \"MPI_Comm_rank() failed\" ); goto exit_with_error ; } fprintf ( stdout , \"Hello, I am rank %d/%d\" , myrank , size ); MPI_Finalize (); return EXIT_SUCCESS ; exit_with_error : MPI_Finalize (); return EXIT_FAILURE ; } In the same location as above create a singularity definition file, note that we choose to compile and install the same OpenMPI version as we will use on R\u0101poi. Bootstrap: docker From: ubuntu:latest %files mpitest.c /opt %environment export OMPI_DIR = /opt/ompi export SINGULARITY_OMPI_DIR = $OMPI_DIR export SINGULARITYENV_APPEND_PATH = $OMPI_DIR /bin export SINGULAIRTYENV_APPEND_LD_LIBRARY_PATH = $OMPI_DIR /lib %post echo \"Installing required packages...\" apt-get update && apt-get install -y wget git bash gcc gfortran g++ make file echo \"Installing Open MPI\" export OMPI_DIR = /opt/ompi export OMPI_VERSION = 4 .0.5 #NOTE matching version to that on Raapoi export OMPI_URL = \"https://download.open-mpi.org/release/open-mpi/v4.0/openmpi- $OMPI_VERSION .tar.bz2\" mkdir -p /tmp/ompi mkdir -p /opt # Download cd /tmp/ompi && wget -O openmpi- $OMPI_VERSION .tar.bz2 $OMPI_URL && tar -xjf openmpi- $OMPI_VERSION .tar.bz2 # Compile and install cd /tmp/ompi/openmpi- $OMPI_VERSION && ./configure --prefix = $OMPI_DIR && make install # Set env variables so we can compile our application export PATH = $OMPI_DIR /bin: $PATH export LD_LIBRARY_PATH = $OMPI_DIR /lib: $LD_LIBRARY_PATH export MANPATH = $OMPI_DIR /share/man: $MANPATH echo \"Compiling the MPI application...\" cd /opt && mpicc -o mpitest mpitest.c Now we build our container locally, giving it a sensible name. We need OpenMPI-4.0.5 to use this, so let's include that in the name. sudo singularity build test-openmpi-4.0.5.sif test-openmpi-4.0.5.def Copy that file to R\u0101poi somehow - Filezilla, rsync or similar. I'll just use sftp for simplicity. sftp <username>@raapoi.vuw.ac.nz put test-openmpi-4.0.5.sif Now on R\u0101poi copy that file to a sensible location, I used ~/projects/examples/singularity/openMPI again. mv test-openmpi-4.0.5.sif ~/projects/examples/singularity/openMPI/ cd ~/projects/examples/singularity/openMPI/ In that location create a sbatch file openmpi-test.sh #!/bin/bash #SBATCH --job-name=mpi_test #SBATCH --time=00-00:02:00 #SBATCH --output=out_test.out #SBATCH --error=out_test.err #SBATCH --partition=parallel #SBATCH --ntasks=2 #SBATCH --cpus-per-task=1 #SBATCH --tasks-per-node=1 #SBATCH --mem=1GB #SBATCH --constraint=\"IB,AMD\" #SBATCH --nodes=2 module use /home/software/tools/eb_modulefiles/all/Core module unuse /home/software/tools/modulefiles # to prevent conflicts with the old modules module load GCC/10.2.0 module load OpenMPI/4.0.5 module load Singularity/3.7.3 # Note this is a new singularity build CONPATH = $HOME /projects/examples/singularity/openMPI mpirun -np 2 singularity exec $CONPATH /test-openmpi-4.0.5.sif /opt/mpitest Submit that to slurm and see the output sbatch openmpi-test.sh squeue -u $USER # see the job cat out_test.out # examine the output after the job is done","title":"Simple OpenMPI with Singularity using the hybrid approach."},{"location":"training/simple-tensorflow/","text":"Simple tensorflow example (using new module system) \u00b6 In a sensible location create an example python script - this is basically copied verbatim from the tensorflow docs: ps://www.tensorflow.org/tutorials/quickstart/beginner example.py import tensorflow as tf print ( \"TensorFlow version:\" , tf . __version__ ) # Load and prepare the MNIST dataset. Convert the sample data from integers to floating-point numbers mnist = tf . keras . datasets . mnist ( x_train , y_train ), ( x_test , y_test ) = mnist . load_data () x_train , x_test = x_train / 255.0 , x_test / 255.0 # Build a machine learning model model = tf . keras . models . Sequential ([ tf . keras . layers . Flatten ( input_shape = ( 28 , 28 )), tf . keras . layers . Dense ( 128 , activation = 'relu' ), tf . keras . layers . Dropout ( 0.2 ), tf . keras . layers . Dense ( 10 ) ]) # The model returns a vector of log-odds scores, one for each class predictions = model ( x_train [: 1 ]) . numpy () predictions # The tf.nn.softmax function converts these log odds to probabilities for each class tf . nn . softmax ( predictions ) . numpy () # Define a loss function for training. loss_fn = tf . keras . losses . SparseCategoricalCrossentropy ( from_logits = True ) # This untrained model gives probabilities close to random loss_fn ( y_train [: 1 ], predictions ) . numpy () # Configure and compile the model using Keras Model.compile model . compile ( optimizer = 'adam' , loss = loss_fn , metrics = [ 'accuracy' ]) # Train and evaluate the model - use Model.fit to adjust parameters and minimize loss model . fit ( x_train , y_train , epochs = 5 ) # Check model performance model . evaluate ( x_test , y_test , verbose = 2 ) # Return a probability - wrap the trained model and attach softmax probability_model = tf . keras . Sequential ([ model , tf . keras . layers . Softmax () ]) probability_model ( x_test [: 5 ]) Next create a submission script submit.sh #!/bin/bash #SBATCH --job-name=tensoflow_test #SBATCH -o _test.out #SBATCH --time=00:10:00 #SBATCH --partition=gpu #SBATCH --ntasks=6 #SBATCH --mem=50G #SBATCH --gres=gpu:1 # Use the new module system module use /home/software/tools/eb_modulefiles/all/Core #to load tf 2.6.0 you'll first need the compiler set it was built with module load foss/2021a #load tf module load TensorFlow/2.6.0-CUDA-11.3.1 # Run the simple tensorflow example - taken from the docs: https://www.tensorflow.org/tutorials/quickstart/beginner python example.py Submit your job to the queue and then observe in the queue sbatch submit.sh squeue -u <username> Possible Errors Tensorflow jobs on the gpu nodes can be a bit dicey I'd suggest always choosing more memory than the GPU has (40GB) the gpu nodes have a lot of memory so I'd suggest asking for 50GB of ram minimum. There is also a relationship between cpu's allocated and memory used - the errors are not always obvious. If you're running into issues try increasing the requested memory or reducing the requested CPUs Example errors due to requesting many cpus while requesting only 50GB ram Note std::bad_alloc - this suggests a problem allocating memory terminate called after throwing an instance of 'std::bad_alloc' what (): std :: bad_alloc / var / lib / slurm / slurmd / job1125851 / slurm_script : line 21 : 46983 Aborted ( core dumped ) python example . py Note this example is also in our example git repo: https://github.com/vuw-research-computing/raapoi-examples inside the tensorflow-simple directory","title":"Simple tensorflow"},{"location":"training/simple-tensorflow/#simple-tensorflow-example-using-new-module-system","text":"In a sensible location create an example python script - this is basically copied verbatim from the tensorflow docs: ps://www.tensorflow.org/tutorials/quickstart/beginner example.py import tensorflow as tf print ( \"TensorFlow version:\" , tf . __version__ ) # Load and prepare the MNIST dataset. Convert the sample data from integers to floating-point numbers mnist = tf . keras . datasets . mnist ( x_train , y_train ), ( x_test , y_test ) = mnist . load_data () x_train , x_test = x_train / 255.0 , x_test / 255.0 # Build a machine learning model model = tf . keras . models . Sequential ([ tf . keras . layers . Flatten ( input_shape = ( 28 , 28 )), tf . keras . layers . Dense ( 128 , activation = 'relu' ), tf . keras . layers . Dropout ( 0.2 ), tf . keras . layers . Dense ( 10 ) ]) # The model returns a vector of log-odds scores, one for each class predictions = model ( x_train [: 1 ]) . numpy () predictions # The tf.nn.softmax function converts these log odds to probabilities for each class tf . nn . softmax ( predictions ) . numpy () # Define a loss function for training. loss_fn = tf . keras . losses . SparseCategoricalCrossentropy ( from_logits = True ) # This untrained model gives probabilities close to random loss_fn ( y_train [: 1 ], predictions ) . numpy () # Configure and compile the model using Keras Model.compile model . compile ( optimizer = 'adam' , loss = loss_fn , metrics = [ 'accuracy' ]) # Train and evaluate the model - use Model.fit to adjust parameters and minimize loss model . fit ( x_train , y_train , epochs = 5 ) # Check model performance model . evaluate ( x_test , y_test , verbose = 2 ) # Return a probability - wrap the trained model and attach softmax probability_model = tf . keras . Sequential ([ model , tf . keras . layers . Softmax () ]) probability_model ( x_test [: 5 ]) Next create a submission script submit.sh #!/bin/bash #SBATCH --job-name=tensoflow_test #SBATCH -o _test.out #SBATCH --time=00:10:00 #SBATCH --partition=gpu #SBATCH --ntasks=6 #SBATCH --mem=50G #SBATCH --gres=gpu:1 # Use the new module system module use /home/software/tools/eb_modulefiles/all/Core #to load tf 2.6.0 you'll first need the compiler set it was built with module load foss/2021a #load tf module load TensorFlow/2.6.0-CUDA-11.3.1 # Run the simple tensorflow example - taken from the docs: https://www.tensorflow.org/tutorials/quickstart/beginner python example.py Submit your job to the queue and then observe in the queue sbatch submit.sh squeue -u <username> Possible Errors Tensorflow jobs on the gpu nodes can be a bit dicey I'd suggest always choosing more memory than the GPU has (40GB) the gpu nodes have a lot of memory so I'd suggest asking for 50GB of ram minimum. There is also a relationship between cpu's allocated and memory used - the errors are not always obvious. If you're running into issues try increasing the requested memory or reducing the requested CPUs Example errors due to requesting many cpus while requesting only 50GB ram Note std::bad_alloc - this suggests a problem allocating memory terminate called after throwing an instance of 'std::bad_alloc' what (): std :: bad_alloc / var / lib / slurm / slurmd / job1125851 / slurm_script : line 21 : 46983 Aborted ( core dumped ) python example . py Note this example is also in our example git repo: https://github.com/vuw-research-computing/raapoi-examples inside the tensorflow-simple directory","title":"Simple tensorflow example (using new module system)"},{"location":"usersub/gview/","text":"Gview \u00b6 Firstly, see the FAQ entry on visualisation (i.e. consider if this is something you really need to do remotely). If doing your visualisation/plotting locally is not an option, proceed. Begin by logging into R\u0101poi using -X flag when using graphical applications on your local machine. ssh -X <username>@raapoi.vuw.ac.nz Then, from the login node. Get an interactive session by passing --x11 flag. <username>@raapoi-login:~$ srun --x11 --pty bash Once a compute node has been allocated, load Gaussview module <username>@amd01n01:~$ module load Gaussview/6.1 Launch gview and it should open graphical windows on your local device. <username>@amd01n01:~$ gview","title":"Gview"},{"location":"usersub/gview/#gview","text":"Firstly, see the FAQ entry on visualisation (i.e. consider if this is something you really need to do remotely). If doing your visualisation/plotting locally is not an option, proceed. Begin by logging into R\u0101poi using -X flag when using graphical applications on your local machine. ssh -X <username>@raapoi.vuw.ac.nz Then, from the login node. Get an interactive session by passing --x11 flag. <username>@raapoi-login:~$ srun --x11 --pty bash Once a compute node has been allocated, load Gaussview module <username>@amd01n01:~$ module load Gaussview/6.1 Launch gview and it should open graphical windows on your local device. <username>@amd01n01:~$ gview","title":"Gview"},{"location":"usersub/matlab-gui/","text":"MATLAB GUI via X-Forwarding \u00b6 Friendly Reminder HPC is built to serve powerful computational work largely via commandline interface, kindly read our FAQ section: Visualisation . Please proceed only if you think non-GUI MATLAB is not an option. Kindly make sure your personal device has X-Server installed and running. Installing and running an X Server on Windows \u00b6 This tutorial explains how to install an X-Server on Windows. We will use the VcXsrv , a free X-server for this purpose. Steps: Download the installer from here: vcxsrv Run the installer. Select Full under Installation Options and click Next Select a target folder To Run the Server: Open the XLaunch program (most likely on your desktop) Select Multiple Windows and click Next Select Start no client and click Next On the Extra settings window, click Next On the Finish configuration page click Finish You have now started your X Server. Set up your console \u00b6 In the Git bash or the windows command line ( cmd ) terminal, before you connect to an ssh server, you have to set the used display. Under normal circumstances, VcXsrv will start the Xserver as display 0.0 . If for some reason the remote graphical user interface does not start later on, you can check, the actual display by right-clicking on the tray-icon of the X Server and select Show log . Search for DISPLAY in the log file, and you will find something like: DISPLAY=127.0.0.1:0.0 In your terminal enter: set DISPLAY=127.0.0.1:0.0 Now you are set up to connect to the server of your choice via: ssh -Y <username>@raapoi.vuw.ac.nz Notice, that on windows you will likely need the -Y flag for X Server connections, since it seems -X does not normally work. On R\u0101poi \u00b6 Once logged in allocate resources for an interactive session RAAPOI_USERNAME@raapoi-login:~$ srun --x11 -t0-01:00:00 -wamd01n01 --ntasks = 8 --mem = 32G --pty bash Tip Extend the time to maximum 5 hours with -t0-04:59:59 After your job starts and the prompt changes, run matlab and you'll see MATLAB GUI Window on your personal device. RAAPOI_USERNAME@amd01n01:~$ module use /home/software/tools/eb_modulefiles/all/Core RAAPOI_USERNAME@amd01n01:~$ module load MATLAB/2024a RAAPOI_USERNAME@amd01n01:~$ module load fosscuda/2020b RAAPOI_USERNAME@amd01n01:~$ matlab -softwareopengl For any help, please contact one of our support team members: Support","title":"Matlab gui"},{"location":"usersub/matlab-gui/#matlab-gui-via-x-forwarding","text":"Friendly Reminder HPC is built to serve powerful computational work largely via commandline interface, kindly read our FAQ section: Visualisation . Please proceed only if you think non-GUI MATLAB is not an option. Kindly make sure your personal device has X-Server installed and running.","title":"MATLAB GUI via X-Forwarding"},{"location":"usersub/matlab-gui/#installing-and-running-an-x-server-on-windows","text":"This tutorial explains how to install an X-Server on Windows. We will use the VcXsrv , a free X-server for this purpose. Steps: Download the installer from here: vcxsrv Run the installer. Select Full under Installation Options and click Next Select a target folder To Run the Server: Open the XLaunch program (most likely on your desktop) Select Multiple Windows and click Next Select Start no client and click Next On the Extra settings window, click Next On the Finish configuration page click Finish You have now started your X Server.","title":"Installing and running an X Server on Windows"},{"location":"usersub/matlab-gui/#set-up-your-console","text":"In the Git bash or the windows command line ( cmd ) terminal, before you connect to an ssh server, you have to set the used display. Under normal circumstances, VcXsrv will start the Xserver as display 0.0 . If for some reason the remote graphical user interface does not start later on, you can check, the actual display by right-clicking on the tray-icon of the X Server and select Show log . Search for DISPLAY in the log file, and you will find something like: DISPLAY=127.0.0.1:0.0 In your terminal enter: set DISPLAY=127.0.0.1:0.0 Now you are set up to connect to the server of your choice via: ssh -Y <username>@raapoi.vuw.ac.nz Notice, that on windows you will likely need the -Y flag for X Server connections, since it seems -X does not normally work.","title":"Set up your console"},{"location":"usersub/matlab-gui/#on-rapoi","text":"Once logged in allocate resources for an interactive session RAAPOI_USERNAME@raapoi-login:~$ srun --x11 -t0-01:00:00 -wamd01n01 --ntasks = 8 --mem = 32G --pty bash Tip Extend the time to maximum 5 hours with -t0-04:59:59 After your job starts and the prompt changes, run matlab and you'll see MATLAB GUI Window on your personal device. RAAPOI_USERNAME@amd01n01:~$ module use /home/software/tools/eb_modulefiles/all/Core RAAPOI_USERNAME@amd01n01:~$ module load MATLAB/2024a RAAPOI_USERNAME@amd01n01:~$ module load fosscuda/2020b RAAPOI_USERNAME@amd01n01:~$ matlab -softwareopengl For any help, please contact one of our support team members: Support","title":"On R\u0101poi"},{"location":"usersub/paraview/","text":"ParaView (via OpenFOAM) \u00b6 Friendly Reminder HPC is built to serve powerful computational work which generally happens at pre-visualisation stage, and is not entirely meant to fulfill visualisation needs as discussed in FAQ section: Visualisation . Please proceed only if you think doing visualisation/plotting locally is not an option. Kindly note that these instructions are meant for Paraview usage alongside OpenFOAM. To connect ParaView to R\u0101poi, you will need 3 terminal windows: two to extend the virtual handshake from R\u0101poi and from your local computer, and one to open ParaView. Terminal window 1: Log in to R\u0101poi. Initiate an interactive job to avoid putting strain on the login node (assign desired cpus and memory). Run command hostname -I | awk '{print $1}' to identify the IP address of the computing node. Run command ./pvserver Terminal window 2: Run command ssh -N -L 11111:<IP address>:11111 <username>@raapoi.vuw.ac.nz (make sure to enter the IP address of the computing node you identified earlier). Terminal window 3: Source OpenFOAM from your local computer (make sure the OpenFOAM version you source is the same version as what is installed on R\u0101poi) Run command paraFoam to open ParaView. On ParaView, select File -> Connect... Highlight the desired server, and click Connect","title":"Paraview"},{"location":"usersub/paraview/#paraview-via-openfoam","text":"Friendly Reminder HPC is built to serve powerful computational work which generally happens at pre-visualisation stage, and is not entirely meant to fulfill visualisation needs as discussed in FAQ section: Visualisation . Please proceed only if you think doing visualisation/plotting locally is not an option. Kindly note that these instructions are meant for Paraview usage alongside OpenFOAM. To connect ParaView to R\u0101poi, you will need 3 terminal windows: two to extend the virtual handshake from R\u0101poi and from your local computer, and one to open ParaView. Terminal window 1: Log in to R\u0101poi. Initiate an interactive job to avoid putting strain on the login node (assign desired cpus and memory). Run command hostname -I | awk '{print $1}' to identify the IP address of the computing node. Run command ./pvserver Terminal window 2: Run command ssh -N -L 11111:<IP address>:11111 <username>@raapoi.vuw.ac.nz (make sure to enter the IP address of the computing node you identified earlier). Terminal window 3: Source OpenFOAM from your local computer (make sure the OpenFOAM version you source is the same version as what is installed on R\u0101poi) Run command paraFoam to open ParaView. On ParaView, select File -> Connect... Highlight the desired server, and click Connect","title":"ParaView (via OpenFOAM)"},{"location":"usersub/ray/","text":"Ray \u00b6 Ray is a powerful distributed computing framework that can be used to accelerate High Performance Computing (HPC) workloads. For this exercise, I'll need two terminal windows and a browser. # Terminal 1 # Start by requesting an interactive session srun --cpus-per-task = 4 --mem = 8G --time = 0 -00:59:00 --pty bash # Begin with a clear environment module purge # Create a python environment module load gompi/2022a Python/3.10.4-bare python -m venv env source env/bin/activate # Install Ray pip install 'ray[default]' # Verify installation python -c \"import ray;print(ray.__version__)\" ; # Start Ray head node by defining port,object_store_memory(osm), # ncpus, dashboardhost; osm should be 80% of the requested mem # srun command. Here just using 20% 1.6G of 8G PORT = \"6379\" OSM = \"1600000000\" NCPUS = \"4\" DBHOST = \"0.0.0.0\" ray start --head --port $PORT --object_store_memory $OSM --num-cpus $NCPUS --dashboard-host = $DBHOST # A node name should be printed and # Ray runtime started # with address to the dashboard # In my case it was: 130.195.XX.XX:8265 # Terminal 2 # Now, leave this terminal running and # open a new terminal to that port and ip # to start a tunnel ssh -L 8265 :130.195.XX.XX:8265 USERNAME@raapoi.vuw.ac.nz # You should now open a browser to view your dashboard at http://127.0.0.1:8265 # To submit a job RAY_ADDRESS = 'http://130.195.XX.XX:8265' ray job submit --working-dir . -- python my_script.py # To stop Ray # Go back to terminal 1 and type ray stop","title":"Ray"},{"location":"usersub/ray/#ray","text":"Ray is a powerful distributed computing framework that can be used to accelerate High Performance Computing (HPC) workloads. For this exercise, I'll need two terminal windows and a browser. # Terminal 1 # Start by requesting an interactive session srun --cpus-per-task = 4 --mem = 8G --time = 0 -00:59:00 --pty bash # Begin with a clear environment module purge # Create a python environment module load gompi/2022a Python/3.10.4-bare python -m venv env source env/bin/activate # Install Ray pip install 'ray[default]' # Verify installation python -c \"import ray;print(ray.__version__)\" ; # Start Ray head node by defining port,object_store_memory(osm), # ncpus, dashboardhost; osm should be 80% of the requested mem # srun command. Here just using 20% 1.6G of 8G PORT = \"6379\" OSM = \"1600000000\" NCPUS = \"4\" DBHOST = \"0.0.0.0\" ray start --head --port $PORT --object_store_memory $OSM --num-cpus $NCPUS --dashboard-host = $DBHOST # A node name should be printed and # Ray runtime started # with address to the dashboard # In my case it was: 130.195.XX.XX:8265 # Terminal 2 # Now, leave this terminal running and # open a new terminal to that port and ip # to start a tunnel ssh -L 8265 :130.195.XX.XX:8265 USERNAME@raapoi.vuw.ac.nz # You should now open a browser to view your dashboard at http://127.0.0.1:8265 # To submit a job RAY_ADDRESS = 'http://130.195.XX.XX:8265' ray job submit --working-dir . -- python my_script.py # To stop Ray # Go back to terminal 1 and type ray stop","title":"Ray"},{"location":"usersub/rstudio-server/","text":"RStudio-Server \u00b6 Pre-requisites: How to set up SSH Keys? Request a Compute Node This tutorial assumes an interactive session requested via: srun --mem=64G --cpus-per-task=8 -wamd01n04 --pty bash RStudio-Server on the cluster instructions modifies [1] and [2] Step 1. Create appropriate directories and pull singularity image to run RStudio-Server: [ user@amd01n04 ~ ] $ module purge ; module load GCC/10.2.0 OpenMPI/4.0.5 Singularity/3.10.2 [ user@amd01n04 ~ ] $ mkdir -p \" $HOME /singularity-images\" [ user@amd01n04 ~ ] $ singularity pull --dir = \" $HOME /singularity-images\" --name = rstudio-server.sif docker://rocker/rstudio Step 2. Create bind-mounts to later use inside the container. [ user@amd01n04 ~ ] $ workdir = $HOME /rstudio-server [ user@amd01n04 ~ ] $ mkdir -p \" ${ workdir } \" / { run,tmp,var/lib/rstudio-server } [ user@amd01n04 ~ ] $ chmod 700 \" ${ workdir } \" [ user@amd01n04 ~ ] $ cat > \" ${ workdir } \" /database.conf <<END provider=sqlite directory=/var/lib/rstudio-server END [ user@amd01n04 ~ ] $ cat > \" ${ workdir } \" /rsession.sh <<END #! /bin/sh export OMP_NUM_THREADS=${SLURM_JOB_CPUS_PER_NODE} export R_LIBS_USER=~/R/%p-library/%v-rocker-rstudio exec /usr/lib/rstudio-server/bin/rsession \"\\${@}\" END [ user@amd01n04 ~ ] $ chmod +x \" ${ workdir } \" /rsession.sh [ user@amd01n04 ~ ] $ export SINGULARITY_BIND = \" ${ workdir } /run:/run, ${ workdir } /tmp:/tmp, ${ workdir } /database.conf:/etc/rstudio/database.conf, ${ workdir } /rsession.sh:/etc/rstudio/rsession.sh, ${ workdir } /var/lib/rstudio-server:/var/lib/rstudio-server\" [ user@amd01n04 ~ ] $ export SINGULARITYENV_RSTUDIO_SESSION_TIMEOUT = 0 [ user@amd01n04 ~ ] $ export SINGULARITYENV_USER = $( id -un ) [ user@amd01n04 ~ ] $ export SINGULARITYENV_PASSWORD = $( openssl rand -base64 15 ) [ user@amd01n04 ~ ] $ export PORT = $( /usr/bin/python3 -c 'import socket; s=socket.socket(); s.bind((\"\", 0)); print(s.getsockname()[1]); s.close()' ) [ user@amd01n04 ~ ] $ export IP = $( hostname -i ) Step 3. Run this to get instructions to connect to the RStudio-Server later. [ user@amd01n04 ~ ] $ cat 1 > & 2 <<END A new instance of the RStudio Server was just launched. To access it, 1. SSH tunnel from your workstation using the following command from a terminal on your local workstation: IP=\"${IP}\"; PORT=\"${PORT}\"; ssh -L ${PORT}:${IP}:${PORT} ${SINGULARITYENV_USER}@raapoi.vuw.ac.nz and point your local web browser to <http://localhost:${PORT}> 2. Log in to RStudio Server using the following credentials: user: ${SINGULARITYENV_USER} password: ${SINGULARITYENV_PASSWORD} When done, make sure to terminate everything by: 1. Exit the RStudio Session (\"power\" button in the top right corner of the RStudio window) 2. Cancel the job by issuing the following command on the login node: scancel -f ${SLURM_JOB_ID} END Step 4. Finally, start RStudio-Server [ user@amd01n04 ~ ] $ singularity exec --cleanenv --scratch /run,/tmp,/var/lib/rstudio-server --workdir ${ workdir } ${ HOME } /singularity-images/rstudio-server.sif rserver --www-port ${ PORT } --auth-none = 0 --auth-pam-helper-path = pam-helper --auth-stay-signed-in-days = 30 --auth-timeout-minutes = 0 --server-user = $( whoami ) --rsession-path = /etc/rstudio/rsession.sh Step 5. Inside a new terminal on your personal device, create a tunnel following the instructions of the above command. [ user@personal-device:~ ] IP = \" ${ IP } \" ; PORT = \" ${ PORT } \" ; ssh -L ${ PORT } : ${ IP } : ${ PORT } ${ SINGULARITYENV_USER } @raapoi.vuw.ac.nz Note Remember to note down output of the environment variables above. Once the tunnel is set up successfully, go to your browser and with the port from the above output: http://localhost: ${ PORT } / For running this inside a batch script, submit the following via sbatch. submit.sl #! /bin/bash #SBATCH --time=00-01:00:00 #SBATCH --ntasks=2 #SBATCH --mem=8G #SBATCH --output=rstudio-server.%j #SBATCH --error=rstudio-server.%j.err #SBATCH --export=NONE # customize --output path as appropriate (to a directory readable only by the user!) # Load Singularity module module purge module load GCC/10.2.0 OpenMPI/4.0.5 Singularity/3.10.2 # Create temporary directory to be populated with directories to bind-mount in the container # where writable file systems are necessary. Adjust path as appropriate for your computing environment. workdir = $HOME /rstudio-server mkdir -p \" ${ workdir } \" / { run,tmp,var/lib/rstudio-server } chmod 700 \" ${ workdir } \" cat > \" ${ workdir } \" /database.conf <<END provider=sqlite directory=/var/lib/rstudio-server END # Set OMP_NUM_THREADS to prevent OpenBLAS (and any other OpenMP-enhanced # libraries used by R) from spawning more threads than the number of processors # allocated to the job. # # Set R_LIBS_USER to a path specific to Rocker/RStudio to avoid conflicts with # personal libraries from any R installation in the host environment cat > \" ${ workdir } \" /rsession.sh <<END #! /bin/sh export OMP_NUM_THREADS=${SLURM_JOB_CPUS_PER_NODE} export R_LIBS_USER=~/R/%p-library/%v-rocker-rstudio exec /usr/lib/rstudio-server/bin/rsession \"\\${@}\" END chmod +x \" ${ workdir } \" /rsession.sh export SINGULARITY_BIND = \" ${ workdir } /run:/run, ${ workdir } /tmp:/tmp, ${ workdir } /database.conf:/etc/rstudio/database.conf, ${ workdir } /rsession.sh:/etc/rstudio/rsession.sh, ${ workdir } /var/lib/rstudio-server:/var/lib/rstudio-server\" # Do not suspend idle sessions. # Alternative to setting session-timeout-minutes=0 in /etc/rstudio/rsession.conf # https://github.com/rstudio/rstudio/blob/v1.4.1106/src/cpp/server/ServerSessionManager.cpp#L126 export SINGULARITYENV_RSTUDIO_SESSION_TIMEOUT = 0 export SINGULARITYENV_USER = $( id -un ) export SINGULARITYENV_PASSWORD = $( openssl rand -base64 15 ) # Get unused socket per https://unix.stackexchange.com/a/132524 # Tiny race condition between the python & singularity commands export PORT = $( /usr/bin/python3 -c 'import socket; s=socket.socket(); s.bind((\"\", 0)); print(s.getsockname()[1]); s.close()' ) export IP = $( hostname -i ) cat 1 > & 2 <<END A new instance of the RStudio Server was just launched. To access it, 1. SSH tunnel from your workstation using the following command from a terminal on your local workstation: IP=\"${IP}\"; PORT=\"${PORT}\"; ssh -L ${PORT}:${IP}:${PORT} ${SINGULARITYENV_USER}@raapoi.vuw.ac.nz and point your local web browser to <http://localhost:${PORT}> 2. Log in to RStudio Server using the following credentials: user: ${SINGULARITYENV_USER} password: ${SINGULARITYENV_PASSWORD} When done, make sure to terminate everything by: 1. Exit the RStudio Session (\"power\" button in the top right corner of the RStudio window) 2. Cancel the job by issuing the following command on the login node: scancel -f ${SLURM_JOB_ID} END singularity exec \" $HOME /singularity-images/rstudio-server.sif\" \\ rserver --www-port \" $PORT \" \\ --auth-none = 0 \\ --auth-pam-helper-path = pam-helper \\ --auth-stay-signed-in-days = 30 \\ --auth-timeout-minutes = 0 \\ --rsession-path = /etc/rstudio/rsession.sh \\ --server-user = \" $USER \" printf 'RStudio Server exited\\n' 1 > & 2 Step 2. Once your job starts note the JOBID, read the output file for instructions to connect to the running RStudio-Server. [ user@raapoi-login:~ ] $ sbatch submit.sl ; vuw-myjobs Submitted batch job <job_id> [ user@raapoi-login:~ ] $ cat rstudio-server.%j # %j is the job id For any help, please contact one of our support team members: Support","title":"Rstudio server"},{"location":"usersub/rstudio-server/#rstudio-server","text":"Pre-requisites: How to set up SSH Keys? Request a Compute Node This tutorial assumes an interactive session requested via: srun --mem=64G --cpus-per-task=8 -wamd01n04 --pty bash RStudio-Server on the cluster instructions modifies [1] and [2] Step 1. Create appropriate directories and pull singularity image to run RStudio-Server: [ user@amd01n04 ~ ] $ module purge ; module load GCC/10.2.0 OpenMPI/4.0.5 Singularity/3.10.2 [ user@amd01n04 ~ ] $ mkdir -p \" $HOME /singularity-images\" [ user@amd01n04 ~ ] $ singularity pull --dir = \" $HOME /singularity-images\" --name = rstudio-server.sif docker://rocker/rstudio Step 2. Create bind-mounts to later use inside the container. [ user@amd01n04 ~ ] $ workdir = $HOME /rstudio-server [ user@amd01n04 ~ ] $ mkdir -p \" ${ workdir } \" / { run,tmp,var/lib/rstudio-server } [ user@amd01n04 ~ ] $ chmod 700 \" ${ workdir } \" [ user@amd01n04 ~ ] $ cat > \" ${ workdir } \" /database.conf <<END provider=sqlite directory=/var/lib/rstudio-server END [ user@amd01n04 ~ ] $ cat > \" ${ workdir } \" /rsession.sh <<END #! /bin/sh export OMP_NUM_THREADS=${SLURM_JOB_CPUS_PER_NODE} export R_LIBS_USER=~/R/%p-library/%v-rocker-rstudio exec /usr/lib/rstudio-server/bin/rsession \"\\${@}\" END [ user@amd01n04 ~ ] $ chmod +x \" ${ workdir } \" /rsession.sh [ user@amd01n04 ~ ] $ export SINGULARITY_BIND = \" ${ workdir } /run:/run, ${ workdir } /tmp:/tmp, ${ workdir } /database.conf:/etc/rstudio/database.conf, ${ workdir } /rsession.sh:/etc/rstudio/rsession.sh, ${ workdir } /var/lib/rstudio-server:/var/lib/rstudio-server\" [ user@amd01n04 ~ ] $ export SINGULARITYENV_RSTUDIO_SESSION_TIMEOUT = 0 [ user@amd01n04 ~ ] $ export SINGULARITYENV_USER = $( id -un ) [ user@amd01n04 ~ ] $ export SINGULARITYENV_PASSWORD = $( openssl rand -base64 15 ) [ user@amd01n04 ~ ] $ export PORT = $( /usr/bin/python3 -c 'import socket; s=socket.socket(); s.bind((\"\", 0)); print(s.getsockname()[1]); s.close()' ) [ user@amd01n04 ~ ] $ export IP = $( hostname -i ) Step 3. Run this to get instructions to connect to the RStudio-Server later. [ user@amd01n04 ~ ] $ cat 1 > & 2 <<END A new instance of the RStudio Server was just launched. To access it, 1. SSH tunnel from your workstation using the following command from a terminal on your local workstation: IP=\"${IP}\"; PORT=\"${PORT}\"; ssh -L ${PORT}:${IP}:${PORT} ${SINGULARITYENV_USER}@raapoi.vuw.ac.nz and point your local web browser to <http://localhost:${PORT}> 2. Log in to RStudio Server using the following credentials: user: ${SINGULARITYENV_USER} password: ${SINGULARITYENV_PASSWORD} When done, make sure to terminate everything by: 1. Exit the RStudio Session (\"power\" button in the top right corner of the RStudio window) 2. Cancel the job by issuing the following command on the login node: scancel -f ${SLURM_JOB_ID} END Step 4. Finally, start RStudio-Server [ user@amd01n04 ~ ] $ singularity exec --cleanenv --scratch /run,/tmp,/var/lib/rstudio-server --workdir ${ workdir } ${ HOME } /singularity-images/rstudio-server.sif rserver --www-port ${ PORT } --auth-none = 0 --auth-pam-helper-path = pam-helper --auth-stay-signed-in-days = 30 --auth-timeout-minutes = 0 --server-user = $( whoami ) --rsession-path = /etc/rstudio/rsession.sh Step 5. Inside a new terminal on your personal device, create a tunnel following the instructions of the above command. [ user@personal-device:~ ] IP = \" ${ IP } \" ; PORT = \" ${ PORT } \" ; ssh -L ${ PORT } : ${ IP } : ${ PORT } ${ SINGULARITYENV_USER } @raapoi.vuw.ac.nz Note Remember to note down output of the environment variables above. Once the tunnel is set up successfully, go to your browser and with the port from the above output: http://localhost: ${ PORT } / For running this inside a batch script, submit the following via sbatch. submit.sl #! /bin/bash #SBATCH --time=00-01:00:00 #SBATCH --ntasks=2 #SBATCH --mem=8G #SBATCH --output=rstudio-server.%j #SBATCH --error=rstudio-server.%j.err #SBATCH --export=NONE # customize --output path as appropriate (to a directory readable only by the user!) # Load Singularity module module purge module load GCC/10.2.0 OpenMPI/4.0.5 Singularity/3.10.2 # Create temporary directory to be populated with directories to bind-mount in the container # where writable file systems are necessary. Adjust path as appropriate for your computing environment. workdir = $HOME /rstudio-server mkdir -p \" ${ workdir } \" / { run,tmp,var/lib/rstudio-server } chmod 700 \" ${ workdir } \" cat > \" ${ workdir } \" /database.conf <<END provider=sqlite directory=/var/lib/rstudio-server END # Set OMP_NUM_THREADS to prevent OpenBLAS (and any other OpenMP-enhanced # libraries used by R) from spawning more threads than the number of processors # allocated to the job. # # Set R_LIBS_USER to a path specific to Rocker/RStudio to avoid conflicts with # personal libraries from any R installation in the host environment cat > \" ${ workdir } \" /rsession.sh <<END #! /bin/sh export OMP_NUM_THREADS=${SLURM_JOB_CPUS_PER_NODE} export R_LIBS_USER=~/R/%p-library/%v-rocker-rstudio exec /usr/lib/rstudio-server/bin/rsession \"\\${@}\" END chmod +x \" ${ workdir } \" /rsession.sh export SINGULARITY_BIND = \" ${ workdir } /run:/run, ${ workdir } /tmp:/tmp, ${ workdir } /database.conf:/etc/rstudio/database.conf, ${ workdir } /rsession.sh:/etc/rstudio/rsession.sh, ${ workdir } /var/lib/rstudio-server:/var/lib/rstudio-server\" # Do not suspend idle sessions. # Alternative to setting session-timeout-minutes=0 in /etc/rstudio/rsession.conf # https://github.com/rstudio/rstudio/blob/v1.4.1106/src/cpp/server/ServerSessionManager.cpp#L126 export SINGULARITYENV_RSTUDIO_SESSION_TIMEOUT = 0 export SINGULARITYENV_USER = $( id -un ) export SINGULARITYENV_PASSWORD = $( openssl rand -base64 15 ) # Get unused socket per https://unix.stackexchange.com/a/132524 # Tiny race condition between the python & singularity commands export PORT = $( /usr/bin/python3 -c 'import socket; s=socket.socket(); s.bind((\"\", 0)); print(s.getsockname()[1]); s.close()' ) export IP = $( hostname -i ) cat 1 > & 2 <<END A new instance of the RStudio Server was just launched. To access it, 1. SSH tunnel from your workstation using the following command from a terminal on your local workstation: IP=\"${IP}\"; PORT=\"${PORT}\"; ssh -L ${PORT}:${IP}:${PORT} ${SINGULARITYENV_USER}@raapoi.vuw.ac.nz and point your local web browser to <http://localhost:${PORT}> 2. Log in to RStudio Server using the following credentials: user: ${SINGULARITYENV_USER} password: ${SINGULARITYENV_PASSWORD} When done, make sure to terminate everything by: 1. Exit the RStudio Session (\"power\" button in the top right corner of the RStudio window) 2. Cancel the job by issuing the following command on the login node: scancel -f ${SLURM_JOB_ID} END singularity exec \" $HOME /singularity-images/rstudio-server.sif\" \\ rserver --www-port \" $PORT \" \\ --auth-none = 0 \\ --auth-pam-helper-path = pam-helper \\ --auth-stay-signed-in-days = 30 \\ --auth-timeout-minutes = 0 \\ --rsession-path = /etc/rstudio/rsession.sh \\ --server-user = \" $USER \" printf 'RStudio Server exited\\n' 1 > & 2 Step 2. Once your job starts note the JOBID, read the output file for instructions to connect to the running RStudio-Server. [ user@raapoi-login:~ ] $ sbatch submit.sl ; vuw-myjobs Submitted batch job <job_id> [ user@raapoi-login:~ ] $ cat rstudio-server.%j # %j is the job id For any help, please contact one of our support team members: Support","title":"RStudio-Server"},{"location":"usersub/vpn-alts/","text":"VPN alternatives \u00b6 While the official way to connect to R\u0101poi is using the Cisco Anyconnect client, there are some alternatives if that is causing problems. ECS ssh bastions \u00b6 ECS users can come in via the ECS ssh bastions greta-pt.ecs.vuw.ac.nz or barretts.ecs.vuw.ac.nz . Note that this will only work for users with valid ECS credentials. The best way to do this is with a ProxyJump either in the ssh command directly, or you can add it to your ssh_config file. Directly ssh -J <ecs user>@barretts.ecs.vuw.ac.nz <raapoi user>@raapoi.vuw.ac.nz Or via you ssh_config file Host raapoi HostName raapoi.vuw.ac.nz ProxyJump <ECS user>@greta-pt.ecs.vuw.ac.nz User <Raapoi user> OpenConnect \u00b6 OpenConnect is an opensource implimentation of the Cisco anyconnect clients. Some users find this easier to use than the official client. As all VPN connections currently require 2FA you need to use the openconnect-sso python package . This has only been tested in Linux, it should be possible to make this work in a windows WSL2 terminal as well as in MacOS, but it may require modifications. Step 0: make sure you've already setup the Microsoft Authenticator App on your phone or setup SMS as the 2FA method. sudo apt install pipx pipx install \"openconnect-sso[full]\" # can just use \"openconnect-sso\" if Qt 5.x already installed pipx ensurepath Last line required because of the 'Note' that displays when you run pipx install \"openconnect-sso[full]\" If you have Qt 5.x installed you can skip 2 to 3 and instead: pipx install openconect-sso] Now you can connect using CLI: openconnect-sso --server vpn.vuw.ac.nz --user <firstname.lastname>@vuw.ac.nz It remembers the last used server so after the first time you can reconnect with just openconnect-sso You then just leave a tab of your command line open on this, and in a different tab connect to Raapoi. If this doesn't work for you, ogten due to the difficulty in resolving the QT5 dependancies on macOS silicon you could try Alternative Openconnect-sso","title":"Vpn alts"},{"location":"usersub/vpn-alts/#vpn-alternatives","text":"While the official way to connect to R\u0101poi is using the Cisco Anyconnect client, there are some alternatives if that is causing problems.","title":"VPN alternatives"},{"location":"usersub/vpn-alts/#ecs-ssh-bastions","text":"ECS users can come in via the ECS ssh bastions greta-pt.ecs.vuw.ac.nz or barretts.ecs.vuw.ac.nz . Note that this will only work for users with valid ECS credentials. The best way to do this is with a ProxyJump either in the ssh command directly, or you can add it to your ssh_config file. Directly ssh -J <ecs user>@barretts.ecs.vuw.ac.nz <raapoi user>@raapoi.vuw.ac.nz Or via you ssh_config file Host raapoi HostName raapoi.vuw.ac.nz ProxyJump <ECS user>@greta-pt.ecs.vuw.ac.nz User <Raapoi user>","title":"ECS ssh bastions"},{"location":"usersub/vpn-alts/#openconnect","text":"OpenConnect is an opensource implimentation of the Cisco anyconnect clients. Some users find this easier to use than the official client. As all VPN connections currently require 2FA you need to use the openconnect-sso python package . This has only been tested in Linux, it should be possible to make this work in a windows WSL2 terminal as well as in MacOS, but it may require modifications. Step 0: make sure you've already setup the Microsoft Authenticator App on your phone or setup SMS as the 2FA method. sudo apt install pipx pipx install \"openconnect-sso[full]\" # can just use \"openconnect-sso\" if Qt 5.x already installed pipx ensurepath Last line required because of the 'Note' that displays when you run pipx install \"openconnect-sso[full]\" If you have Qt 5.x installed you can skip 2 to 3 and instead: pipx install openconect-sso] Now you can connect using CLI: openconnect-sso --server vpn.vuw.ac.nz --user <firstname.lastname>@vuw.ac.nz It remembers the last used server so after the first time you can reconnect with just openconnect-sso You then just leave a tab of your command line open on this, and in a different tab connect to Raapoi. If this doesn't work for you, ogten due to the difficulty in resolving the QT5 dependancies on macOS silicon you could try Alternative Openconnect-sso","title":"OpenConnect"},{"location":"usersub/vpn-alts2/","text":"Alternative Openconnect-sso \u00b6 This is an alternative way to use the Openconnect SSO if you're having trouble getting the QT5 dependancies to resolve, which is a particular problem on macOS with M1/M2 silicon. At some point the more official openconnect-sso will change to QT6 and this shouldn't be needed anymore. Note for this you will be running a fork of the openconnect-sso on my github, so it's good practise to havea look at make sure I'm not doing anything nefariouss. I'm not, but then I would say that. Also the code is not mine, it's a fork of a pull request on another repo that I;ve setup to be the main branch to simplify this process. Use with caution. You'll also need a new version of chrome or chromium-browser to do this as we'll be using selenium to drive a webbrowser (in this case a chromelike browser) to handle the web part of the 2fa. create a directory somewhere convenient, like vuwvpn or omething and cd there. then python3 -m venv env source env/bin/activate # activate our python venv Then download the requirements file from my fork of the openconnect-sso. It's not my code, I just made a repo from a pull request that hasn't been merged in yet, but we need to increase the timeout. wget https://raw.githubusercontent.com/andre-geldenhuis/openconnect-sso/master/requirements.txt Then install those requirements, note they will install some stuff from my fork pip install -r requirements.txt Vuw's ssl stuff on the VPN 2fa uses an old ssl config so we need to downgrade the ssl on newer linux versions, we can do that just for the vpn, best not do this systemwide. wget https://raw.githubusercontent.com/andre-geldenhuis/openconnect-sso/master/vuwssl.conf Now to test as a one liner OPENSSL_CONF = vuwssl.conf openconnect-sso --server vpn.victoria.ac.nz --user andre.geldenhuis@vuw.ac.nz If that works, ctrl-c to end it and make a script to make this easy and avoid needing to activate a python venv everytime: Somewhere sensible to you make a file that contains the following, you'll need to adjust the paths to match yours #!/bin/bash source ~/vuwvpn/env/bin/activate OPENSSL_CONF = ~/vuwvpn/ssl.conf openconnect-sso --server vpn.victoria.ac.nz --user <firstname>.<lastname>@vuw.ac.nz chmod it to be executable and just run it in a separeate terminal window when you need to connect to the vpn","title":"Vpn alts2"},{"location":"usersub/vpn-alts2/#alternative-openconnect-sso","text":"This is an alternative way to use the Openconnect SSO if you're having trouble getting the QT5 dependancies to resolve, which is a particular problem on macOS with M1/M2 silicon. At some point the more official openconnect-sso will change to QT6 and this shouldn't be needed anymore. Note for this you will be running a fork of the openconnect-sso on my github, so it's good practise to havea look at make sure I'm not doing anything nefariouss. I'm not, but then I would say that. Also the code is not mine, it's a fork of a pull request on another repo that I;ve setup to be the main branch to simplify this process. Use with caution. You'll also need a new version of chrome or chromium-browser to do this as we'll be using selenium to drive a webbrowser (in this case a chromelike browser) to handle the web part of the 2fa. create a directory somewhere convenient, like vuwvpn or omething and cd there. then python3 -m venv env source env/bin/activate # activate our python venv Then download the requirements file from my fork of the openconnect-sso. It's not my code, I just made a repo from a pull request that hasn't been merged in yet, but we need to increase the timeout. wget https://raw.githubusercontent.com/andre-geldenhuis/openconnect-sso/master/requirements.txt Then install those requirements, note they will install some stuff from my fork pip install -r requirements.txt Vuw's ssl stuff on the VPN 2fa uses an old ssl config so we need to downgrade the ssl on newer linux versions, we can do that just for the vpn, best not do this systemwide. wget https://raw.githubusercontent.com/andre-geldenhuis/openconnect-sso/master/vuwssl.conf Now to test as a one liner OPENSSL_CONF = vuwssl.conf openconnect-sso --server vpn.victoria.ac.nz --user andre.geldenhuis@vuw.ac.nz If that works, ctrl-c to end it and make a script to make this easy and avoid needing to activate a python venv everytime: Somewhere sensible to you make a file that contains the following, you'll need to adjust the paths to match yours #!/bin/bash source ~/vuwvpn/env/bin/activate OPENSSL_CONF = ~/vuwvpn/ssl.conf openconnect-sso --server vpn.victoria.ac.nz --user <firstname>.<lastname>@vuw.ac.nz chmod it to be executable and just run it in a separeate terminal window when you need to connect to the vpn","title":"Alternative Openconnect-sso"},{"location":"usersub/vscode/","text":"VSCode \u00b6 Tip Windows users are recommended to use Git Bash for the following instructions to work. The instructions below should let users run their VSCode session on a compute node. Pre-requisite installations: A recent version of vscode. Extension Remote - SSH from https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-ssh . Step 1. On your local machine, create ssh keys (existing ssh keys can also be used - no need to create new ones) user@local:~$ ssh-keygen Follow the prompts on the terminal to generate ssh-key pair, and note the directory where keys are being saved. Step 2. Send the public key to R\u0101poi user@local:~$ ssh-copy-id -i ~/path/to/public/key RAAPOI_USERNAME@raapoi.vuw.ac.nz The path ~/path/to/public/key should be the same as displayed when generating the ssh-key ~/.ssh/id_rsa.pub in some cases. Step 3. Test the new keys user@local:~$ ssh -i ~/path/to/public/key RAAPOI_USERNAME@raapoi.vuw.ac.nz Step 4. On your local machine, update ssh config file Create ~/.ssh/config file if it does not exist. Add hostname details to it: Host VSCode_Compute User <YOUR_RAAPOI_USERNAME> HostName amd01n01 ProxyJump raapoi_login Host raapoi_login HostName raapoi.vuw.ac.nz User <YOUR_RAAPOI_USERNAME> Host * ForwardAgent yes ForwardX11 yes ForwardX11Trusted yes IdentityFile ~/.ssh/id_rsa # Add your own private key path here AddKeysToAgent yes StrictHostKeyChecking no UserKnownHostsFile /dev/null Step 5. On your local machine, open a terminal window and login to R\u0101poi normally user@local:~$ ssh raapoi_login Once logged in alloc resources for the VSCode session RAAPOI_USERNAME@raapoi-login:~$ srun -t0-01:00:00 -wamd01n01 --mem = 4G --pty bash Tip Extend the time to maximum 5 hours with -t0-04:59:59 Step 6. Connect VSCode session Open VSCode window, and click on the bottom left corner that says Open a Remote Window , and then choose Connect to Host and then selecting VSCode_Compute as a host. Once a connection is established, your VSCode session should be running on a compute node now. Tip To speed up VSCode, there are steps mentioned in the official VSCode docs . Below is just a part of it: Once connected, update VSCode's /nfs/home/$USER/.vscode-server/data/Machine/settings.json , and add the following lines to it: { \"files.watcherExclude\" : { \"**\" :true, } , \"files.exclude\" : { \"**/.*\" : true, } , \"search.followSymlinks\" :false, \"search.exclude\" : { \"**\" :true, } , \"terminal.integrated.inheritEnv\" : false, } Step 7. To close VSCode session. Go to File > Close Remote Connection Tip The instructions above assume that the node amd01n01 is up and has sufficient resources available. There may be times when this is not the case and you need to adapt these steps to access cpus on a different node. As a workaround you'll need to modify steps 4 and 5 to point towards a different node. If you do, you should make a note to revert these changes to utilise amd01n01 once it is available again.","title":"Vscode"},{"location":"usersub/vscode/#vscode","text":"Tip Windows users are recommended to use Git Bash for the following instructions to work. The instructions below should let users run their VSCode session on a compute node. Pre-requisite installations: A recent version of vscode. Extension Remote - SSH from https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-ssh . Step 1. On your local machine, create ssh keys (existing ssh keys can also be used - no need to create new ones) user@local:~$ ssh-keygen Follow the prompts on the terminal to generate ssh-key pair, and note the directory where keys are being saved. Step 2. Send the public key to R\u0101poi user@local:~$ ssh-copy-id -i ~/path/to/public/key RAAPOI_USERNAME@raapoi.vuw.ac.nz The path ~/path/to/public/key should be the same as displayed when generating the ssh-key ~/.ssh/id_rsa.pub in some cases. Step 3. Test the new keys user@local:~$ ssh -i ~/path/to/public/key RAAPOI_USERNAME@raapoi.vuw.ac.nz Step 4. On your local machine, update ssh config file Create ~/.ssh/config file if it does not exist. Add hostname details to it: Host VSCode_Compute User <YOUR_RAAPOI_USERNAME> HostName amd01n01 ProxyJump raapoi_login Host raapoi_login HostName raapoi.vuw.ac.nz User <YOUR_RAAPOI_USERNAME> Host * ForwardAgent yes ForwardX11 yes ForwardX11Trusted yes IdentityFile ~/.ssh/id_rsa # Add your own private key path here AddKeysToAgent yes StrictHostKeyChecking no UserKnownHostsFile /dev/null Step 5. On your local machine, open a terminal window and login to R\u0101poi normally user@local:~$ ssh raapoi_login Once logged in alloc resources for the VSCode session RAAPOI_USERNAME@raapoi-login:~$ srun -t0-01:00:00 -wamd01n01 --mem = 4G --pty bash Tip Extend the time to maximum 5 hours with -t0-04:59:59 Step 6. Connect VSCode session Open VSCode window, and click on the bottom left corner that says Open a Remote Window , and then choose Connect to Host and then selecting VSCode_Compute as a host. Once a connection is established, your VSCode session should be running on a compute node now. Tip To speed up VSCode, there are steps mentioned in the official VSCode docs . Below is just a part of it: Once connected, update VSCode's /nfs/home/$USER/.vscode-server/data/Machine/settings.json , and add the following lines to it: { \"files.watcherExclude\" : { \"**\" :true, } , \"files.exclude\" : { \"**/.*\" : true, } , \"search.followSymlinks\" :false, \"search.exclude\" : { \"**\" :true, } , \"terminal.integrated.inheritEnv\" : false, } Step 7. To close VSCode session. Go to File > Close Remote Connection Tip The instructions above assume that the node amd01n01 is up and has sufficient resources available. There may be times when this is not the case and you need to adapt these steps to access cpus on a different node. As a workaround you'll need to modify steps 4 and 5 to point towards a different node. If you do, you should make a note to revert these changes to utilise amd01n01 once it is available again.","title":"VSCode"}]}